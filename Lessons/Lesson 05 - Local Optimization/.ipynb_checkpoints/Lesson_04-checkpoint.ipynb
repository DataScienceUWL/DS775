{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixes\n",
    "\n",
    "<font color=\"red\">\n",
    "    \n",
    "* remove matplotlib notebook\n",
    "* f-strings\n",
    "* introduce a local search version of simanneal \n",
    "* when working with local search carefully label \"move\" and \"energy\" parts of code\n",
    "* just a lot here, how can it be simplified?\n",
    "\n",
    "</font>\n",
    "\n",
    "<font size=18>Lesson 04: Quadratic Programming and Local Optimization</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# execute to import notebook styling for tables and width etc.\n",
    "from IPython.core.display import HTML\n",
    "import urllib.request\n",
    "response = urllib.request.urlopen('https://raw.githubusercontent.com/DataScienceUWL/DS775v2/master/ds755.css')\n",
    "HTML(response.read().decode(\"utf-8\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "# comment out the next line if running in CoCalc front end\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "from scipy.optimize import minimize\n",
    "import babel.numbers as numbers\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "import json\n",
    "import time\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "from mpl_toolkits.basemap import Basemap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Optimization Basics (video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Work your way through the embedded storybook below to learn some basic ideas about optimization.  This material complements the material in the textbook.  The graphs and demos in Slides 9-11 are available in the separate file Storybook_Graphs_04.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"600\"\n",
       "            src=\"https://media.uwex.edu/content/ds/ds775_r19/ds775_lesson4-optimization-basics/index.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10605e7f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execute this cell for video\n",
    "from IPython.display import IFrame\n",
    "IFrame(\n",
    "    \"https://media.uwex.edu/content/ds/ds775_r19/ds775_lesson4-optimization-basics/index.html\",\n",
    "    width=900,\n",
    "    height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Quadratic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You should read about quadratic programming in the textbook.  In short, the constraints are the same as they are in linear programming and the objective function can have degree 2 and interaction terms.  In Pyomo it is only a matter of changing the solver to one capable of solving quadratic programs, `ipopt`, instead of `glpk`.  Other solvers like CPLEX, a commercial solver, could also be used.  You  may need to install `ipopt` using conda on your own machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Wyndor Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We'll use Pyomo to solve the quadratic variation of the Wyndor problem illustrated in Figure 13.6 on page 554.\n",
    "\n",
    "<img src=\"images/wyndor_quad.png\" width=\"600\">\n",
    "\n",
    "This profit function is kind of nonsensical, but it will serve to illustrate a Pyomo solution.  We'll use a concrete model formulation for simplicity.  Note, the Pyomo package `minimize` conflicts with the `minimize` from `scipy.optimize` we're using elsewhere in this notebook, so we'll import `pyomo` a bit differently here than usual.  Alternately we could re-import `minimize` from `scipy.optimize` after we're done with Pyomo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profit =  $857,000.00\n",
      "Batches of Doors = 2.67\n",
      "Batches of Windows = 5.00\n"
     ]
    }
   ],
   "source": [
    "# unfold to see Pyomo solution for Wyndor Quadratic Program\n",
    "import pyomo.environ as pyo\n",
    "\n",
    "# Concrete Model\n",
    "model = pyo.ConcreteModel(name=\"Wyndor\")\n",
    "\n",
    "products = ['drs', 'wdw']\n",
    "\n",
    "bounds_dict = {'drs': (0, 4), 'wdw': (0, 6)}\n",
    "\n",
    "\n",
    "def bounds_rule(model, product):\n",
    "    return (bounds_dict[product])\n",
    "\n",
    "\n",
    "model.x = pyo.Var(products, domain=pyo.Reals, bounds=bounds_rule)\n",
    "\n",
    "# Objective\n",
    "model.profit = pyo.Objective(expr=126.0 * model.x['drs'] -\n",
    "                         9.0 * model.x['drs']**2 + 182.0 * model.x['wdw'] -\n",
    "                         13.0 * model.x['wdw']**2.0,\n",
    "                         sense=pyo.maximize)\n",
    "\n",
    "# Constraints\n",
    "model.Constraint3 = pyo.Constraint(\n",
    "    expr=3.0 * model.x['drs'] + 2.0 * model.x['wdw'] <= 18)\n",
    "\n",
    "# Solve\n",
    "solver = pyo.SolverFactory('ipopt')\n",
    "solver.solve(model)\n",
    "\n",
    "# display(model)\n",
    "\n",
    "# display solution\n",
    "import babel.numbers as numbers  # needed to display as currency\n",
    "print(\"Profit = \",\n",
    "      numbers.format_currency(1000 * model.profit(), 'USD', locale='en_US'))\n",
    "print(\"Batches of Doors = {:1.2f}\".format(model.x['drs']()))\n",
    "print(\"Batches of Windows = {:1.2f}\".format(model.x['wdw']()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Search - Continuous Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local search algorithms for continuous variables are generally based on approximating the objective function near the current search point, then using that approximation to compute an improved search point.  For instance if we can calculate the gradient (calculus) or approximate it, then a move along the gradient direction will increase the value of the function.  \n",
    "\n",
    "We'll primarily use the `scipy.optimize` function `minimize` for local search on continuous functions.  You can read more about it below.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Minimize from scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"600\"\n",
       "            src=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x11ad92550>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execute this cell to see scipy.optimize.minimize documentation\n",
    "from IPython.display import IFrame\n",
    "IFrame(\n",
    "    \"https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize\",\n",
    "    width=900,\n",
    "    height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "While there are many options here, the defaults will serve well for our purposes.  The `method` specifies a variety of different numerical algorithms for local search.  We'll use `BFGS` for unbounded problems and `L-BFGS-B` for problems with bounds.  You shouldn't have to specify that choice as those are the defaults.  The BFGS methods are robust algorithms and are known as quasi-Newton methods.  What this means is that they approximate the shape of the objective function near the current point by approximating the derivatives (slopes and curvature) of the function and that shape information is used to produce an improved search point. \n",
    "\n",
    "One of the things to pay attention to here is how we have to write our objective functions so that they can be passed to the `minimize` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### A univariate function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Lets investigate the fourth degree polynomial $$p(x) = x^4 + 2 x^3 + 3 x^2 + 2 x + 1.$$  Let's graph it to get an idea of the behavior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Some output was deleted.\n"
     ]
    }
   ],
   "source": [
    "# plot p(x) on [-10,10]\n",
    "x = np.linspace(-10,10,201)\n",
    "p = lambda x:x**4 + 2*x**3 + 3*x**2 + 2*x + 1\n",
    "fig = plt.figure(figsize=(4,3.5))\n",
    "plt.plot(x,p(x));\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The overall \"U\" shape is not surprising since for polynomials the behavior for large values of $x$ is determined by the highest degree term which is, in this case, $x^4$.  It appears that there is a minimum or minima close to the origin.  Let's zoom in a bit to see what we can:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Some output was deleted.\n"
     ]
    }
   ],
   "source": [
    "# plot p(x) on [-3,3]\n",
    "x = np.linspace(-3,3,201)\n",
    "p = lambda x:x**4 + 2*x**3 + 3*x**2 + 2*x + 1\n",
    "fig = plt.figure(figsize=(4,3.5))\n",
    "plt.plot(x,p(x));\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There appears to be only one minimum somewhere around $x=0$ or $x=-1$.  Since the function appears to be convex, the starting point doesn't matter.  Let's search for the minimum beginning at $x_0 = -2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 0.562500000000177\n",
       " hess_inv: array([[0.3331038]])\n",
       "      jac: array([-9.983778e-07])\n",
       "  message: 'Optimization terminated successfully.'\n",
       "     nfev: 21\n",
       "      nit: 6\n",
       "     njev: 7\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([-0.50000034])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execute for local search\n",
    "result = minimize(p,-2)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum value is 0.5625 and occurs at x = -0.5000\n"
     ]
    }
   ],
   "source": [
    "print('The minimum value is {:0.4f} and occurs at x = {:0.4f}'.format(result.fun,result.x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "source": [
    "###  <font color = \"blue\">Self Assessment:  Minimize to Maximize</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "An apartment complex has 250 apartments to rent and that their profit in thousands of dollars is given by the function \n",
    "$$P(x) = -0.008 x^2 + 3.1 x - 80.$$\n",
    "Find the maximum profit and how many apartments to rent to achieve the maximum profit.  Use minimize from scipy.optimize to find the maximum.  Review the video above to see how to \"flip\" the problem to find a maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Some output was deleted.\n"
     ]
    }
   ],
   "source": [
    "# graph of profit function\n",
    "x = np.linspace(0,250,201) # 201 points between 0 and 250\n",
    "P = lambda x:-0.008*x**2 + 3.1*x - 80 # lambda is for writing one line functions\n",
    "fig = plt.figure(figsize=(4,3.5));\n",
    "plt.plot(x,P(x));\n",
    "plt.xlabel('apartments');\n",
    "plt.ylabel('profit (\\$ thousands)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# add your code here, note you should be able to guess an initial value from the graph ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Your answer shouldn't be an integer.  We could use discrete optimization and only optimize integer numbers of apartments, but it will usually be more computationally intensive.  Instead we're using a continuous variable to get an approximation to the discrete problem, this is called **relaxation** (we've relaxed the integer variable condition).  So what whole number of apartments should you rent?  Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### <font color = \"blue\">Self Assessment:  Finding Multiple Extrema</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The function $f(x) = x^5 - x^4 - 18 x^3 + 16 x^2 + 32 x - 2$ for $-4 \\leq x \\leq 3.6$ appears in the video \"Gradient Descent and Local Minima\" above.  Plot the function on the given interval.  Use the graph to guess where the local maxima and minima are.  Now use minimize from scipy.optimize to find the $x$ and $y$ coordinates of all the extrema.  You can add bounds to the minimize call like this:\n",
    "\n",
    "```\n",
    "minimize(f,x0,bounds=[(-4,3.6)],method='TNC')\n",
    "```\n",
    "The 'TNC' method works well with continuous optimization when bounds are included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "## Example: Simple Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Walkthrough of this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"640\"\n",
       "            height=\"360\"\n",
       "            src=\"https://media.uwex.edu/content/ds/ds775_r19/ds775_lesson4-logistic-regression/index.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x105c28400>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execute this cell for video\n",
    "from IPython.display import IFrame\n",
    "IFrame(\n",
    "    \"https://media.uwex.edu/content/ds/ds775_r19/ds775_lesson4-logistic-regression/index.html\",\n",
    "    width=640,\n",
    "    height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most machine learning algorithms are driven by optimization.  Usually we want to minimize a loss function which measures the difference between the model predictions and the observed data.  Neural network training uses a version of the gradient descent algorithm to optimize the weights in the network.  Here we'll show how to fit a logistic regression model by maximizing a function.\n",
    "\n",
    "In simple logistic regression we try to predict the value of the label $y$, which can be 0 or 1, for each value of a continuous predictor variable $x$.  In particular, the conditional probability that $y=1$ given the current value of $x$ is modeled by a sigmoid function (\"s\" curve) $$p(x) = \\frac{1}{1 + e^{-(b_0 + b_1 x)}}.$$\n",
    "\n",
    "For example, the more hours a student studies to prepare for an exam, the higher the probability that they will pass the test.  Shown below is some data.  For each student we have the number of hours they studied and whether or not they passed the exam (1 for passed, 0 for failed).  This example data comes from the <a href=\"https://en.wikipedia.org/wiki/Logistic_regression\">Wikipedia article on Logistic Regresssion</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data from \n",
    "x_hours = np.array([\n",
    "    0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 1.75, 2.0, 2.25, 2.50, 2.75, 3.00, 3.25,\n",
    "    3.5, 4.0, 4.25, 4.5, 4.75, 5.0, 5.5\n",
    "])\n",
    "y_passed = np.array([0,0,0,0,0,0,1,0,1,0,1,0,1,0,1,1,1,1,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a graph of the data along with the graph of the fitted sigmoid function that models the probality of $y=1$ at each $x$.  Don't worry, we'll see where the fitted curve comes from in a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Some output was deleted.\n"
     ]
    }
   ],
   "source": [
    "# graph of data and sigmoid\n",
    "b0 = -4.07771657\n",
    "b1 = 1.5046468\n",
    "hours_studied = np.linspace(0,6,101)\n",
    "def sigmoid(x,intercept,slope):\n",
    "    return( 1.0 / (1.0 + np.exp( -(intercept + slope * x) ) ) )\n",
    "prob_passed = sigmoid(hours_studied, b0, b1)\n",
    "\n",
    "fig = plt.figure();\n",
    "fig.set_size_inches(6,3.5);\n",
    "ax = fig.add_subplot(111);\n",
    "ax.scatter(x_hours, y_passed);\n",
    "ax.plot(hours_studied, prob_passed);\n",
    "ax.set_xlabel('hours studied');\n",
    "ax.set_ylabel('passed');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the model to make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A student who studies 4 hours has approximately 87.4% chance of passing.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    'A student who studies 4 hours has approximately {:3.1f}% chance of passing.'\n",
    "    .format(100 * sigmoid(4, b0, b1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:  It isn't important to understand the details of logistic regression for this class, but the text in this cell gives a bit of background.*\n",
    "\n",
    "Where do those values for the slope and intercept come from?  To obtain those we find the values of $b_0$ and $b_1$ that maximize the likelihood function:\n",
    "$$ L(b_0,b_1) = \\prod_{i=1}^{n} p(x_i)^{y_i} (1-p(x_i))^{(1-y_i)}$$\n",
    "where $(x_i,y_i)$ are the data pairs for each student and $p(x) = \\displaystyle \\frac{1}{1 + e^{-(b_0 + b_1 x)}}$ is the sigmoid function.  By maximizing the likelihood function we are maximizing the probability that this model produced the observed data.  Note that the $\\prod$ symbol means to take the product of the values, like $\\sum$ means to take the sum.\n",
    "\n",
    "In practice, maximizing a product can lead to numerical difficulties, so we instead maximize the log-likelihood function found by taking the logarithm of $L(b_0,b_1)$ to get:\n",
    "$$LL(b_0, b_1) = \\sum_{i = 1}^{n} \\left[ y_i \\log( p(x_i) ) + (1-y_i) \\log(1-p(x_i)) \\right].$$\n",
    "\n",
    "Now we need to find $b_0$ and $b_1$ to maximize this.  The log-likelihood function turns out to be concave so that ascending from any starting point will lead to the global maximum.  \n",
    "\n",
    "Because we will use the `minimize` function from `scipy.optimize` to find the maximum log-likelihood we'll minimize the negative log-likelihood:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the model with `minimize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def neg_log_loss( coef, *args):\n",
    "    b0 = coef[0]\n",
    "    b1 = coef[1]\n",
    "    x = args[0]\n",
    "    y = args[1]\n",
    "    yhat = b0 + b1 * x\n",
    "    p = 1.0/(1.0 + np.exp(-(b0 + b1*x)))\n",
    "    ll = sum( y*np.log(p)+(1-y)*np.log(1-p) )\n",
    "    return(-ll) # here's the minus sign!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a minute to see how this function is structured and perhaps glance at the documentation again.  `coef` is a one-dimensional array with shape (n,) that contains all $n$ optimization variables.  In this case there are two which we assign to $b_0$ and $b_1$.  `*args` is a pointer to tuple `args` that contains any additional parameters that should be passed to the function.  In the case `minimize` will be passing the tuple $(x,y)$ that contains the training data.  We'll pass `args = (x_hours, y_passed)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-6-606f5d1e4af2>(10)neg_log_loss()\n",
      "-> return(-ll) # here's the minus sign!\n",
      "(Pdb) p\n",
      "*** SyntaxError: unexpected EOF while parsing\n",
      "(Pdb) p\n",
      "*** SyntaxError: unexpected EOF while parsing\n",
      "(Pdb) print(p)\n",
      "[0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5]\n",
      "(Pdb) print(ll)\n",
      "-13.862943611198906\n",
      "(Pdb) ll\n",
      "  1  \tdef neg_log_loss( coef, *args):\n",
      "  2  \t    b0 = coef[0]\n",
      "  3  \t    b1 = coef[1]\n",
      "  4  \t    x = args[0]\n",
      "  5  \t    y = args[1]\n",
      "  6  \t    yhat = b0 + b1 * x\n",
      "  7  \t    p = 1.0/(1.0 + np.exp(-(b0 + b1*x)))\n",
      "  8  \t    ll = sum( y*np.log(p)+(1-y)*np.log(1-p) )\n",
      "  9  \t    import pdb; pdb.set_trace()\n",
      " 10  ->\t    return(-ll) # here's the minus sign!\n",
      "(Pdb) print(ll)\n",
      "-13.862943611198906\n"
     ]
    }
   ],
   "source": [
    "result = minimize(neg_log_loss,[0,0],args=(x_hours,y_passed))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum likelihood estimate for p(x) has intercept b0 = -4.078 and slope b1 = 1.505\n"
     ]
    }
   ],
   "source": [
    "b0 = result.x[0]\n",
    "b1 = result.x[1]\n",
    "print('The maximum likelihood estimate for p(x) has intercept b0 = {:2.3f} and slope b1 = {:2.3f}'.format(b0,b1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:  the approach outlined here for logisitic regression is very similar to the actual algorithms used by most software for computing logistic regression models. Many machine learning predictive models are trained by optimization.  To verify our results we check our results against those from Sci-kit Learn.  By default sklearn uses an L2 regularization term to avoid overfitting (more about this in DS740).  The amount of regularization is proportional to $1/C$ so we just use a huge $C$ to mimic no regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum likelihood estimate for p(x) has intercept b0 = -4.078 and slope b1 = 1.505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jbaggett/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression    \n",
    "model = LogisticRegression(C=1.0e10,fit_intercept = True)\n",
    "model.fit(x_hours.reshape(-1,1), y_passed)\n",
    "b0 = model.intercept_[0]\n",
    "b1 = model.coef_[0][0]\n",
    "print('The maximum likelihood estimate for p(x) has intercept b0 = {:2.3f} and slope b1 = {:2.3f}'.format(b0,b1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example:  The Rastrigin Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Rastrigin function is a common test case for optimization algorithms because it has many local minima.  The definition of the function is \n",
    "$$f(\\mathbf{x})=10 n+\\sum_{i=1}^{n}\\left[x_{i}^{2}-A \\cos \\left(2 \\pi x_{i}\\right)\\right]$$\n",
    "Where $n$ is the dimensionality of input vector $\\mathbf{x}$.  For instance if $n=2$ then $\\mathbf{x} = (x_1, x_2)$.  The domain is restricted so that each $x_i \\in [-5.12, 5.12].$ .   Here is a graph of the the Rastrigin function with dimension $n=1.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Some output was deleted.\n"
     ]
    }
   ],
   "source": [
    "# open to reveal graph code\n",
    "def rastrigin_1D(x):\n",
    "    return (x**2 + 10 - 10 * np.cos(2 * np.pi * x))\n",
    "    \n",
    "x = np.linspace(-5.12,5.12,201)\n",
    "y = rastrigin_1D(x)\n",
    "\n",
    "fig = plt.figure(figsize=(3,3))\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Some output was deleted.\n"
     ]
    }
   ],
   "source": [
    "# 3D Graph of Rastrigin with dimension n = 2\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import numpy as np\n",
    "\n",
    "x = np.linspace(-5.12, 5.12, 401)     \n",
    "y = np.linspace(-5.12, 5.12, 401)     \n",
    "X, Y = np.meshgrid(x, y) \n",
    "Z = (X**2 - 10 * np.cos(2 * np.pi * X)) + \\\n",
    "  (Y**2 - 10 * np.cos(2 * np.pi * Y)) + 20\n",
    "\n",
    "data = [\n",
    "    go.Surface( x = X, y = Y, z = Z, colorscale = 'Jet',\n",
    "        contours=go.surface.Contours(\n",
    "            z=go.surface.contours.Z(\n",
    "              show=True,\n",
    "              usecolormap=True,\n",
    "              highlightcolor=\"#42f462\",\n",
    "              project=dict(z=True)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "]\n",
    "\n",
    "layout = go.Layout(title='Rastrigin',width=600,height=600)\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Rastrigin function isn't important as a real-life example, but it does serve as a good test problem with oodles of local minima and we know that global minimum occurs at the origin.  This is similar to what can happen in training in neural networks and other complex models except that we don't know where the global optimum is.\n",
    "\n",
    "A simple approach for trying to find the global minimum of a multi-modal function is called a **restart** or **multistart strategy** in which local searches are started at randomly generated initial points and the most optimal result of all the local searches is recorded.\n",
    "\n",
    "Here is pseudo-code for a multistart code:\n",
    "```\n",
    "for num_searches:\n",
    " choose random initial state\n",
    " do local search\n",
    " if new optimum\n",
    "     remember it\n",
    "endfor\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The smallest value find is 19.899 at x = -1.990 and y = 0.995\n"
     ]
    }
   ],
   "source": [
    "def rastrigin(x):\n",
    "    # pass a single vector of length n (=dim) to evaluate Rastrigin\n",
    "    return sum(x**2 + 10 - 10 * np.cos(2 * np.pi * x))\n",
    "\n",
    "dim = 10\n",
    "num_local_searches = 1000\n",
    "best_value = 1.e10\n",
    "bounds = [(-5.12,5.12) for i in range(dim)]\n",
    "\n",
    "for i in range(num_local_searches):\n",
    "    x_initial = np.random.uniform(-5.12, 5.12, dim)\n",
    "    result = minimize(rastrigin,x_initial,bounds=bounds, method='TNC')\n",
    "    if result.fun < best_value:\n",
    "        best_value = result.fun\n",
    "        best_x = result.x\n",
    "\n",
    "print(\n",
    "    'The smallest value find is {:4.3f} at x = {:1.3f} and y = {:1.3f}'.format(\n",
    "        best_value, best_x[0], best_x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Self-Assessment: Rastrigin with dim = 3, 4</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many iterations does it take to reliably find the global minimum with dim = 3?  With dim = 4?  Use the multi-start strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = \"blue\">Self-Assessment:  Rastrigin with dim = 10 </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do 1000 local search with Rastrigin with dim = 10.  What is the smallest value you find?  How long do you think it would take to find the minimum from randomly chosen initial points like this?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## The curse of dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If we are maximizing a function of one variable, $f(x)$, we might choose to use 10 starting points.  For a function of two variables, $g(x,y)$ to get the same search power we would choose 10 points in the $x$ direction and 10 points in the $y$ direction to make a grid of $10^2 = 100$ starting points in the $xy$-plane.  For three variables we need $10^3 = 1000$ points in $xyz$-space.  For a function of $n$ variables we would need $10^n$ starting points.\n",
    "\n",
    "*The volume of the search space grows exponentially with the number of variables or dimensionality of the problem.*\n",
    "\n",
    "This is called the curse of dimensionality.  For high dimensional functions like those that occur in training neural networks and other applications with many local minima it can be very difficult to find the global minima because the volume of the search space grows exponentially with the number of variables.\n",
    "\n",
    "For the Rastrigin function to find the global minimum you need an initial starting point in the interval (-0.5,0.5) in each dimension.  The search interval is [-5.12,5.12] in each dimension.  Thus the probability that a single uniformly sampled point in [-5.12,5.12] is $\\frac{1}{10.28} \\approx 0.0973$ (the ratio of the lengths of the two intervals).  The probability of finding the global minimum using local search from a uniformly sampled point in $n$ dimensions is $$\\left( \\frac{1}{10.28} \\right)^n$$.  That means we'd have to, on average, start $10.28^n$ local searches from uniformly sampled points to find the global minimum once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### <font color=\"blue\">Self-Assessment:  How many searches?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For the Rastrigin function write a while loop that runs until the global minimum value is found ($|\\mbox{best_val}|<0.01$) and track the number of iterations. Use a for loop to repeat this three times until your code is debugged.  After your code is working, repeat the process 100 times and report the average number of searches until the global minimum is found when $n=1,2,3$.  Are these numbers in approximate agreement with with the estimated numbers $10.28^n$ (they very likely won't be all that close, but how is the overall trend)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### <font color = \"blue\">Self Assessment: How many searches when dim = 10?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Approximately now many local searches are required to find the global minimum one time when dim = 10?  Is it surprising that you (very likely) didn't find it with 1000 local searches?  Explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Search - Discrete Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization with discrete variables tends to be more complicated than with continuous variables.  In the continuous case we can take advantage of calculus or numerical methods to compute gradient search directions that allow us to move to nearby points that are closer to optimal.  However with discrete random variables there is no generic way to compute better nearby points.  Often the best we can do is find nearby points, which is usually problem specific, and try them to see if they produce closer to optimal results.\n",
    "\n",
    "We'll look carefully at the traveling salesman problem (TSP).  In addition to the information about the TSP in the textbook, there is copious information available on the internet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Basics of Local Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is pseudo-code for a simple local search.  Many variations are possible, but they often look like this:\n",
    "```\n",
    " set starting state \n",
    " while local_condition \n",
    "     select a move \n",
    "     if acceptable \n",
    "         do the move \n",
    "         if new optimum \n",
    "             remember it \n",
    " endwhile \n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* Often the starting state is one selected at random.  State refers to the \"state\" or values of the variables.\n",
    "\n",
    "* The local condition is a stopping condition.  It could be something like stopping after a fixed number of iterations or stopping after making no or insignificant process for a while.\n",
    "\n",
    "* Selecting a move is where things get problem specific.  Often the move involves a random change to the variables.\n",
    "\n",
    "* If acceptable means that we are checking to see that the state is feasible, that is, does it satisfy the constraints?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Local Search for TSP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's see what this looks like for the TSP.  We'll use the subtour reversal algorithm, described in the textbook, to generate moves.  Here's what the local search looks like for the TSP:\n",
    "\n",
    "```\n",
    " choose a random tour  \n",
    " while shorter tours have been found in last max_tries\n",
    "     propose new tour with one random segment reversed\n",
    "     if acceptable (it will always be a valid tour) \n",
    "         compute new tour distance \n",
    "         if new shortest tour \n",
    "             remember it \n",
    "         else\n",
    "             reject new tour\n",
    " endwhile \n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Video for TSP Local Search Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"640\"\n",
       "            height=\"360\"\n",
       "            src=\"https://media.uwex.edu/content/ds/ds775_r19/ds775_lesson4-logistic-traveling-salesman-problem/index.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1240a9e10>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execute this cell for video\n",
    "from IPython.display import IFrame\n",
    "IFrame(\n",
    "    \"https://media.uwex.edu/content/ds/ds775_r19/ds775_lesson4-logistic-traveling-salesman-problem/index.html\",\n",
    "    width=640,\n",
    "    height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSP Local Search Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be minimizing the total length of a tour that visits all 48 state capitals in the continental United States and ends back in the same city in which it begins.  The latitudes and longitudes of the cities were projected onto a rectangular coordinate system with $x$ and $y$ coordinates representing positions in meters which we convert to kilometers.  We have stored the $x$ and $y$ coordinates and a distance matrix with distances between all of the cities in the json file `Caps48.json`.  First we load the data and define a function visualize tours of the 48 capitals.  We plot the best possible tour just to show how the plotting routine works.  The coordinates in the json file are in meters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Some output was deleted.\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/Caps48.json\", \"r\") as tsp_data:\n",
    "    tsp = json.load(tsp_data)\n",
    "distance_matrix = tsp[\"DistanceMatrix\"]\n",
    "optimal_tour = tsp[\"OptTour\"]\n",
    "opt_dist = tsp[\"OptDistance\"]/1000 # converted to kilometers\n",
    "xy = np.array(tsp[\"Coordinates\"])\n",
    "\n",
    "def plot_tour(best_tour, xy_meters, best_dist, height, width):\n",
    "    \n",
    "    meters_to_pxl = 0.0004374627441064968\n",
    "    intercept_x = 2.464\n",
    "    intercept_y = 1342.546\n",
    "    xy_pixels = np.zeros(xy_meters.shape)\n",
    "    xy_pixels[:,0] = meters_to_pxl * xy_meters[:,0] + intercept_x\n",
    "    xy_pixels[:,1] = -meters_to_pxl * xy_meters[:,1] + intercept_y\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(height, width))\n",
    "    im = plt.imread('images/caps48.png')\n",
    "    implot = ax.imshow(im)\n",
    "    plt.setp(ax.get_xticklabels(), visible=False)\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.tick_params(axis='both', which='both', length=0)\n",
    "\n",
    "    loop_tour = np.append(best_tour, best_tour[0])\n",
    "    ax.plot(xy_pixels[loop_tour, 0],\n",
    "            xy_pixels[loop_tour, 1],\n",
    "            c='b',\n",
    "            linewidth=1,\n",
    "            linestyle='-')\n",
    "    plt.title('Best Distance {:d} km'.format(int(best_dist)))\n",
    "\n",
    "plot_tour(optimal_tour, xy, opt_dist, 6, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define both the local \"move\" function which reverses a tour segment to generate a new tour and the objective function which computes the length of the tour in kilometers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# define move and objective functions\n",
    "\n",
    "def sub_tour_reversal(tour):\n",
    "    # reverse a random tour segment\n",
    "    num_cities = len(tour)\n",
    "    i, j = np.sort(np.random.choice(num_cities, 2, replace=False))\n",
    "    return np.concatenate((tour[0:i], tour[j:-num_cities + i - 1:-1],\n",
    "                              tour[j + 1:num_cities]))\n",
    "def tour_distance(tour, dist_mat):\n",
    "    distance = dist_mat[tour[-1]][tour[0]]\n",
    "    for gene1, gene2 in zip(tour[0:-1], tour[1:]):\n",
    "        distance += dist_mat[gene1][gene2]\n",
    "    return distance/1000 # convert to kilometers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Some output was deleted.\n"
     ]
    }
   ],
   "source": [
    "# local search for TSP solution with a random segment reversal at each iteration\n",
    "\n",
    "# Try running this cell several times to see how the results of local search can vary\n",
    "\n",
    "def random_reversals(dist_mat, max_no_improve):\n",
    "    num_cities = len(dist_mat)\n",
    "    # starts from a random tour\n",
    "    current_tour = np.random.permutation(np.arange(num_cities))\n",
    "    current_dist = tour_distance(current_tour, dist_mat)\n",
    "    best_tour = current_tour\n",
    "    best_dist = current_dist\n",
    "\n",
    "    # stop search if no better tour is found within max_no_improve iterations, can increase to eliminate crossovers\n",
    "    num_moves_no_improve = 0\n",
    "    iterations = 0\n",
    "    while (num_moves_no_improve < max_no_improve):\n",
    "        num_moves_no_improve += 1\n",
    "        iterations += 1  # just for tracking\n",
    "        new_tour = sub_tour_reversal(current_tour)\n",
    "        new_dist = tour_distance(new_tour, dist_mat)\n",
    "        if new_dist < current_dist:\n",
    "            num_moves_no_improve = 0\n",
    "            current_tour = new_tour\n",
    "            current_dist = new_dist\n",
    "            if current_dist < best_dist:  # not really needed since current_tour will be best\n",
    "                best_tour = current_tour  # but we'll use this in the next lesson\n",
    "                best_dist = current_dist\n",
    "    return best_tour, best_dist, iterations\n",
    "\n",
    "\n",
    "best_tour, best_dist, iterations = random_reversals(distance_matrix, 1000)\n",
    "plot_tour(best_tour, xy, best_dist, 9, 6)\n",
    "print('The minimum distance found is {:d} after {:d} iterations'.format(\n",
    "    int(best_dist), iterations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Local Search with Graphing for TSP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Though it's not important, if you like you can execute the two cells below to see an animated version of this search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Some output was deleted.\n"
     ]
    }
   ],
   "source": [
    "# execute this cell first to initialize the plot\n",
    "# and choose a random starting tour\n",
    "\n",
    "# load problem data\n",
    "with open(\"data/Caps48.json\", \"r\") as tsp_data:\n",
    "    tsp = json.load(tsp_data)\n",
    "dist_mat = tsp[\"DistanceMatrix\"]\n",
    "optimal_tour = tsp[\"OptTour\"]\n",
    "opt_dist = tsp[\"OptDistance\"]/1000 # converted to kilometers\n",
    "xy_meters = np.array(tsp[\"Coordinates\"])\n",
    "\n",
    "# initialize with a random tour\n",
    "n = 48\n",
    "current_tour = np.random.permutation(np.arange(n))\n",
    "current_dist = tour_distance(current_tour, dist_mat)\n",
    "\n",
    "# plot initial tour\n",
    "meters_to_pxl = 0.0004374627441064968\n",
    "intercept_x = 2.464\n",
    "intercept_y = 1342.546\n",
    "xy_pixels = np.zeros(xy_meters.shape)\n",
    "xy_pixels[:,0] = meters_to_pxl * xy_meters[:,0] + intercept_x\n",
    "xy_pixels[:,1] = -meters_to_pxl * xy_meters[:,1] + intercept_y\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "im = plt.imread('images/caps48.png')\n",
    "implot = ax.imshow(im)\n",
    "plt.setp(ax.get_xticklabels(), visible=False)\n",
    "plt.setp(ax.get_yticklabels(), visible=False)\n",
    "ax.tick_params(axis='both', which='both', length=0)\n",
    "\n",
    "loop_tour = np.append(current_tour, current_tour[0])\n",
    "lines, = ax.plot(xy_pixels[loop_tour, 0],\n",
    "        xy_pixels[loop_tour, 1],\n",
    "        c='b',\n",
    "        linewidth=1,\n",
    "        linestyle='-')\n",
    "dst_label = plt.text(100, 1200, '{:d} km'.format(int(current_dist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# initialize with current tour from previous cell\n",
    "best_tour = current_tour\n",
    "best_dist = current_dist\n",
    "\n",
    "count = 1\n",
    "iteration = np.array([count])\n",
    "distances = np.array([best_dist])\n",
    "\n",
    "max_moves_no_improve = 5000\n",
    "num_moves_no_improve = 0\n",
    "while( num_moves_no_improve < max_moves_no_improve):\n",
    "    num_moves_no_improve += 1\n",
    "    new_tour = sub_tour_reversal(current_tour)\n",
    "    new_dist = tour_distance(new_tour, dist_mat)\n",
    "    if new_dist < current_dist:\n",
    "        current_tour = new_tour\n",
    "        current_dist = new_dist\n",
    "        num_moves_no_improve = 0\n",
    "        if current_dist < best_dist: # not really needed since current_tour will be best\n",
    "            best_tour = current_tour # but we'll use this in the next lesson\n",
    "            best_dist = current_dist\n",
    "        loop_tour = np.append(best_tour, best_tour[0])\n",
    "        lines.set_data(xy_pixels[loop_tour, 0], xy_pixels[loop_tour, 1])\n",
    "        dst_label.set_text('{:d} km'.format(int(best_dist)))   \n",
    "        fig.canvas.draw()\n",
    "        fig.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Another local search algorithm for TSP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We'll have a look at an algorithm called \"2-opt\" that was proposed by Croes in 1958.  We won't focus on it too much since the idea doesn't really extend to other problems.  The main idea  is to reverse segments that cross over themselves to remove the cross over.  We loop repeatedly over all the possible reversals until there are no more cross overs.  Here is some Python to do 2-opt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Some output was deleted.\n"
     ]
    }
   ],
   "source": [
    "def sub_tour_reversal_ij(tour, i, j):\n",
    "    # reverse the segment from city i to city j\n",
    "    n = len(tour)\n",
    "    return (np.concatenate((tour[0:i], tour[j:-n + i - 1:-1], tour[j + 1:n])))\n",
    "\n",
    "# 2-opt local search for TSP\n",
    "def two_opt(dist_mat):\n",
    "    num_cities = len(dist_mat)\n",
    "    current_tour = np.random.permutation(np.arange(num_cities))\n",
    "    current_dist = tour_distance(current_tour, dist_mat)\n",
    "    best_tour = current_tour\n",
    "    best_dist = current_dist\n",
    "\n",
    "    improvement = True\n",
    "    iterations = 0\n",
    "    while improvement:\n",
    "        improvement = False\n",
    "        for i in range(num_cities - 1):\n",
    "            for j in range(i + 1, num_cities):\n",
    "                iterations += 1\n",
    "                new_tour = sub_tour_reversal_ij(best_tour, i, j)\n",
    "                new_dist = tour_distance(new_tour, dist_mat)\n",
    "                if new_dist < best_dist:\n",
    "                    best_tour = new_tour\n",
    "                    best_dist = new_dist\n",
    "                    improvement = True\n",
    "    return best_tour, best_dist, iterations\n",
    "\n",
    "best_tour, best_dist, iterations = two_opt(distance_matrix)\n",
    "                \n",
    "plot_tour(best_tour,xy,best_dist,9,6)\n",
    "print('The minimum distance found is {:d} km after {:d} iterations'.format(int(best_dist),iterations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "2-opt is generally uses fewer iterations to find a reasonable tour (local minimum) than does our local search with random segment reversals.  In the next lesson we'll try using 2-opt to find starting points for a global search algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerrymandering Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is based on textbook problem 13.10-6 which is reproduced here:\n",
    "\n",
    "Because of population growth, the state of Washington has been given an additional seat in the House of Representatives, making a total of 10. The state legislature, which is currently controlled by the Republicans, needs to develop a plan for redistricting the state. There are 18 major cities in the state of Washington that need to be assigned to one of the 10 congressional districts. The table below gives the numbers of registered Democrats and registered Republicans in each city. Each district must contain between 150,000 and 350,000 of these registered voters. Assign each city to one of the 10 congressional districts in order to maximize the number of districts that have more registered Republicans than registered Democrats.\n",
    "\n",
    "<img src=\"images/gerrymandering.png\" width=\"300\">\n",
    "\n",
    "We'll provide the data and objective function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we load the data and create an assignment of the 18 cities to the 10 districts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 1, 9, 4, 0, 2, 8, 0, 9, 8, 0, 0, 0, 8, 7, 3, 6])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data + random assignment\n",
    "num_districts = 10\n",
    "min_voters_in_district = 150\n",
    "max_voters_in_district = 350\n",
    "\n",
    "dems = [152,81,75,34,62,38,48,74,98,66,83,86,72,28,112,45,93,72]\n",
    "reps = [62,59,83,52,87,87,69,49,62,72,75,82,83,53,98,82,68,98]\n",
    "cities = pd.DataFrame( data = {'dems':dems, 'reps':reps})\n",
    "\n",
    "# assign = np.random.randint(low=0,high=num_districts,size = 18)\n",
    "assign = np.array([4, 3, 1, 9, 4, 0, 2, 8, 0, 9, 8, 0, 0, 0, 8, 7, 3, 6])\n",
    "assign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `summarize_districts` function below isn't used directly in the initialization, but it helps us by printing out the number of voters (in thousands) assigned to each district and shows us which districts are won by republicans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reps</th>\n",
       "      <th>dems</th>\n",
       "      <th>total</th>\n",
       "      <th>rep_win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>367</td>\n",
       "      <td>322</td>\n",
       "      <td>689</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83</td>\n",
       "      <td>75</td>\n",
       "      <td>158</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69</td>\n",
       "      <td>48</td>\n",
       "      <td>117</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>127</td>\n",
       "      <td>174</td>\n",
       "      <td>301</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>149</td>\n",
       "      <td>214</td>\n",
       "      <td>363</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>98</td>\n",
       "      <td>72</td>\n",
       "      <td>170</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>82</td>\n",
       "      <td>45</td>\n",
       "      <td>127</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>222</td>\n",
       "      <td>269</td>\n",
       "      <td>491</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>124</td>\n",
       "      <td>100</td>\n",
       "      <td>224</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reps  dems  total  rep_win\n",
       "0   367   322    689     True\n",
       "1    83    75    158     True\n",
       "2    69    48    117     True\n",
       "3   127   174    301    False\n",
       "4   149   214    363    False\n",
       "5     0     0      0    False\n",
       "6    98    72    170     True\n",
       "7    82    45    127     True\n",
       "8   222   269    491    False\n",
       "9   124   100    224     True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def summarize_districts(assign, cities):\n",
    "    reps = np.zeros(num_districts, dtype=np.int32)\n",
    "    dems = np.zeros(num_districts, dtype=np.int32)\n",
    "    df = cities.groupby(assign).sum()\n",
    "    reps[df.index] = df['reps']\n",
    "    dems[df.index] = df['dems']\n",
    "    total = reps + dems\n",
    "    delta = np.minimum(np.maximum(total, min_voters_in_district),\n",
    "                       max_voters_in_district) - total\n",
    "    rep_win = reps > dems\n",
    "    dict = {\n",
    "        'reps': reps,\n",
    "        'dems': dems,\n",
    "        'total': total,\n",
    "        'rep_win': rep_win\n",
    "    }\n",
    "    return (pd.DataFrame(data=dict))\n",
    "\n",
    "summarize_districts(assign, cities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the objective function, `fitness_districts` we count the number of districts won by republicans.  Instead of checking each potential solution to see if it's feasible (between 150 and 350 voters in each district) we subtract the total number of thousands by which each district is out of bounds.  This approach to optimization is called a **penalty function method**.  It doesn't prevent infeasible solutions, but it strongly penalizes them so that when the function is optimized the solution generally will be feasible.  Note that in the table above we have several districts that too many or two few voters, when we compute the fitness of that assignment to districts it is negative because of the penalty term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-693"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fitness_districts(assign, cities):\n",
    "    df = cities.groupby(assign).sum()\n",
    "    fitness = sum( df['reps'] > df['dems'] )\n",
    "    total_voters = np.zeros(num_districts,dtype=np.int32)\n",
    "    total_voters[df.index] = df.sum(axis=1)\n",
    "    fitness -= np.abs(np.minimum(np.maximum(total_voters,150),350)-total_voters).sum()\n",
    "    return (fitness)\n",
    "\n",
    "fitness_districts(assign,cities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = \"blue\"> Self Assessment: Gerrymandering Local Search</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with a random state and move to a nearby state by changing one of the city assignments randomly.  You don't have to check for feasibility of each new state, just accept the move if you get a larger value of the fitness function.  Repeat until no progress is made for 1000 moves.  Your local search should be inside of a function with inputs the cities and the initial assignment.  The output should be the optimized assignment and the value of the fitness function.  \n",
    "\n",
    "Now write a loop that does 100 local searches.  Make sure the final, best solution is feasible.  What is the maximum number of districts that Republicans win?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "294.55px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
