{"backend_state":"running","connection_file":"/tmp/xdg-runtime-user/jupyter/kernel-fbbfe0af-9a46-4dd9-9d4d-5e3034599fbb.json","kernel":"python3","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"294.55px"},"toc_section_display":true,"toc_window_display":true},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1673904160789,"exec_count":1,"id":"a39f17","input":"# EXECUTE FIRST\n\n# computational imports\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize, minimize_scalar\nfrom sklearn.linear_model import LogisticRegression\nimport json\n\n# plotting imports\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\n\n# display imports\nfrom IPython.display import display, IFrame\nfrom IPython.core.display import HTML\n\n# for playing videos, customize height and width if desired\ndef play_video(vid_name, w = 640, h = 360):\n    vid_path = \"https://media.uwex.edu/content/ds/ds775_r19/\"\n    return IFrame( vid_path + vid_name + \"/index.html\", width = w, height = h )","kernel":"python3","metadata":{"code_folding":[0]},"no_halt":true,"pos":0,"start":1673904158026,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904160841,"exec_count":2,"id":"547423","input":"# execute this cell for video\nplay_video(\"ds775_lesson5-optimization-basics\", w = 900, h = 600)","kernel":"python3","metadata":{"code_folding":[0],"hidden":true},"no_halt":true,"output":{"0":{"data":{"iframe":"11f8f8245a535e1112e6d9fdcc417e69288ac009","text/plain":"<IPython.lib.display.IFrame at 0x7f8cf00fb310>"},"exec_count":2}},"pos":4,"start":1673904160813,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904163783,"exec_count":3,"id":"dfd775","input":"# unfold to see Pyomo solution for Wyndor Quadratic Program\nimport pyomo.environ as pyo\n\n# Concrete Model\nmodel = pyo.ConcreteModel(name=\"Wyndor\")\n\nproducts = ['drs', 'wdw']\n\nbounds_dict = {'drs': (0, 4), 'wdw': (0, 6)}\n\ndef bounds_rule(model, product):  # using a bounds_rule is a good way to implement simple bounds\n    return (bounds_dict[product])\n\n\nmodel.x = pyo.Var(products, domain=pyo.Reals, bounds=bounds_rule)\n\n# Objective\nmodel.profit = pyo.Objective(expr=126.0 * model.x['drs'] -\n                         9.0 * model.x['drs']**2 + 182.0 * model.x['wdw'] -\n                         13.0 * model.x['wdw']**2,\n                         sense=pyo.maximize)\n\n# Constraints\nmodel.Constraint3 = pyo.Constraint(\n    expr=3.0 * model.x['drs'] + 2.0 * model.x['wdw'] <= 18)\n\n# Solve\nsolver = pyo.SolverFactory('ipopt')\nsolver.solve(model)\n\n# display solution\nprint(f\"Profit = ${1000*model.profit():,.2f}\")\nprint(f\"Batches of Doors = {model.x['drs']():1.2f}\")\nprint(f\"Batches of Windows = {model.x['wdw']():1.2f}\")","kernel":"python3","metadata":{"code_folding":[],"hidden":true},"no_halt":true,"output":{"0":{"name":"stdout","text":"Profit = $857,000.00\nBatches of Doors = 2.67\nBatches of Windows = 5.00\n"}},"pos":9,"start":1673904160854,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904163828,"exec_count":4,"id":"749641","input":"# execute this cell to see scipy.optimize.minimize documentation\nIFrame(\n    \"https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize\",\n    width=900,\n    height=600)","kernel":"python3","metadata":{"code_folding":[0],"hidden":true},"no_halt":true,"output":{"0":{"data":{"iframe":"d1c0a0d989233e39a28f5257875e627e8acfe485","text/plain":"<IPython.lib.display.IFrame at 0x7f8cce6b1ca0>"},"exec_count":4}},"pos":13,"scrolled":true,"start":1673904163809,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904164526,"exec_count":5,"id":"e60b27","input":"# plot p(x) on [-10,10]\nx = np.linspace(-10,10,201)\np = lambda x:x**4 + 2*x**3 + 3*x**2 + 2*x + 1 \nfig = plt.figure(figsize=(8,7)) # adjust figsize if needed\nplt.plot(x,p(x));\nplt.xlabel('x');\nplt.ylabel('y');\n# two notes here:\n# 1. lambda is useful for defining one-line functions\n# 2. if x is a numpy array then x**2 and so on makes sense ... it squares each element in the array\n#.   numpy arrays can be useful for streamlining computations","kernel":"python3","metadata":{"code_folding":[0],"hidden":true},"no_halt":true,"output":{"0":{"data":{"image/png":"346fcdc15be8426f99828f5a93c5569977ccdc5e","text/plain":"<Figure size 576x504 with 1 Axes>"},"metadata":{"image/png":{"height":424,"width":513}}}},"pos":17,"start":1673904163845,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904165161,"exec_count":6,"id":"c8c7c0","input":"# plot p(x) on [-3,3]\nx = np.linspace(-3,3,201)\np = lambda x:x**4 + 2*x**3 + 3*x**2 + 2*x + 1\nfig = plt.figure(figsize=(8,7))\nplt.plot(x,p(x));\nplt.xlabel('x');\nplt.ylabel('y');","kernel":"python3","metadata":{"code_folding":[0],"hidden":true},"no_halt":true,"output":{"0":{"data":{"image/png":"a8382bd0a52daf189cfef5be08be32e15ea5c05f","text/plain":"<Figure size 576x504 with 1 Axes>"},"metadata":{"image/png":{"height":424,"width":500}}}},"pos":19,"start":1673904164550,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904165183,"exec_count":7,"id":"5b6dd7","input":"# execute for local search\nresult = minimize(p,-2)\nresult","kernel":"python3","metadata":{"code_folding":[],"hidden":true},"no_halt":true,"output":{"0":{"data":{"text/plain":"      fun: 0.5625000000001792\n hess_inv: array([[0.33310385]])\n      jac: array([-1.00582838e-06])\n  message: 'Optimization terminated successfully.'\n     nfev: 14\n      nit: 6\n     njev: 7\n   status: 0\n  success: True\n        x: array([-0.50000035])"},"exec_count":7}},"pos":21,"start":1673904165172,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904165216,"exec_count":8,"id":"417825","input":"print(f\"The minimum value is {result.fun:0.4f} and occurs at x = {result.x[0]:0.4f}\")","kernel":"python3","metadata":{"hidden":true},"no_halt":true,"output":{"0":{"name":"stdout","text":"The minimum value is 0.5625 and occurs at x = -0.5000\n"}},"pos":22,"start":1673904165208,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904165233,"exec_count":9,"id":"812535","input":"result = minimize_scalar(p, bounds=(-3, 3), method='bounded')\n\nprint(f'The minimum value is {result.fun:0.4f} and occurs at x = {result.x:0.4f}')","kernel":"python3","no_halt":true,"output":{"0":{"name":"stdout","text":"The minimum value is 0.5625 and occurs at x = -0.5000\n"}},"pos":24,"start":1673904165223,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904165839,"exec_count":10,"id":"643d28","input":"# graph of profit function\nx = np.linspace(0,250,201) # 201 points between 0 and 250\nP = lambda x:-0.008*x**2 + 3.1*x - 80 # lambda is for writing one line functions\nfig = plt.figure(figsize=(8,7));\nplt.plot(x,P(x));\nplt.xlabel('apartments');\nplt.ylabel('profit (\\$ thousands)');","kernel":"python3","metadata":{"code_folding":[0],"hidden":true},"no_halt":true,"output":{"0":{"data":{"image/png":"5e65d5f594a75f00a7439d343ab3a1b854b6da42","text/plain":"<Figure size 576x504 with 1 Axes>"},"metadata":{"image/png":{"height":424,"width":502}}}},"pos":27,"start":1673904165241,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904165931,"exec_count":11,"id":"38da3f","input":"# add your code here, note you should be able to guess an initial value from the graph ...","kernel":"python3","metadata":{"hidden":true},"no_halt":true,"pos":28,"start":1673904165849,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904166313,"exec_count":12,"id":"be557d","input":"# execute this cell for video\nplay_video(\"ds775_lesson5-logistic-regression\")","kernel":"python3","no_halt":true,"output":{"0":{"data":{"iframe":"9eba3b5e85cc0b5827ff3f3abbc9d5a3e0da0493","text/plain":"<IPython.lib.display.IFrame at 0x7f8cdf6c3d00>"},"exec_count":12}},"pos":34,"start":1673904166241,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904166333,"exec_count":13,"id":"a1d5b6","input":"# data from \nx_hours = np.array([\n    0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 1.75, 2.0, 2.25, 2.50, 2.75, 3.00, 3.25,\n    3.5, 4.0, 4.25, 4.5, 4.75, 5.0, 5.5\n])\ny_passed = np.array([0,0,0,0,0,0,1,0,1,0,1,0,1,0,1,1,1,1,1,1])","kernel":"python3","no_halt":true,"pos":38,"start":1673904166330,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904167101,"exec_count":14,"id":"270b80","input":"# graph of data and sigmoid\nb0 = -4.07771657\nb1 = 1.5046468\nhours_studied = np.linspace(0,6,101)\ndef sigmoid(x,intercept,slope):\n    return( 1.0 / (1.0 + np.exp( -(intercept + slope * x) ) ) )\nprob_passed = sigmoid(hours_studied, b0, b1)\n\nfig = plt.figure();\nfig.set_size_inches(6,3.5); # change size if needed on your display\nax = fig.add_subplot(111);\nax.scatter(x_hours, y_passed);\nax.plot(hours_studied, prob_passed);\nax.set_xlabel('hours studied');\nax.set_ylabel('passed');","kernel":"python3","metadata":{"code_folding":[]},"no_halt":true,"output":{"0":{"data":{"image/png":"ca514b85bb716041fa8b6a55ade717141d5e6767","text/plain":"<Figure size 432x252 with 1 Axes>"},"metadata":{"image/png":{"height":234,"width":385}}}},"pos":40,"start":1673904166346,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904167115,"exec_count":15,"id":"f083b5","input":"print(f\"A student who studies 4 hours has approximately {100 * sigmoid(4, b0, b1):3.1f}% chance of passing.\")","kernel":"python3","no_halt":true,"output":{"0":{"name":"stdout","text":"A student who studies 4 hours has approximately 87.4% chance of passing.\n"}},"pos":42,"start":1673904167108,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904167388,"exec_count":16,"id":"43a6ea","input":"def neg_log_loss( coef, *args):\n    b0 = coef[0]\n    b1 = coef[1]\n    x = args[0]\n    y = args[1]\n    p = 1.0/(1.0 + np.exp(-(b0 + b1*x)))\n    ll = sum( y*np.log(p)+(1-y)*np.log(1-p) )\n    return(-ll) # here's the minus sign!","kernel":"python3","metadata":{"code_folding":[]},"no_halt":true,"pos":45,"start":1673904167128,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904167637,"exec_count":17,"id":"e1fb87","input":"result = minimize(neg_log_loss,[0,0],args=(x_hours,y_passed))\nresult","kernel":"python3","no_halt":true,"output":{"0":{"data":{"text/plain":"      fun: 8.029878464344703\n hess_inv: array([[ 3.06176005, -1.02900858],\n       [-1.02900858,  0.39490317]])\n      jac: array([2.38418579e-07, 3.57627869e-07])\n  message: 'Optimization terminated successfully.'\n     nfev: 42\n      nit: 12\n     njev: 14\n   status: 0\n  success: True\n        x: array([-4.07771302,  1.50464531])"},"exec_count":17}},"pos":47,"start":1673904167402,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904167888,"exec_count":18,"id":"069ff0","input":"b0 = result.x[0]\nb1 = result.x[1]\nprint(f\"The maximum likelihood estimate for p(x) has intercept b0 = {b0:2.3f} and slope b1 = {b1:2.3f}\")","kernel":"python3","no_halt":true,"output":{"0":{"name":"stdout","text":"The maximum likelihood estimate for p(x) has intercept b0 = -4.078 and slope b1 = 1.505\n"}},"pos":48,"start":1673904167644,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904168107,"exec_count":19,"id":"2fcd79","input":"model = LogisticRegression(C=1.0e10,fit_intercept = True)\nmodel.fit(x_hours.reshape(-1,1), y_passed)\nb0 = model.intercept_[0]\nb1 = model.coef_[0][0]\nprint(f\"The maximum likelihood estimate for p(x) has intercept b0 = {b0:2.3f} and slope b1 = {b1:2.3f}\")","kernel":"python3","no_halt":true,"output":{"0":{"name":"stdout","text":"The maximum likelihood estimate for p(x) has intercept b0 = -4.078 and slope b1 = 1.505\n"}},"pos":50,"start":1673904167894,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904168656,"exec_count":20,"id":"9f6754","input":"# open to reveal graph code\ndef rastrigin_1D(x):\n    return (x**2 + 10 - 10 * np.cos(2 * np.pi * x))\n\nx = np.linspace(-5.12,5.12,201)\ny = rastrigin_1D(x)\n\nfig = plt.figure(figsize=(5,5)) # adjust figsize as needed for your display\nplt.plot(x,y)\nplt.xlabel('x');\nplt.ylabel('y');","kernel":"python3","metadata":{"code_folding":[0]},"no_halt":true,"output":{"0":{"data":{"image/png":"2c89f826319a0c1a9314921c96101459992ea899","text/plain":"<Figure size 360x360 with 1 Axes>"},"metadata":{"image/png":{"height":316,"width":326}}}},"pos":53,"scrolled":true,"start":1673904168115,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904173711,"exec_count":21,"id":"96782c","input":"# Graph of Rastrigin with dimension n = 2\n%run scripts/rastrigin_2d.py","kernel":"python3","metadata":{"code_folding":[]},"no_halt":true,"output":{"0":{"data":{"iframe":"f991aa6015c69ed7ff00fcb1ee4977a8d7c21caa"}},"1":{"data":{"text/plain":"<Figure size 864x504 with 0 Axes>"}}},"pos":54,"start":1673904168667,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904173735,"exec_count":22,"id":"6117f2","input":"import numpy as np\ndef rastrigin(x):\n    # pass a single vector of length n to evaluate Rastrigin\n    # the length of the vector = number of input variables = dimension\n    return sum(x**2 + 10 - 10 * np.cos(2 * np.pi * x))","kernel":"python3","no_halt":true,"pos":56,"start":1673904173721,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904181818,"exec_count":23,"id":"da984d","input":"from scipy.optimize import minimize\nimport numpy as np\ndef rastrigin(x):\n    # pass a single vector of length n to evaluate Rastrigin\n    # the length of the vector = number of input variables = dimension\n    return sum(x**2 + 10 - 10 * np.cos(2 * np.pi * x))\n\n\n\ndim = 10 # this determines the number of inputs to the Rastrigin function, there are 10 inputs or variables with dim=10\nnum_local_searches = 1000\nbest_value = 1.e10\nbounds = [(-5.12,5.12) for i in range(dim)] # make a list of tuples to give bounds for each of the dim = 10 variables\n# bounds = [(-5.12,5.12)]*dim  # this is a nice trick for lists that does the same thing\n\nfor i in range(num_local_searches):\n    x_initial = np.random.uniform(-5.12, 5.12, dim)\n    result = minimize(rastrigin,x_initial,bounds=bounds)\n    if result.fun < best_value:\n        best_value = result.fun\n        best_x = result.x\n        print(f\"New best value is {best_value:1.3f}\")\n\nprint(f\"\\nThe smallest value found is {best_value:4.3f}\")\nprint(f\"The location where the smallest values occurs is:\")\nfor i in range(dim):\n    print(f\"    x{i} = {best_x[i]:1.3f}\")","kernel":"python3","no_halt":true,"output":{"0":{"name":"stdout","text":"New best value is 86.561\nNew best value is 73.627\nNew best value is 60.692\nNew best value is 48.753\nNew best value is 48.753\nNew best value is 37.808\nNew best value is 34.823\n"},"1":{"name":"stdout","text":"New best value is 24.874\n"},"2":{"name":"stdout","text":"New best value is 23.879\n"},"3":{"name":"stdout","text":"New best value is 22.884\n"},"4":{"name":"stdout","text":"New best value is 22.884\n"},"5":{"name":"stdout","text":"New best value is 20.894\n"},"6":{"name":"stdout","text":"New best value is 19.899\n"},"7":{"name":"stdout","text":"New best value is 15.919\n"},"8":{"name":"stdout","text":"\nThe smallest value found is 15.919\nThe location where the smallest values occurs is:\n    x0 = 1.990\n    x1 = -0.000\n    x2 = -1.990\n    x3 = -0.995\n    x4 = 0.995\n    x5 = 0.000\n    x6 = -0.000\n    x7 = 1.990\n    x8 = -0.995\n    x9 = -0.995\n"}},"pos":57,"start":1673904173751,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904181903,"exec_count":24,"id":"a518bd","input":"# execute this cell for video\nplay_video(\"ds775_lesson5-logistic-traveling-salesman-problem\")","kernel":"python3","metadata":{"hidden":true},"no_halt":true,"output":{"0":{"data":{"iframe":"5ccb2f6c026c54eeee413848783c59fa2a44cd96","text/plain":"<IPython.lib.display.IFrame at 0x7f8cdea14790>"},"exec_count":24}},"pos":76,"start":1673904181827,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904183196,"exec_count":25,"id":"acc6a4","input":"with open(\"data/Caps48.json\", \"r\") as tsp_data:\n    tsp = json.load(tsp_data)\ndistance_matrix = tsp[\"DistanceMatrix\"]\noptimal_tour = tsp[\"OptTour\"]\nopt_dist = tsp[\"OptDistance\"]/1000 # converted to kilometers\nxy = np.array(tsp[\"Coordinates\"])\n\ndef plot_tour(best_tour, xy_meters, best_dist, height, width):\n\n    meters_to_pxl = 0.0004374627441064968\n    intercept_x = 2.464\n    intercept_y = 1342.546\n    xy_pixels = np.zeros(xy_meters.shape)\n    xy_pixels[:,0] = meters_to_pxl * xy_meters[:,0] + intercept_x\n    xy_pixels[:,1] = -meters_to_pxl * xy_meters[:,1] + intercept_y\n\n    fig, ax = plt.subplots(1, 1, figsize=(height, width))\n    im = plt.imread('images/caps48.png')\n    implot = ax.imshow(im)\n    plt.setp(ax.get_xticklabels(), visible=False)\n    plt.setp(ax.get_yticklabels(), visible=False)\n    ax.tick_params(axis='both', which='both', length=0)\n\n    loop_tour = np.append(best_tour, best_tour[0])\n    ax.plot(xy_pixels[loop_tour, 0],\n            xy_pixels[loop_tour, 1],\n            c='b',\n            linewidth=1,\n            linestyle='-')\n    plt.title(f\"Best Distance {best_dist:.0f} km\")\n\nplot_tour(optimal_tour, xy, opt_dist, 9, 6) # change the height and width in the last two arguments as needed","kernel":"python3","metadata":{"code_folding":[]},"no_halt":true,"output":{"0":{"data":{"image/png":"d818ef35130deb4f9a5ab8dcc99936801b61f54a","text/plain":"<Figure size 648x432 with 1 Axes>"},"metadata":{"image/png":{"height":352,"width":516}}}},"pos":79,"start":1673904181911,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904183219,"exec_count":26,"id":"7f2f84","input":"# define move, objective, and local search functions\n\ndef sub_tour_reversal(tour):\n    # reverse a random tour segment\n    num_cities = len(tour)\n    i, j = np.sort(np.random.choice(num_cities, 2, replace=False))\n    return np.concatenate((tour[0:i], tour[j:-num_cities + i - 1:-1],\n                              tour[j + 1:num_cities]))\n\ndef tour_distance(tour, dist_mat):\n    distance = dist_mat[tour[-1]][tour[0]]\n    for gene1, gene2 in zip(tour[0:-1], tour[1:]):\n        distance += dist_mat[gene1][gene2]\n    return distance/1000 # convert to kilometers\n\ndef random_reversal_search(dist_mat, max_no_improve):\n    num_cities = len(dist_mat)\n    # starts from a random tour\n    current_tour = np.random.permutation(np.arange(num_cities)) # x0 CHANGE\n    current_dist = tour_distance(current_tour, dist_mat) # y = f(x0) CHANGE\n\n    # stop search if no better tour is found within max_no_improve iterations, can increase to eliminate crossovers\n    num_moves_no_improve = 0\n    iterations = 0\n    while (num_moves_no_improve < max_no_improve):\n        num_moves_no_improve += 1\n        iterations += 1  # just for tracking\n        new_tour = sub_tour_reversal(current_tour) # make a move, create x_new from x_curr CHANGE\n        new_dist = tour_distance(new_tour, dist_mat) # y_new = f(x_new) # CHANGE\n        new_weight = total weight of items represented in x_new \n        if new_dist > current_dist and new_weight <= 50: ### Add logic here that rejects overweight knapsacks\n            num_moves_no_improve = 0\n            current_tour = new_tour # accept the move if it's an improvement\n            current_dist = new_dist\n    return current_tour, current_dist, iterations","kernel":"python3","no_halt":true,"output":{"0":{"ename":"SyntaxError","evalue":"invalid syntax (480574088.py, line 30)","traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_2740/480574088.py\"\u001b[0;36m, line \u001b[0;32m30\u001b[0m\n\u001b[0;31m    new_weight = total weight of items represented in x_new\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}},"pos":81,"start":1673904183204,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904183669,"exec_count":27,"id":"cebd04","input":"best_tour, best_dist, iterations = random_reversal_search(distance_matrix, 200)\n\nprint(f\"The minimum distance found is {best_dist:.0f} after {iterations:d} iterations\")\n\nplot_tour(best_tour, xy, best_dist, 9, 6)","kernel":"python3","metadata":{"code_folding":[]},"no_halt":true,"output":{"0":{"ename":"NameError","evalue":"name 'random_reversal_search' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_2740/2897600054.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_tour\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_reversal_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The minimum distance found is {best_dist:.0f} after {iterations:d} iterations\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplot_tour\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_tour\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'random_reversal_search' is not defined"]}},"pos":83,"start":1673904183228,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904183715,"exec_count":28,"id":"202739","input":"def sub_tour_reversal_ij(tour, i, j):\n    # reverse the segment from city i to city j\n    n = len(tour)\n    return (np.concatenate((tour[0:i], tour[j:-n + i - 1:-1], tour[j + 1:n])))\n\n# 2-opt local search for TSP\ndef two_opt(dist_mat):\n    num_cities = len(dist_mat)\n    current_tour = np.random.permutation(np.arange(num_cities))\n    current_dist = tour_distance(current_tour, dist_mat)\n    best_tour = current_tour\n    best_dist = current_dist\n\n    improvement = True\n    iterations = 0\n    while improvement:\n        improvement = False\n        for i in range(num_cities - 1):\n            for j in range(i + 1, num_cities):\n                iterations += 1\n                new_tour = sub_tour_reversal_ij(best_tour, i, j)\n                new_dist = tour_distance(new_tour, dist_mat)\n                if new_dist < best_dist:\n                    best_tour = new_tour\n                    best_dist = new_dist\n                    improvement = True\n    return best_tour, best_dist, iterations\n\nbest_tour, best_dist, iterations = two_opt(distance_matrix)\n\nprint(f\"The minimum distance found is {best_dist:.0f} km after {iterations:d} iterations\")\n\nplot_tour(best_tour, xy, best_dist, 9, 6)","kernel":"python3","metadata":{"hidden":true},"no_halt":true,"output":{"0":{"ename":"NameError","evalue":"name 'tour_distance' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_2740/2386612478.py\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbest_tour\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mbest_tour\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtwo_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The minimum distance found is {best_dist:.0f} km after {iterations:d} iterations\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_2740/2386612478.py\u001b[0m in \u001b[0;36mtwo_opt\u001b[0;34m(dist_mat)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mnum_cities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mcurrent_tour\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_cities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcurrent_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtour_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_tour\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mbest_tour\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_tour\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mbest_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_dist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'tour_distance' is not defined"]}},"pos":86,"start":1673904183686,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904183751,"exec_count":29,"id":"aa7cf0","input":"import numpy as np\nimport pandas as pd\n\nnum_groups = 2\n\n#first we'll set up our values and our groups\nvalues = np.array([5,10,23,8])\n\n#let's start by just assigning every other item to a different group - these are the group labels\ngroups = np.array([0,1,0,1])\n\n#Numpy arrays can easily be tossed into dataframes for display\n# not needed for solution\ndf = pd.DataFrame({'values': values, 'groups': groups})\nprint('Our data at the start')\ndisplay(df)\n\n#we can loop over the data and total the sum for each group\nsums = np.array([ sum( values[ groups == i] ) for i in range(num_groups) ])\n\n# display the sums in a dataframe (not necessary for solution)\nvalue_sums_df = pd.DataFrame({'Group': np.arange(0,num_groups), 'Sum':sums})\ndisplay(value_sums_df)\n\n#compute the range of the value sums (will be >= 0)\nmaxdif = max(sums) - min(sums)\n\nprint(f'The maximum difference between groups is {maxdif}')","kernel":"python3","no_halt":true,"output":{"0":{"name":"stdout","text":"Our data at the start\n"},"1":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>values</th>\n      <th>groups</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>23</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   values  groups\n0       5       0\n1      10       1\n2      23       0\n3       8       1"}},"2":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Group</th>\n      <th>Sum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>18</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   Group  Sum\n0      0   28\n1      1   18"}},"3":{"name":"stdout","text":"The maximum difference between groups is 10\n"}},"pos":89,"start":1673904183725,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904183765,"exec_count":30,"id":"3f8153","input":"# the objective function\n\ndef group_fitness(groups,num_groups,values):\n    # groups must be a numpy array for this to work\n    sums = np.array([ sum( values[ groups == i] ) for i in range(num_groups) ])\n    return max(sums)-min(sums)\n\nprint(f'The current group fitness is {group_fitness(np.asarray([0,1,0,1]),2,values)}')","kernel":"python3","no_halt":true,"output":{"0":{"name":"stdout","text":"The current group fitness is 10\n"}},"pos":91,"start":1673904183759,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904183848,"exec_count":31,"id":"e7487c","input":"# the move function\n\ndef change_group(groups, num_groups, debug=False):\n    #get the unique groups\n    choices = np.arange(0,num_groups)\n    #get a copy of the groups\n    new_groups = groups.copy()\n    #select item to change\n    switch = np.random.randint(0, groups.shape[0])\n    #select new group value\n    new_group = np.random.choice(choices)\n    while groups[switch] == new_group:\n        new_group = np.random.choice(choices)\n    new_groups[switch] = new_group\n    if debug:\n        print(f'The item at {switch} should change to {new_group}')\n        print(f'The initial groups are: {groups} and the changed groups are {new_groups}')\n    return new_groups\n\ngroups = np.array([0,1,0,1])\n\nprint(f'The new group is {change_group(groups, num_groups, debug=True)}')","kernel":"python3","no_halt":true,"output":{"0":{"name":"stdout","text":"The item at 1 should change to 0\nThe initial groups are: [0 1 0 1] and the changed groups are [0 0 0 1]\nThe new group is [0 0 0 1]\n"}},"pos":93,"start":1673904183818,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904183870,"exec_count":32,"id":"f42f86","input":"# generate data for 1000 item / 4 group problem\n\n# the weird construction here guarantees that 4 groups can be perfectly balanced so the global min is zero, the algorithm probably won't find exactly the global min though\nimport numpy as np\n\nnp.random.seed(123)\n\ntot_num_items = 1000 # should be divisible by 4\nnum_items = int(tot_num_items / 4)\nnum_groups = 4\n\nvalues = np.random.randint(10,100,size=num_items)\nvalues = np.hstack([values,values,values,values]) # values of all items\ngroups = np.random.randint(num_groups,size=tot_num_items) # initial assignment of items to groups\n\nnp.random.seed()\n","kernel":"python3","no_halt":true,"pos":96,"start":1673904183858,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904183912,"exec_count":33,"id":"b9722c","input":"def sub_tour_reversal(tour):\n    # reverse a random tour segment\n    num_cities = len(tour)\n    i, j = np.sort(np.random.choice(num_cities, 2, replace=False))\n    return np.concatenate((tour[0:i], tour[j:-num_cities + i - 1:-1],\n                              tour[j + 1:num_cities]))\n\ndef tour_distance(tour, dist_mat):\n    distance = dist_mat[tour[-1]][tour[0]]\n    for gene1, gene2 in zip(tour[0:-1], tour[1:]):\n        distance += dist_mat[gene1][gene2]\n    return distance/1000 # convert to kilometers","kernel":"python3","no_halt":true,"pos":100,"start":1673904183878,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904184345,"exec_count":34,"id":"53c35e","input":"from locsearch import LocalSearcher\n\nclass TravelingSalesmanProblem(LocalSearcher):\n    \"\"\"\n    Test local search with a traveling salesman problem\n    \"\"\"\n    \n    # pass extra data (the distance matrix) into the constructor\n    def __init__(self, state, distance_matrix):\n        self.distance_matrix = distance_matrix\n        \n        \n        # important - super() calls the constructor of the parent class\n        super(TravelingSalesmanProblem, self).__init__(state)\n\n    def move(self):\n        self.state = sub_tour_reversal(self.state)\n\n    def objective(self):\n        return tour_distance(self.state,self.distance_matrix)","kernel":"python3","no_halt":true,"pos":102,"start":1673904183930,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904184646,"exec_count":35,"id":"b70a06","input":"# read problem data\nwith open(\"data/Caps48.json\", \"r\") as tsp_data:\n    tsp = json.load(tsp_data)\ndistance_matrix = tsp[\"DistanceMatrix\"]\n\n# create initial state\nnum_cities = len(distance_matrix)\ninit_tour = np.random.permutation(np.arange(num_cities))\n\n# create a tsp object of the TravelingSalesmanProblem class\ntsp = TravelingSalesmanProblem(init_tour, distance_matrix)\n\n# call the local search method in our object to do the search\nbest_tour, best_dist = tsp.localsearch()","kernel":"python3","no_halt":true,"output":{"0":{"name":"stdout","text":"\n Obj Fun Val | Iterations\n    53448.76 | 100\n    43131.46 | 200\n    37602.73 | 300\n    35456.32 | 400\n    32582.70 | 500\n    30814.88 | 600\n    28665.06 | 700\n    26812.37 | 800\n    26651.47 | 900\n    26014.36 | 1000\n    24620.38 | 1100\n    23919.68 | 1200\n    23262.46 | 1300\n    22518.35 | 1400\n    22514.99 | 1500\n    21434.06 | 1600\n    20456.37 | 1700\n    20252.27 | 1800\n    20168.36 | 1900\n    20153.96 | 2000\n    19736.84 | 2100\n    19736.84 | 2200\n    19705.33 | 2300\n    19705.33 | 2400\n    19691.99 | 2500\n    19646.47 | 2600\n    19646.47 | 2700\n    19646.47 | 2800\n    19646.47 | 2900\n    19646.47 | 3000\n    19646.47 | 3100\n    19646.47 | 3200\n    19499.33 | 3300\n    18526.63 | 3400\n    18526.63 | 3500\n    18526.63 | 3600\n    18526.63 | 3700\n    18526.63 | 3800\n    18526.63 | 3900\n    18526.63 | 4000\n"},"1":{"name":"stdout","text":"    18526.63 | 4100\n    18526.63 | 4200\n    18526.63 | 4300\n    18526.63 | 4365\n"}},"pos":104,"start":1673904184417,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904184844,"exec_count":36,"id":"cb4540","input":"# read problem data\nwith open(\"data/Caps48.json\", \"r\") as tsp_data:\n    tsp = json.load(tsp_data)\ndistance_matrix = tsp[\"DistanceMatrix\"]\n\n# create initial state\nnum_cities = len(distance_matrix)\ninit_tour = np.random.permutation(np.arange(num_cities))\n\n# create a tsp object of the TravelingSalesmanProblem class\ntsp = TravelingSalesmanProblem(init_tour, distance_matrix)\n\n# override default settings\ntsp.max_no_improve = 300\ntsp.update_iter = 200\n\n# call the local search method in our object to do the search\nbest_tour, best_dist = tsp.localsearch()","kernel":"python3","no_halt":true,"output":{"0":{"name":"stdout","text":"\n Obj Fun Val | Iterations\n    40916.39 | 200\n    32589.90 | 400\n    27592.07 | 600\n    25998.00 | 800\n    24493.90 | 1000\n    24456.67 | 1200\n    22655.76 | 1400\n    22129.86 | 1600\n    20849.98 | 1800\n    20345.92 | 2000\n"},"1":{"name":"stdout","text":"    20345.92 | 2200\n    20345.92 | 2267\n"}},"pos":106,"start":1673904184655,"state":"done","type":"cell"}
{"cell_type":"code","end":1673904185410,"exec_count":37,"id":"fe7c03","input":"def sub_tour_reversal(tour):\n    # reverse a random tour segment\n    num_cities = len(tour)\n    i, j = np.sort(np.random.choice(num_cities, 2, replace=False))\n    return np.concatenate((tour[0:i], tour[j:-num_cities + i - 1:-1],\n                              tour[j + 1:num_cities]))\n\ndef tour_distance(tour, dist_mat):\n    distance = dist_mat[tour[-1]][tour[0]]\n    for gene1, gene2 in zip(tour[0:-1], tour[1:]):\n        distance += dist_mat[gene1][gene2]\n    return distance/1000 # convert to kilometers\n\nfrom locsearch import LocalSearcher\n\nclass TravelingSalesmanProblem(LocalSearcher):\n    \"\"\"\n    Test local search with a traveling salesman problem\n    \"\"\"\n    \n    # pass extra data (the distance matrix) into the constructor\n    def __init__(self, state, distance_matrix):\n        self.distance_matrix = distance_matrix\n        super(TravelingSalesmanProblem, self).__init__(state)  # important!\n        \n    def move(self):\n        self.state = sub_tour_reversal(self.state)\n        \n    def objective(self):\n        return tour_distance(self.state,self.distance_matrix)\n    \n# read problem data\nimport json\nwith open(\"data/Caps48.json\", \"r\") as tsp_data:\n    tsp = json.load(tsp_data)\ndistance_matrix = tsp[\"DistanceMatrix\"]\n\n# create initial state\nnum_cities = len(distance_matrix)\ninit_tour = np.random.permutation(np.arange(num_cities))\n\n# create a tsp object of the TravelingSalesmanProblem class\ntsp = TravelingSalesmanProblem(init_tour, distance_matrix)\n\n# uncomment to override default search and output settings\n# tsp.max_no_improve = 300\ntsp.update_iter = 500\n\n# call the local search method in our object to do the search\nbest_tour, best_dist = tsp.localsearch()","kernel":"python3","no_halt":true,"output":{"0":{"name":"stdout","text":"\n Obj Fun Val | Iterations\n"},"1":{"name":"stdout","text":"    27588.35 | 500\n    22638.32 | 1000\n"},"2":{"name":"stdout","text":"    20883.54 | 1500\n    20196.55 | 2000\n    20001.66 | 2500\n"},"3":{"name":"stdout","text":"    19803.21 | 3000\n    19392.22 | 3500\n"},"4":{"name":"stdout","text":"    19099.59 | 4000\n    19099.59 | 4500\n    18838.49 | 5000\n    18834.59 | 5500\n"},"5":{"name":"stdout","text":"    18432.43 | 6000\n    18432.43 | 6500\n    18218.11 | 7000\n"},"6":{"name":"stdout","text":"    17924.83 | 7500\n"},"7":{"name":"stdout","text":"    17924.83 | 8000\n    17924.83 | 8071\n"}},"pos":108,"start":1673904184857,"state":"done","type":"cell"}
{"cell_type":"code","id":"7825f3","input":"","pos":109,"type":"cell"}
{"cell_type":"markdown","id":"015c3b","input":"**Note: dimensions or `dim` is the number of input variables to the objective function.**\n\n### *Self-Assessment: Rastrigin with dim = 3, 4*","pos":58,"type":"cell"}
{"cell_type":"markdown","id":"02a151","input":"The overall \"U\" shape is not surprising since for polynomials the behavior for large values of $x$ is determined by the highest degree term which is, in this case, $x^4$.  It appears that there is a minimum or minima close to the origin.  Let's zoom in a bit to see what we can:","metadata":{"hidden":true},"pos":18,"type":"cell"}
{"cell_type":"markdown","id":"09d744","input":"### Find the model with `minimize`","pos":44,"type":"cell"}
{"cell_type":"markdown","id":"0a9f17","input":"## Minimize_scalar from scipy.optimize\n\nFor functions of a single variable we can also use the minimize scalar function as below.  In some cases you may need to use this instead of `minimize` which applies to multivariable functions.","metadata":{"heading_collapsed":true},"pos":23,"type":"cell"}
{"cell_type":"markdown","id":"0d88d2","input":"## An Object-Oriented Approach to Local Search","pos":97,"type":"cell"}
{"cell_type":"markdown","id":"0f7d97","input":"Your answer shouldn't be an integer.  We could use discrete optimization and only optimize integer numbers of apartments, but it will usually be more computationally intensive.  Instead we're using a continuous variable to get an approximation to the discrete problem, this is called **relaxation** (we've relaxed the integer variable condition).  So what whole number of apartments should you rent?  Why? ","metadata":{"hidden":true},"pos":29,"type":"cell"}
{"cell_type":"markdown","id":"102525","input":"# Lesson 05: Local Optimization\n\n**Students find Lessons 5, 6, and 7 to be the most difficult to follow.  I'd love to improve these lessons and would really like your feedback and suggestions in Piazza.  Please don't hesitate to ask lots of questions.**","pos":1,"type":"cell"}
{"cell_type":"markdown","id":"11db09","input":"*Note:  It isn't important to understand the details of logistic regression for this class, but the text in this cell gives a bit of background.*\n\nWhere do those values for the slope and intercept come from?  To obtain those we find the values of $b_0$ and $b_1$ that maximize the likelihood function:\n$$ L(b_0,b_1) = \\prod_{i=1}^{n} p(x_i)^{y_i} (1-p(x_i))^{(1-y_i)}$$\nwhere $(x_i,y_i)$ are the data pairs for each student and $p(x) = \\displaystyle \\frac{1}{1 + e^{-(b_0 + b_1 x)}}$ is the sigmoid function.  By maximizing the likelihood function we are maximizing the probability that this model produced the observed data.  Note that the $\\prod$ symbol means to take the product of the values, like $\\sum$ means to take the sum.\n\nIn practice, maximizing a product can lead to numerical difficulties, so we instead maximize the log-likelihood function found by taking the logarithm of $L(b_0,b_1)$ to get:\n$$LL(b_0, b_1) = \\sum_{i = 1}^{n} \\left[ y_i \\log( p(x_i) ) + (1-y_i) \\log(1-p(x_i)) \\right].$$\n\nNow we need to find $b_0$ and $b_1$ to maximize this.  The log-likelihood function turns out to be concave so that ascending from any starting point will lead to the global maximum.  \n\nBecause we will use the `minimize` function from `scipy.optimize` to find the maximum log-likelihood we'll minimize the negative log-likelihood:","pos":43,"type":"cell"}
{"cell_type":"markdown","id":"1250f3","input":"The function $f(x) = x^5 - x^4 - 18 x^3 + 16 x^2 + 32 x - 2$ for $-4 \\leq x \\leq 3.6$ appears in the video \"Gradient Descent and Local Minima\" above.  Plot the function on the given interval.  Use the graph to guess where the local maxima and minima are.  Now use minimize from scipy.optimize to find the $x$ and $y$ coordinates of all the extrema.  You can add bounds to the minimize call like this:\n\n```python\nminimize(f, x0, bounds=[(-4,3.6)])\n```\nLocal extrema only occur on the interior of the interval and not at the endpoints.","metadata":{"hidden":true},"pos":31,"type":"cell"}
{"cell_type":"markdown","id":"12a957","input":"## Minimize from scipy.optimize","metadata":{"heading_collapsed":true},"pos":12,"type":"cell"}
{"cell_type":"markdown","id":"149a6b","input":"We'll use Pyomo to solve the quadratic variation of the Wyndor problem illustrated in Figure 13.6 on page 554.\n\n<img src=\"images/wyndor_quad.png\" width=\"600\">\n\nThis kind of profit function occurs when the price depends on demand.  We'll use a concrete model formulation for simplicity.  Note, the Pyomo package `minimize` conflicts with the `minimize` from `scipy.optimize` we're using elsewhere in this notebook, so we'll import `pyomo` a bit differently here than usual.  Alternately we could re-import `minimize` from `scipy.optimize` after we're done with Pyomo.","metadata":{"hidden":true},"pos":8,"type":"cell"}
{"cell_type":"markdown","id":"1cbe9d","input":"While there are many options here, the defaults will serve well for our purposes.  The `method` specifies a variety of different numerical algorithms for local search.  We'll use `BFGS` for unbounded problems and `L-BFGS-B` for problems with bounds.  You shouldn't have to specify that choice as those are the defaults.  The BFGS methods are robust algorithms and are known as quasi-Newton methods.  What this means is that they approximate the shape of the objective function near the current point by approximating the derivatives (slopes and curvature) of the function and that shape information is used to produce an improved search point. \n\nOne of the things to pay attention to here is how we have to write our objective functions so that they can be passed to the `minimize` function.","metadata":{"hidden":true},"pos":14,"type":"cell"}
{"cell_type":"markdown","id":"1d0ba0","input":"First, we import the `LocalSearcher` class from the `locsearch` package.\n```python\nfrom locsearch import LocalSearcher\n```\n\nIf you want to see all the details have a look at `localsearch/ls.py` in the same directory as this notebook.\n\nNow we define a new class called `TravelingSalesmanProblem` that inherits from the `LocalSearcher` class:\n\n```python\nclass TravelingSalesmanProblem(LocalSearcher):\n    \"\"\"\n    Test local search with a traveling salesman problem\n    \"\"\"\n```\n\nFor the TSP we'll need to provide the distance matrix as an extra input to the objective function.  We need to tell the `__init__` constructor that we're going to input an extra argument in addition to the initial state. The `super()` command tells Python to use the `__init__` constructor that is inherited from the parent class `LocalSearcher` to set self.state:\n```python\n# pass extra data (the distance matrix) into the constructor\ndef __init__(self, state, distance_matrix):\n    self.distance_matrix = distance_matrix\n    super(TravelingSalesmanProblem, self).__init__(state)\n```\nIt's important to include the `super()` command and also to add the additional arguments after `self` and `state` in the arguments to `__init__`.  If you don't need to pass arguments other than `state` to your objective function then you can leave out the whole `__init__` constructor from your subclass definition.\n\nNow we have to define the `move()` method.  The parent class has a `move()` method that is inherited by our `TravelingSalesmanProblem` class, but it doesn't actually do anything (this is Polymorphism in action).  To create our `move()` method we simply call our `sub_tour_reversal()` to create a new tour.\n```python\ndef move(self):\n    self.state = sub_tour_reversal(self.state)\n```\n\nYou could also include all the code to compute the new tour into the `move()` method, but it's easier to call the `sub_tour_reversal()` function we defined already.\n\nFinally, we have to define our objective function.  There is an `objective()` method inherited from the parent `LocalSearcher` class, but it doesn't do anything.  To build our own we just call the already defined `tour_distance()` function.  Notice that `tour_distance()` needs two arguments:  a tour which is stored in `self.state` and the distance matrix which is stored in `self.distance_matrix` because we told our `__init__` constructor to add that to our object.\n\n```python\ndef objective(self):\n    return tour_distance(self.state,self.distance_matrix)\n```\n\nSo far we haven't actually created an object of the `TravelingSalesmanProblem` class, instead we've just defined the class.\n\n**NOTE:  it's important that you store the decision variables in self.state and not somewhere else.  The search method expects the variables to be in self.state.**\n\nThe next steps are to read the problem data, create an initial state, create the TSP object, and do the optimization.  These steps are shown in the code in the next cell.","pos":103,"type":"cell"}
{"cell_type":"markdown","id":"220074","input":"For the Rastrigin function write a while loop that runs until the global minimum value is found ($|\\mbox{best_val}|<0.01$) and track the number of iterations. Use a for loop to repeat this three times until your code is debugged.  After your code is working, repeat the process 100 times and report the average number of searches until the global minimum is found when $n=1,2,3$.  Are these numbers in approximate agreement with with the estimated numbers $10.28^n$ (they very likely won't be all that close, but how is the overall trend)?","metadata":{"hidden":true},"pos":65,"type":"cell"}
{"cell_type":"markdown","id":"249160","input":"**Execute cell below to do the local search.  Repeat a few times to see how each local search results in a different tour corresponding to a different local minimum.**","pos":82,"type":"cell"}
{"cell_type":"markdown","id":"26b6b6","input":"## The Basics of Local Search","pos":70,"type":"cell"}
{"cell_type":"markdown","id":"275528","input":"# Quadratic Programming","metadata":{"heading_collapsed":true},"pos":5,"type":"cell"}
{"cell_type":"markdown","id":"2c98e3","input":"# Local Search - Discrete Variables","pos":68,"type":"cell"}
{"cell_type":"markdown","id":"41a197","input":"We'll have a look at an algorithm called \"2-opt\" that was proposed by Croes in 1958.  We won't focus on it much since the idea doesn't really extend to other problems.  The main idea  is to reverse segments that intersect themselves to remove the crossover.  We loop repeatedly over all the possible reversals until there are no more crossovers.  Here is some Python to do 2-opt:","metadata":{"hidden":true},"pos":85,"type":"cell"}
{"cell_type":"markdown","id":"42bb30","input":"2-opt is can use fewer iterations to find a reasonable tour (local minimum) than does our local search with random segment reversals.  2-opt guarantees that there are no \"crossovers\" in the final tour.  In the next lesson, we'll try using 2-opt to find starting points for a global search algorithm.","metadata":{"hidden":true},"pos":87,"type":"cell"}
{"cell_type":"markdown","id":"431787","input":"Work your way through the embedded storybook below to learn some basic ideas about optimization.  This material complements the material in the textbook.  The graphs and demos in Slides 9-11 are available in the separate file Graphs_for_Video.ipynb in the Extras folder.","metadata":{"hidden":true},"pos":3,"type":"cell"}
{"cell_type":"markdown","id":"469f6b","input":"Most machine learning algorithms are driven by optimization.  Usually we want to minimize a loss function which measures the difference between the model predictions and the observed data.  Neural network training uses a version of the gradient descent algorithm to optimize the weights in the network.  Here we'll show how to fit a logistic regression model by maximizing a function.\n\nIn simple logistic regression we try to predict the value of the label $y$, which can be 0 or 1, for each value of a continuous predictor variable $x$.  In particular, the conditional probability that $y=1$ given the current value of $x$ is modeled by a sigmoid function (\"s\" curve) $$p(x) = \\frac{1}{1 + e^{-(b_0 + b_1 x)}}.$$\n\nFor example, the more hours a student studies to prepare for an exam, the higher the probability that they will pass the test.  Shown below is some data.  For each student we have the number of hours they studied and whether or not they passed the exam (1 for passed, 0 for failed).  This example data comes from the <a href=\"https://en.wikipedia.org/wiki/Logistic_regression\">Wikipedia article on Logistic Regresssion</a>.","pos":36,"type":"cell"}
{"cell_type":"markdown","id":"4fd60a","input":"### Setup for Logistic Regression","pos":35,"type":"cell"}
{"cell_type":"markdown","id":"52dd99","input":"###  *Self Assessment:  Minimize to Maximize*","metadata":{"code_folding":[],"hidden":true},"pos":25,"type":"cell"}
{"cell_type":"markdown","id":"5451f0","input":"Let's see what this looks like for the TSP.  We'll use the subtour reversal algorithm, described in the textbook, to generate moves.  Here's what the local search looks like for the TSP:\n\n```\n choose a random tour  \n while shorter tours have been found in last max_tries\n     propose new tour with one random segment reversed and compute new distance\n     if acceptable (it will always be a valid tour) \n         if new shortest tour \n             remember it \n         else\n             reject new tour\n endwhile \n ```","metadata":{"hidden":true},"pos":74,"type":"cell"}
{"cell_type":"markdown","id":"54ae90","input":"### *Self-Assessment:  How many searches?*","metadata":{"hidden":true},"pos":64,"type":"cell"}
{"cell_type":"markdown","id":"57cf33","input":"Local search algorithms for continuous variables are generally based on approximating the objective function near the current search point, then using that approximation to compute an improved search point.  For instance if we can calculate the gradient (calculus) or approximate it, then a move along the gradient direction will increase the value of the function.  \n\nWe'll primarily use the `scipy.optimize` function `minimize` for local search on continuous functions.  You can read more about it below.  ","pos":11,"type":"cell"}
{"cell_type":"markdown","id":"6053b1","input":"The Rastrigin function is a common test case for optimization algorithms because it has many local minima.  The definition of the function is \n$$f(\\mathbf{x})=10 n+\\sum_{i=1}^{n}\\left[x_{i}^{2}-A \\cos \\left(2 \\pi x_{i}\\right)\\right]$$\n\n$$f(\\mathbf{x})=\\sum_{i=1}^{n}\\left[x_{i}^{2} + 10-A \\cos \\left(2 \\pi x_{i}\\right)\\right]$$\n\n\nWhere $n$ is the dimensionality of input vector $\\mathbf{x}$.  For instance if $n=2$ then $\\mathbf{x} = (x_1, x_2)$.  The domain is restricted so that each $x_i \\in [-5.12, 5.12].$ .   Here is a graph of the the Rastrigin function with dimension $n=1.$","pos":52,"type":"cell"}
{"cell_type":"markdown","id":"6178f6","input":"### An example","pos":37,"type":"cell"}
{"cell_type":"markdown","id":"62589f","input":"## Another local search algorithm for TSP","metadata":{"heading_collapsed":true},"pos":84,"type":"cell"}
{"cell_type":"markdown","id":"634678","input":"### Video Walkthrough of this example","pos":33,"type":"cell"}
{"cell_type":"markdown","id":"663c1f","input":"## Example:  The Rastrigin Function","pos":51,"type":"cell"}
{"cell_type":"markdown","id":"66776c","input":"If we are maximizing a function of one variable, $f(x)$, we might choose to use 10 starting points.  For a function of two variables, $g(x,y)$ to get the same search power we would choose 10 points in the $x$ direction and 10 points in the $y$ direction to make a grid of $10^2 = 100$ starting points in the $xy$-plane.  For three variables we need $10^3 = 1000$ points in $xyz$-space.  For a function of $n$ variables we would need $10^n$ starting points. ($n$ is the same as `dim` in the code above.)\n\n*The volume of the search space grows exponentially with the number of variables or dimensionality of the problem.*\n\nThis is called the curse of dimensionality.  For high dimensional functions like those that occur in training neural networks and other applications with many local minima it can be very difficult to find the global minima because the volume of the search space grows exponentially with the number of variables.\n\nFor the Rastrigin function to find the global minimum you need an initial starting point in the interval (-0.5,0.5) in each dimension.  The search interval is [-5.12,5.12] in each dimension.  Thus the probability that a single uniformly sampled point in [-5.12,5.12] is $\\frac{1}{10.24} \\approx 0.0977$ (the ratio of the lengths of the two intervals).  The probability of finding the global minimum using local search from a uniformly sampled point in $n$ dimensions is $$\\left( \\frac{1}{10.24} \\right)^n.$$  That means we'd have to, on average, start $10.24^n$ local searches from uniformly sampled points to find the global minimum once.","metadata":{"hidden":true},"pos":63,"type":"cell"}
{"cell_type":"markdown","id":"67be48","input":"Note:  the approach outlined here for logisitic regression is very similar to the actual algorithms used by most software for computing logistic regression models. Many machine learning predictive models are trained by optimization.  To verify our results we check our results against those from Sci-kit Learn.  By default sklearn uses an L2 regularization term to avoid overfitting (more about this in DS740).  The amount of regularization is proportional to $1/C$ so we just use a huge $C$ to mimic no regularization.","pos":49,"type":"cell"}
{"cell_type":"markdown","id":"6c0d95","input":"Now we need to create an object of the `LocalSearcher` class.  We'll do that in the cell below and then explain each part:","pos":101,"type":"cell"}
{"cell_type":"markdown","id":"6e239a","input":"We can condense the above code into a single function that returns the fitness (the number we'll be minimizing) of our current grouping.\n","pos":90,"type":"cell"}
{"cell_type":"markdown","id":"73eb3a","input":"## Video for TSP Local Search Code","metadata":{"heading_collapsed":true},"pos":75,"type":"cell"}
{"cell_type":"markdown","id":"74635e","input":"## Wyndor Example","metadata":{"hidden":true},"pos":7,"type":"cell"}
{"cell_type":"markdown","id":"75dc54","input":"Here is pseudo-code for a simple local search.  Many variations are possible, but they often look like this:\n```\n set starting state \n while local_condition \n     select a move \n     if acceptable \n         do the move \n         if new optimum \n             remember it \n endwhile \n ```","pos":71,"type":"cell"}
{"cell_type":"markdown","id":"76e10c","input":"That sure seemed like a lot of work, but notice that all the messy bookkeeping details of the local search are inherited from the `LocalSearcher` class so that we don't have to pay attention to the details.  Instead, we can focus on just the essence of the problem - the objective function and a procedure for generating a local search step or move.  You can change default values of search and settings variables too.  For example, suppose we wanted to stop the search after making no progress for 300 iterations and wanted to print output every 200 iterations:","pos":105,"type":"cell"}
{"cell_type":"markdown","id":"7fcb28","input":"## Local Search for TSP","metadata":{"heading_collapsed":true},"pos":73,"type":"cell"}
{"cell_type":"markdown","id":"80fbda","input":"Optimization with discrete variables tends to be more complicated than with continuous variables.  In the continuous case we can take advantage of calculus or numerical methods to compute gradient search directions that allow us to move to nearby points that are closer to optimal.  However with discrete random variables there is no generic way to compute better nearby points.  Often the best we can do is find nearby points, which is usually problem specific, and try them to see if they produce closer to optimal results.\n\nWe'll look carefully at the traveling salesman problem (TSP).  In addition to the information about the TSP in the textbook, there is copious information available on the internet.","pos":69,"type":"cell"}
{"cell_type":"markdown","id":"86e7ad","input":"Below is a graph of the data along with the graph of the fitted sigmoid function that models the probality of $y=1$ at each $x$.  Don't worry, we'll see where the fitted curve comes from in a bit.","pos":39,"type":"cell"}
{"cell_type":"markdown","id":"8772cf","input":"## The curse of dimensionality","metadata":{"heading_collapsed":true},"pos":62,"type":"cell"}
{"cell_type":"markdown","id":"8a1ce4","input":"### A univariate function","metadata":{"hidden":true},"pos":15,"type":"cell"}
{"cell_type":"markdown","id":"8f85fb","input":"Take a minute to see how this function is structured and perhaps glance at the documentation again.  `coef` is a one-dimensional array with shape (n,) that contains all $n$ optimization variables.  In this case there are two which we assign to $b_0$ and $b_1$.  `*args` is a pointer to tuple `args` that contains any additional parameters that should be passed to the function.  In the case `minimize` will be passing the tuple $(x,y)$ that contains the training data.  We'll pass `args = (x_hours, y_passed)`.","pos":46,"type":"cell"}
{"cell_type":"markdown","id":"9167e7","input":"# Optimization Basics (video)","metadata":{"heading_collapsed":true},"pos":2,"type":"cell"}
{"cell_type":"markdown","id":"943903","input":"### *Self Assessment: Value Balancing Local Search*","pos":94,"type":"cell"}
{"cell_type":"markdown","id":"98c44a","input":"Lets investigate the fourth degree polynomial $$p(x) = x^4 + 2 x^3 + 3 x^2 + 2 x + 1.$$  Let's graph it to get an idea of the behavior. ","metadata":{"hidden":true},"pos":16,"type":"cell"}
{"cell_type":"markdown","id":"9a81a6","input":"Notes:\n\n* Often the starting state is one selected at random.  State refers to the \"state\" or values of the variables.\n\n* The local condition is a stopping condition.  It could be something like stopping after a fixed number of iterations or stopping after making no or insignificant process for a while.\n\n* Selecting a move is where things get problem specific.  Often the move involves a random change to the variables.\n\n* If acceptable means that we are checking to see that the state is feasible, that is, does it satisfy the constraints?","pos":72,"type":"cell"}
{"cell_type":"markdown","id":"9dd365","input":"You should read about quadratic programming in the textbook.  In short, the constraints are the same as they are in linear programming and the objective function can have degree 2 and interaction terms.  In Pyomo it is only a matter of changing the solver to one capable of solving quadratic programs, `ipopt`, instead of `glpk`.  Other solvers like CPLEX, a commercial solver, could also be used.  You  may need to install `ipopt` using conda on your own machine.","metadata":{"hidden":true},"pos":6,"type":"cell"}
{"cell_type":"markdown","id":"9e84e8","input":"## Example: Simple Logistic Regression","metadata":{"code_folding":[]},"pos":32,"type":"cell"}
{"cell_type":"markdown","id":"ab6278","input":"Since the framework for local search is independent of the particular task, we'll take advantage of Abstraction to hide the details of the local search algorithm.  Here is the local search method as it's implemented in the `LocalSearcher` class:\n\n```python\ndef localsearch(self):\n    \"\"\"Minimizes the objective function value by local search.\n\n    Parameters\n    state : an initial arrangement of the system\n\n    Returns\n    (state, objective): the best state and objective function value found.\n    \"\"\"\n    self.iterations = 0\n    num_moves_no_improve = 0\n\n    # set initial state and compute initial objective value\n    self.best_x = self.copy_state(self.state)\n    self.best_f = self.objective()\n    self.update() # output to screen\n\n    # main local search loop\n    while num_moves_no_improve < self.max_no_improve:\n        num_moves_no_improve += 1\n        self.iterations += 1\n        curr_state = self.copy_state(self.state)\n        self.move() # stores new state with move in self.state\n        new_f = self.objective()\n        if new_f < self.best_f:\n            num_moves_no_improve = 0\n            self.best_x = self.copy_state(self.state)\n            self.best_f = new_f\n        else: # if move not improvement reset state to curr_state\n            self.state = self.copy_state(curr_state)\n        if( self.iterations % self.update_iter == 0): # output every update_iter iterations\n            self.update() # print output\n\n    # output one last time for final iteration\n    self.update()\n\n    # Return best state and energy\n    return self.best_x, self.best_f\n```\n\nNotice that this method works on the object itself.  We don't pass in an initial state or other data.  Instead, we'll make a subclass of `LocalSearcher` that inherits the localsearch method. Then we'll set the initial state and other problem data when we create an instance of our subclass.\n\nWe'll also have to define two methods in our subclass:  `objective()` and `move()`.  The first contains the objective function to be minimized and the second contains the details for creating a local change, or move, to the current state (values of the decision variables that are stored in self.state).  If our objective function requires more than just the decision variables, we'll also have to add that to the subclass by defining an `__init__` constructor in the subclass.\n\nTo solve a TSP problem we'll use the `sub_tour_reversal` and `tour_distance()` functions from above to define our `move()` and `objective()` methods respectively:","pos":99,"type":"cell"}
{"cell_type":"markdown","id":"ad1aaf","input":"We'll be minimizing the total length of a tour that visits all 48 state capitals in the continental United States and ends back in the same city in which it begins.  The latitudes and longitudes of the cities were projected onto a rectangular coordinate system with $x$ and $y$ coordinates representing positions in meters which we convert to kilometers.  We have stored the $x$ and $y$ coordinates and a distance matrix with distances between all of the cities in the json file `Caps48.json`.  First we load the data and define a function visualize tours of the 48 capitals.  We plot the best possible tour just to show how the plotting routine works.  The coordinates in the json file are in meters.\n\n**Here is the optimal tour.**","pos":78,"type":"cell"}
{"cell_type":"markdown","id":"c0861b","input":"Here is all the code in one cell for convenience:","pos":107,"type":"cell"}
{"cell_type":"markdown","id":"c69af5","input":"There appears to be only one minimum somewhere around $x=0$ or $x=-1$.  Since the function appears to be convex, the starting point doesn't matter.  Let's search for the minimum beginning at $x_0 = -2$:","metadata":{"hidden":true},"pos":20,"type":"cell"}
{"cell_type":"markdown","id":"c8d194","input":"An apartment complex has 250 apartments to rent and that their profit in thousands of dollars is given by the function \n$$P(x) = -0.008 x^2 + 3.1 x - 80.$$\nFind the maximum profit and how many apartments to rent to achieve the maximum profit.  Use minimize from scipy.optimize to find the maximum.  Review the video above to see how to \"flip\" the problem to find a maximum.","metadata":{"hidden":true},"pos":26,"type":"cell"}
{"cell_type":"markdown","id":"c9cfad","input":"## TSP Local Search Code","pos":77,"type":"cell"}
{"cell_type":"markdown","id":"d1b3b8","input":"The Rastrigin function isn't important as a real-life example, but it does serve as a good test problem with oodles of local minima and we know that global minimum occurs at the origin.  This is similar to what can happen in training in neural networks and other complex models except that we don't know where the global optimum is.\n\n### Multistart for finding Global Optima\n\nA simple approach for trying to find the global minimum of a multi-modal function is called a **restart** or **multistart strategy** in which local searches are started at randomly generated initial points and the most optimal result of all the local searches is recorded.\n\nHere is pseudo-code for a multistart code:\n```\nfor num_searches:\n choose random initial state\n do local search\n if new optimum\n     remember it\nendfor\n```\n","pos":55,"type":"cell"}
{"cell_type":"markdown","id":"d1e507","input":"Here we define both the local \"move\" function which reverses a randomly selected tour segment to generate a new tour and the objective function which computes the length of the tour in kilometers.  The local search continues until `max_no_improve` iterations have occured in which no shorter tour was found.","pos":80,"type":"cell"}
{"cell_type":"markdown","id":"d4cf21","input":"To be clear, the decision variables in this problem are the vector group assignments which were '[0,1,0,1]' in the example above (items A and C are assigned to group 0, items B and D are assigned to group 1).  Our job is to intelligently vary those group assignments until we balance, as best we can, the total values of the groups\n\nNext, we need some mechanism to change the groups. In the next cell, we have a function that does just that. It does the following:\n* determines what the possible choices are (each of the unique groups)\n* makes a copy of the groups\n* selects an item to change, by drawing a random integer between 0 and the length of the array of groups\n* pulls a random draw from our choices of groups\n* checks to make sure that our new choice isn't the same as our old choice\n    * If it is the same, it iteratively loops until it's not the same (we want to ensure that we're actually making a change on each attempt\n* returns the new group\n\nRun the following cell multiple times with debug set to True until you understand what's happening.","pos":92,"type":"cell"}
{"cell_type":"markdown","id":"de0d15","input":"### *Self-Assessment:  Rastrigin with dim = 10*","pos":60,"type":"cell"}
{"cell_type":"markdown","id":"e00632","input":"# Local Search - Continuous Variables","pos":10,"type":"cell"}
{"cell_type":"markdown","id":"e8a488","input":"### *Self Assessment: How many searches when dim = 10?*","metadata":{"hidden":true},"pos":66,"type":"cell"}
{"cell_type":"markdown","id":"e8e8ac","input":"Approximately now many local searches are required to find the global minimum one time when dim = 10?  Is it surprising that you (very likely) didn't find it with 1000 local searches?  Explain","metadata":{"hidden":true},"pos":67,"type":"cell"}
{"cell_type":"markdown","id":"e9345a","input":"### *Self Assessment:  Finding Multiple Extrema*","metadata":{"hidden":true},"pos":30,"type":"cell"}
{"cell_type":"markdown","id":"ec1bb0","input":"How many iterations does it take to reliably find the global minimum with dim = 3?  With dim = 4?  Use the multi-start strategy.  There is not an exact answer to this question, just experiment with `num_local_searches` ... what is the smallest number of searches so that you find the global minimum value of 0 almost every time?","pos":59,"type":"cell"}
{"cell_type":"markdown","id":"f55d65","input":"Your job is to write a function that accepts the values, the number of groups, and the maximum moves with no improvement and attempts to minimize the difference in value between each group. You may also include a debug parameter to help you in writing the code. (Feel free to add your own print statements using the debug parameter.)\n\nSee if your code can find the correct answer to our 4-item/2 group problem first, using just 10 for the number of loops without improvement. Even if you don't use the debug flag, use plenty of print statements to help you follow the action at first.\n\nWhen you have that working, using the following code to generate a larger list of item values. Break it into 4 groups. You will need to up the maximum moves without improvement for the second problem, but we were able to do it with the maximum moves with no improvement set to 200.","pos":95,"type":"cell"}
{"cell_type":"markdown","id":"fa0232","input":"In the next lesson we're going to use a Python package called `simanneal` that uses an object-oriented programming (OOP) approach to optimizing a function with a simulated annealing algorithm.  We'll introduce an object-oriented approach to local search in this lesson to help you understand how it works.\n\n### OOP Review\n\nHere is a 23-minute video to review some ideas from object-oriented programming in Python.  This video isn't comprehensive, but it should serve to refresh your memory a little.\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/ka6-_RfR9XA\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n### Solving the Traveling Salesman Problem with OOP\n\nWe'll solve the Traveling Salesman Problem (TSP) from above using this approach.  To begin with we import the `LocalSearcher` class from the `localsearch` package contained in the same directory as this presentation.\n\n```python\nfrom locsearch import LocalSearcher\n```","pos":98,"type":"cell"}
{"cell_type":"markdown","id":"fb1549","input":"Do 1000 local search with Rastrigin with dim = 10.  What is the smallest value you find?  How long do you think it would take to find the minimum from randomly chosen initial points like this?  Again, there is no right answer, just experiment to get an idea how increasing the number of variables can make it **much** more difficult to find the global minimum.","pos":61,"type":"cell"}
{"cell_type":"markdown","id":"fd41cf","input":"## Value Balancing Example\n\nThis is an example of a fairly simple discrete optimization problem. You have a collection of Y item values. Break the group of items into N sub-groups, having equal value. For instance, given a group of 4 items with the following values:\n\n* Item A = 5\n* Item B = 10\n* Item C = 23\n* Item D = 8\n\nwe would want to put items A, B, and D in one group and item C in another group. If the values represent times, then the total time used by each group is 23. Think of this as a load-balancing task where the goal is to balance the times used by the processors.\n\nWe'll make this easier by using numpy arrays to store our values, and using another numpy array to do assignment into groups.\nSee the code in the next cell for one way we would determine the difference between the value of each of our groups, given a numpy array. (Note that there are simpler ways to do this if we know that we're always going to break this into 2 groups, but this is a quick way to do it given an arbitrary number of groups.)","pos":88,"type":"cell"}
{"cell_type":"markdown","id":"fee847","input":"We can use the model to make predictions:","pos":41,"type":"cell"}
{"id":0,"time":1673904078239,"type":"user"}
{"last_load":1673904080516,"type":"file"}