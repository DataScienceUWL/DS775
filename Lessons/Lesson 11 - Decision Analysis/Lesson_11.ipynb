{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Decision Analysis\n",
    "\n",
    "Decision analysis is a framework for rational decision making when the outcomes are uncertain.  We'll introduce all of the ideas in the context of a prototype example explained below.  This is the same example used in the textbook, but we'll add some different perspectives.  Read through the example to get an idea of the problem before we start discussing it.\n",
    "\n",
    "### Prototype Problem - GOFERBROKE\n",
    "\n",
    "The GOFERBROKE COMPANY owns a tract of land that may contain oil. A consulting geologist has reported to management that she believes there is one chance in four of oil. Because of this prospect, another oil company has offered to purchase the land for \\$90,000. However, Goferbroke is considering holding the land in order to drill for oil itself. The cost of drilling is \\$100,000. If oil is found, the resulting expected revenue will be $800,000, so the companyâ€™s expected profit (after deducting the cost of drilling) will be \\$700,000. A loss of \\$100,000 (the drilling cost) will be incurred if the land is dry (no oil).\n",
    "\n",
    "### With or without experimentation?\n",
    "\n",
    "Often some **testing** is done to reduce the level of uncertainty about the outcome.  For example, a new product might be introduced in a test market to gain insight into whether it is likely to be successful.  The testing is referred to as **experimentation** and we will talk about decisions **with experimentation** and **without experimentation**.\n",
    "\n",
    "# Decision Analysis without Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Before we try to choose an optimal decision it helps to summarize the problem information in a payoff table or a decision tree to prepare for analysis.  We show how to do this in the video below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### (1) video with payoff table and decision tree for Goferbroke."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Decision Strategies\n",
    "\n",
    "In this section we present four different strategies for decision making in this context.\n",
    "\n",
    "## Pessimistic Decision Strategy (Maximin)\n",
    "\n",
    "**At each chance node assume that the worst case outcome occurs and then choose the alternative that optimizes among these worst case outcomes.  Think \"best of the worst.\"**\n",
    "\n",
    "For maximizing a payoff this is called a maximin strategy.  At each chance node choose the outcome with the smallest payoff then maximize these minimum payoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# (2) video showing how to use pessimistic decision strategy for Goferbroke problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Optimistic Decision Strategy (Maximax)\n",
    "\n",
    "This one isn't discussed in the textbook.\n",
    "\n",
    "**At each chance node assume that the best case outcome occurs and then choose the alternative that optimizes among these best case outcomes. Think \"best of the best.\"**\n",
    "\n",
    "For maximizing a payoff this is called a maximax strategy.  At each chance node choose the outcome with the largest payoff then maximize these maximum payoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# (3) video showing how to use optimistic decision strategy for Goferbroke problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Maximum Likelihood Strategy\n",
    "\n",
    "**At each chance node assume the most probable outcome occurs then choose the alternative that optimizes among these most probable outcomes. Think \"best of the most likely.\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# (4) video showing how to use the max likelihood strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Bayes' Decision Rule\n",
    "\n",
    "**At each chance node compute the expected (average) payoff then choose the alternative that optimizes among these expected payoffs.  Think \"best of the averages.\"**\n",
    "\n",
    "If you're unfamiliar with the idea of the expected value of a discrete random variable you can watch the video below otherwise skip the video and watch the next one which shows the strategy applied to the Goferbroke problem.  Bayes decision rule is the most commonly used strategy and we'll focus on it for the remainder of this unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# (5) video showing expected value of a discrete random variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# (6) video showing Baye's Decision Rule applied to Goferbroke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## What if you're minimizing instead?\n",
    "\n",
    "Suppose you're trying to minimize cost instead of maximizing payoff.  You've got two possible options:\n",
    "\n",
    "* Option 1:  Negate all of the costs and maximize the negative cost.  This will yield the decision strategy that minimizes cost.\n",
    "* Option 2:  Adjust the decision strategy to minimize.  \n",
    "\n",
    "For example, if you want to use the Pessimistic Decision Strategy to minimize cost then at each chance node choose the outcome with the largest cost then minmize these maximum costs.  This would be a Minimax strategy.\n",
    "\n",
    "### Minimization Example (give setup and video where Bayes and Pessimistic are employed)\n",
    "\n",
    "A nuclear power company is deciding whether to build a nuclear plant at site A or site B.  The cost of building the plant at site A is \\$10 million but there is a 20\\% chance of an earthquake at site A.  If an earthquake occurs, then construction will be halted and the plant will be built (for additional cost) at site B.  The cost of building the plant at site B is \\$20 million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# (7) insert video here that shows both options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Sensitivity Analysis\n",
    "\n",
    "How does the optimal strategy change if a payoff amount changes or if the prior probabilities of the states of nature change?  Is the optimal strategy robust or is it sensitive to a small change in one of the parameters?\n",
    "\n",
    "In the Goferbroke problem if the probability of oil is larger then we should be more likely to drill and the probability of oil is smaller we should be more likely to sell the land, but at what point should we change strategies.\n",
    "\n",
    "In the example shown in the next video we'll vary the prior probability of oil, $p$, to see how the optimal strategy depends on this parameter.  The Desmos graph we use in the video can be found here: <a href=\"https://www.desmos.com/calculator/djej05rakk\" target=\"_blank\">Desmos Goferbroke Sensitivity.</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# insert video here for sensitivity analysis of Goferbroke (derive equations, plot in desmos, discuss optimal strat in terms of p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Decision Analysis with Experimentation\n",
    "\n",
    "If possible, we'd like to do some testing to reduce the level of uncertainty about the outcomes.  Conducting a test is called experimentation in this context (this language comes from statistics).  For example we might pay a geological consultant to collect additional information and predict whether or not there is oil.  Ultimately we want to use these new probabilities to analyze a larger decision tree like the one below:\n",
    "\n",
    "<img src=\"./images/goferbroke_full_raw.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### (9) video discussion of full large tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Decision Analysis with Experimentation\n",
    "\n",
    "If possible, we'd like to do some testing to reduce the level of uncertainty about the outcomes.  Conducting a test is called experimentation in this context (this language comes from statistics).  For example we might pay a geological consultant to collect additional information and predict whether or not there is oil.  To get an idea if it is worth doing the testing/experimentation we can compute the **Expected Value of Perfect Information**.\n",
    "\n",
    "## Expected Value of Perfect Information (EVPI)\n",
    "\n",
    "EVPI answers the question:  \"How much larger is the expected (average) payoff if we have a perfect diagnostic test?\"  \n",
    "\n",
    "EVPI = expected payoff with perfect information - expected payoff without experimentation\n",
    "\n",
    "The video below shows how to find EVPI for the Goferbroke problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### (10) video EVPI for Goferbroke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Given the results of the experiment we can adjust the probabilities of the outcomes to reflect the experimental findings.  For example, if the geological consultant predicts that there is oil at the current site then we may adjust the probability of finding oil upward.  These post-experiment probabilities are called **posterior probabilities**.  The original probabilities we have from before the experiment are called **prior probabilities**.  Bayes' Theorem can be used to go between prior and posterior probabilities, but before we introduce that let's review a bit about conditional probability first.\n",
    "\n",
    "## Conditional Probability\n",
    "\n",
    "**Conditional probability** is a measure of the probability of an event occurring, given that another event has already occurred.  We give a simple example using dice in the example below but if you want additional background we suggest you study Chapter 3 in the free textbook <a href=\"https://openintro.org/\">OpenIntro Statistics</a>.\n",
    "\n",
    "###  Dice Example\n",
    "\n",
    "In the next video we'll compute the conditional probabilities intuitively and with formulas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### (11) Even and Low Dice example video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Our review will consist of working through just a couple of examples but if you want additional background we suggest you study Chapter 3 in the free textbook <a href=\"https://openintro.org/\">OpenIntro Statistics</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Understanding Posterior Probabilities\n",
    "\n",
    "In the context of decision analysis the conditional probabilities of interest are called posterior probabilities.  We discuss these a bit in the next video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### (12) States and Findings (evidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Goferbroke Example with Contingency Table\n",
    "\n",
    "Contingency tables, also known as classification matrices, are a great way to work with and understand conditional probabilities and Bayes' Theorem.  In the video below we derive the posterior probabilities for the Goferbroke problem using a contingency table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### (13) Goferbroke with Contingency Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The contingency table approach implicitly uses Bayes' Theorem to compute the posterior probabilities.  The posterior probabilities are the conditional probabilities of states of nature given the experimental findings.  The formula version of Bayes' Theorem shows how to compute the posterior probability of the state of nature $S_j$ given the experimental finding $F_i$.  The entries on the right side of the formula are all prior probabilities.\n",
    "\n",
    "$$ P(S_j | F_i) = \\frac{P(S_j \\cap F_i)}{P(F_i)} = \\frac{P(S_j \\cap F_i)}{P(F_i \\cap S_1) + P(F_i \\cap S_2) + \\ldots} = \\frac{P(F_i | S_j) P(S_j)}{P(F_i | S_1) P(S_1) + P(F_i | S_2) P(S_2) + \\ldots}$$\n",
    "\n",
    "In the next video we'll use the contingency table approach to motivate Bayes' Theorem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### (14) video for contingency table and Baye's Theorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Goferbroke Example with Probability Trees\n",
    "\n",
    "An alternative way to understand and derive posterior probabilities is to exploit the natural tree structure of conditional probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### (15) video for probability tree approach to posterior probabilities (show by )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Flipping a probability tree in Silver Decisions\n",
    "\n",
    "Silver Decisions can also be used to compute the posterior probabilities by flipping a probability tree.  This is shown in the video below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### (16) video - flip a probability tree in Silver Decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Another Example for Practice\n",
    "\n",
    "You are off to soccer and want to be the goalkeeper, but that depends on who is coach today:\n",
    "\n",
    "* if Sam is coach the probability of being goalkeeper is 0.5\n",
    "* if Ann is coach the probability of begin goalkeeper is 0.3\n",
    "\n",
    "What is the probability you are goalkeeper today?  Also, if you are goalkeeper then what is the probability Ann is the coach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### (17) Soccer Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Putting it together in Silver Decisions\n",
    "\n",
    "Now we are going to switch gears and analyze both decisions together.  Should the Goferbroke company hire the consultant and should they drill for oil?  In the video below we'll show how to enter and analyze a complete decision tree in Silver Decisions for the Goferbroke problem.  We'll also demonstrate how to find the **Expected Value of the Experiment (EVE)**:\n",
    "\n",
    "EVE answers the question:  \"How much larger is the expected (average) payoff if we gather information by performing an experiment?\"  \n",
    "\n",
    "EVE = expected payoff with experimentation - expected payoff without experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### (18) GoFerBroke - Putting the whole tree in Silver Decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Sensitivity Analysis Including Experimentation\n",
    "\n",
    "Thus far we haven't encountered any computations which aren't simple enough to do by hand.  However it's often important to gain understanding of how the optimal strategy changes as the numbers changed.  For example if one of the payoffs increases or if the probabilities of the states of nature change.  To provide an example we'll analyze how the optimal strategy depends on the prior probability of finding oil in the Goferbroke problem.  This kind of sensitivity analysis is complex enough that it makes it worthwhile to use decision analysis software such as Silver Decisions.\n",
    "\n",
    "Before we demonstrate Silver Decisions we'll show you how to derive posterior probabilities when you have an unspecified parameter such as the prior probability of finding oil, $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### video for deriving posterior probabilities with parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the next video we show how to complete a sensitivity analysis using Silver Decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### video - sensitify analysis in Silver Decisions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}