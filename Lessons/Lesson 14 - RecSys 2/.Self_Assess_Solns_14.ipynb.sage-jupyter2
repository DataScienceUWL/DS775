{"backend_state":"init","connection_file":"/tmp/xdg-runtime-user/jupyter/kernel-f3983f06-0253-464c-96a4-dd368ff04397.json","kernel":"python3","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"234.363px"},"toc_section_display":true,"toc_window_display":true},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"trust":true,"type":"settings"}
{"cell_type":"code","exec_count":1,"id":"476ca7","input":"# EXECUTE FIRST\n\n# computational imports\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom surprise import Reader, Dataset, KNNBasic, NormalPredictor,BaselineOnly,KNNWithMeans,KNNBaseline\nfrom surprise import SVD, SVDpp, NMF, SlopeOne, CoClustering\nfrom surprise.model_selection import cross_validate\nfrom surprise.model_selection import GridSearchCV\nfrom surprise import accuracy\n\nimport random\nfrom ast import literal_eval\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel\n\n# plotting imports\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\nmatplotlib.style.use('ggplot')\n# for reading files from urls\nimport urllib.request\n# display imports\nfrom IPython.display import display, IFrame\nfrom IPython.core.display import HTML\n","metadata":{"code_folding":[0]},"pos":0,"type":"cell"}
{"cell_type":"code","exec_count":10,"id":"9fb52b","input":"r_matrix_dummy.head()","metadata":{"hidden":true},"output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>ISBN</th>\n      <th>006101351X</th>\n      <th>014025448X</th>\n      <th>014028009X</th>\n      <th>034540288X</th>\n      <th>038079487X</th>\n      <th>043935806X</th>\n      <th>044021145X</th>\n      <th>044022165X</th>\n      <th>044023722X</th>\n      <th>044651652X</th>\n      <th>...</th>\n      <th>743418174</th>\n      <th>767902521</th>\n      <th>767905180</th>\n      <th>786868716</th>\n      <th>786881852</th>\n      <th>804106304</th>\n      <th>804114986</th>\n      <th>805063897</th>\n      <th>842329129</th>\n      <th>971880107</th>\n    </tr>\n    <tr>\n      <th>User-ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6251</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>6575</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>7346</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>11601</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>11676</th>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 200 columns</p>\n</div>","text/plain":"ISBN     006101351X  014025448X  014028009X  034540288X  038079487X  \\\nUser-ID                                                               \n6251            0.0         0.0         0.0         1.0         0.0   \n6575            0.0         0.0         0.0         0.0         0.0   \n7346            1.0         0.0         0.0         0.0         0.0   \n11601           0.0         1.0         0.0         0.0         0.0   \n11676           9.0         0.0         0.0         0.0         0.0   \n\nISBN     043935806X  044021145X  044022165X  044023722X  044651652X  ...  \\\nUser-ID                                                              ...   \n6251            0.0         1.0         0.0         0.0         0.0  ...   \n6575            0.0         0.0         0.0         0.0         0.0  ...   \n7346            0.0         0.0         0.0         0.0         0.0  ...   \n11601           0.0         0.0         0.0         0.0         0.0  ...   \n11676           0.0         0.0         0.0         0.0         0.0  ...   \n\nISBN     743418174  767902521  767905180  786868716  786881852  804106304  \\\nUser-ID                                                                     \n6251           1.0        0.0        0.0        1.0        0.0        0.0   \n6575           0.0        0.0        0.0        0.0        0.0        0.0   \n7346           0.0        0.0        0.0        0.0        0.0       10.0   \n11601          0.0        0.0        0.0        0.0        0.0        0.0   \n11676          0.0        0.0        0.0        0.0        0.0        0.0   \n\nISBN     804114986  805063897  842329129  971880107  \nUser-ID                                              \n6251           0.0        0.0        0.0        0.0  \n6575           0.0        0.0        0.0        0.0  \n7346           1.0        0.0        0.0        0.0  \n11601          0.0        0.0        0.0        0.0  \n11676          0.0        0.0        0.0        0.0  \n\n[5 rows x 200 columns]"},"exec_count":10,"output_type":"execute_result"}},"pos":13,"scrolled":true,"type":"cell"}
{"cell_type":"code","exec_count":11,"id":"f0fdac","input":"#Compute the cosine similarity matrix using the dummy ratings matrix\ncosine_sim = cosine_similarity(r_matrix_dummy, r_matrix_dummy)","metadata":{"hidden":true},"pos":14,"type":"cell"}
{"cell_type":"code","exec_count":12,"id":"9cf89f","input":"#Convert into pandas dataframe \ncosine_sim = pd.DataFrame(cosine_sim, index=r_matrix.index, columns=r_matrix.index)\n\ncosine_sim.head()","metadata":{"hidden":true},"output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>User-ID</th>\n      <th>6251</th>\n      <th>6575</th>\n      <th>7346</th>\n      <th>11601</th>\n      <th>11676</th>\n      <th>13552</th>\n      <th>14521</th>\n      <th>16795</th>\n      <th>21014</th>\n      <th>23768</th>\n      <th>...</th>\n      <th>238781</th>\n      <th>254465</th>\n      <th>258534</th>\n      <th>260897</th>\n      <th>261829</th>\n      <th>265115</th>\n      <th>265313</th>\n      <th>266226</th>\n      <th>269566</th>\n      <th>274308</th>\n    </tr>\n    <tr>\n      <th>User-ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6251</th>\n      <td>1.000000</td>\n      <td>0.019892</td>\n      <td>0.030961</td>\n      <td>0.005078</td>\n      <td>0.142988</td>\n      <td>0.048059</td>\n      <td>0.064752</td>\n      <td>0.018610</td>\n      <td>0.002779</td>\n      <td>0.035858</td>\n      <td>...</td>\n      <td>0.015235</td>\n      <td>0.023108</td>\n      <td>0.006664</td>\n      <td>0.026021</td>\n      <td>0.018473</td>\n      <td>0.029876</td>\n      <td>0.003219</td>\n      <td>0.052636</td>\n      <td>0.079578</td>\n      <td>0.045950</td>\n    </tr>\n    <tr>\n      <th>6575</th>\n      <td>0.019892</td>\n      <td>1.000000</td>\n      <td>0.001540</td>\n      <td>0.022224</td>\n      <td>0.115155</td>\n      <td>0.002390</td>\n      <td>0.000000</td>\n      <td>0.001018</td>\n      <td>0.132236</td>\n      <td>0.046586</td>\n      <td>...</td>\n      <td>0.075006</td>\n      <td>0.145943</td>\n      <td>0.014581</td>\n      <td>0.001582</td>\n      <td>0.000000</td>\n      <td>0.012711</td>\n      <td>0.019368</td>\n      <td>0.014397</td>\n      <td>0.051694</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>7346</th>\n      <td>0.030961</td>\n      <td>0.001540</td>\n      <td>1.000000</td>\n      <td>0.003931</td>\n      <td>0.193686</td>\n      <td>0.016909</td>\n      <td>0.019491</td>\n      <td>0.024488</td>\n      <td>0.002151</td>\n      <td>0.104082</td>\n      <td>...</td>\n      <td>0.043239</td>\n      <td>0.030895</td>\n      <td>0.000000</td>\n      <td>0.044759</td>\n      <td>0.017873</td>\n      <td>0.256943</td>\n      <td>0.007474</td>\n      <td>0.112041</td>\n      <td>0.281033</td>\n      <td>0.032603</td>\n    </tr>\n    <tr>\n      <th>11601</th>\n      <td>0.005078</td>\n      <td>0.022224</td>\n      <td>0.003931</td>\n      <td>1.000000</td>\n      <td>0.002773</td>\n      <td>0.030508</td>\n      <td>0.005024</td>\n      <td>0.031187</td>\n      <td>0.003880</td>\n      <td>0.006260</td>\n      <td>...</td>\n      <td>0.007092</td>\n      <td>0.002934</td>\n      <td>0.006204</td>\n      <td>0.407819</td>\n      <td>0.029023</td>\n      <td>0.046359</td>\n      <td>0.000000</td>\n      <td>0.055132</td>\n      <td>0.020838</td>\n      <td>0.085563</td>\n    </tr>\n    <tr>\n      <th>11676</th>\n      <td>0.142988</td>\n      <td>0.115155</td>\n      <td>0.193686</td>\n      <td>0.002773</td>\n      <td>1.000000</td>\n      <td>0.009544</td>\n      <td>0.212180</td>\n      <td>0.081307</td>\n      <td>0.022762</td>\n      <td>0.088125</td>\n      <td>...</td>\n      <td>0.033282</td>\n      <td>0.022946</td>\n      <td>0.000000</td>\n      <td>0.018948</td>\n      <td>0.001261</td>\n      <td>0.010877</td>\n      <td>0.275982</td>\n      <td>0.079053</td>\n      <td>0.027163</td>\n      <td>0.043917</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 100 columns</p>\n</div>","text/plain":"User-ID    6251      6575      7346      11601     11676     13552     14521   \\\nUser-ID                                                                         \n6251     1.000000  0.019892  0.030961  0.005078  0.142988  0.048059  0.064752   \n6575     0.019892  1.000000  0.001540  0.022224  0.115155  0.002390  0.000000   \n7346     0.030961  0.001540  1.000000  0.003931  0.193686  0.016909  0.019491   \n11601    0.005078  0.022224  0.003931  1.000000  0.002773  0.030508  0.005024   \n11676    0.142988  0.115155  0.193686  0.002773  1.000000  0.009544  0.212180   \n\nUser-ID    16795     21014     23768   ...    238781    254465    258534  \\\nUser-ID                                ...                                 \n6251     0.018610  0.002779  0.035858  ...  0.015235  0.023108  0.006664   \n6575     0.001018  0.132236  0.046586  ...  0.075006  0.145943  0.014581   \n7346     0.024488  0.002151  0.104082  ...  0.043239  0.030895  0.000000   \n11601    0.031187  0.003880  0.006260  ...  0.007092  0.002934  0.006204   \n11676    0.081307  0.022762  0.088125  ...  0.033282  0.022946  0.000000   \n\nUser-ID    260897    261829    265115    265313    266226    269566    274308  \nUser-ID                                                                        \n6251     0.026021  0.018473  0.029876  0.003219  0.052636  0.079578  0.045950  \n6575     0.001582  0.000000  0.012711  0.019368  0.014397  0.051694  0.000000  \n7346     0.044759  0.017873  0.256943  0.007474  0.112041  0.281033  0.032603  \n11601    0.407819  0.029023  0.046359  0.000000  0.055132  0.020838  0.085563  \n11676    0.018948  0.001261  0.010877  0.275982  0.079053  0.027163  0.043917  \n\n[5 rows x 100 columns]"},"exec_count":12,"output_type":"execute_result"}},"pos":15,"type":"cell"}
{"cell_type":"code","exec_count":13,"id":"9e3972","input":"#User Based Collaborative Filter using Weighted Mean Ratings\ndef cf_wmean(user_id, item_id, ratings_matrix, c_sim_matrix, median_rating):\n    \n    #Check if item_id exists in r_matrix\n    if item_id in ratings_matrix:\n        \n        #Get the similarity scores for the user in question with every other user\n        sim_scores = c_sim_matrix[user_id]\n        \n        #Get the user ratings for the item in question\n        i_ratings = ratings_matrix[item_id]\n        \n        #Extract the indices containing NaN in the i_ratings series\n        idx = i_ratings[i_ratings.isnull()].index\n        \n        #Drop the NaN values from the m_ratings Series\n        i_ratings = i_ratings.dropna()\n        \n        #Drop the corresponding cosine scores from the sim_scores series\n        sim_scores = sim_scores.drop(idx)\n\n        #Compute the final weighted mean\n        if sim_scores.sum()>0:\n            wmean_rating = np.dot(sim_scores, i_ratings)/ sim_scores.sum()\n        else:  # user had zero cosine similarity with other users\n            wmean_rating = median_rating\n\n    else:\n        #Default to the median in the absence of any information\n        wmean_rating = median_rating\n    \n    return wmean_rating","metadata":{"hidden":true},"pos":16,"type":"cell"}
{"cell_type":"code","exec_count":14,"id":"2514d3","input":"score(cf_wmean, X_test, r_matrix, cosine_sim, 6)","metadata":{"hidden":true},"output":{"0":{"data":{"text/plain":"3.607093266358255"},"exec_count":14,"output_type":"execute_result"}},"pos":17,"type":"cell"}
{"cell_type":"code","exec_count":15,"id":"6f0ec6","input":"#Build the ratings matrix using pivot_table function\n#r_matrix = X_train.pivot_table(values='Book-Rating', index='ISBN', columns='User-ID')\nr_matrix_item = X_train.pivot(values='Book-Rating', index='ISBN', columns='User-ID')\n\nr_matrix_item.head()","metadata":{"hidden":true},"output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>User-ID</th>\n      <th>6251</th>\n      <th>6575</th>\n      <th>7346</th>\n      <th>11601</th>\n      <th>11676</th>\n      <th>13552</th>\n      <th>14521</th>\n      <th>16795</th>\n      <th>21014</th>\n      <th>23768</th>\n      <th>...</th>\n      <th>238781</th>\n      <th>254465</th>\n      <th>258534</th>\n      <th>260897</th>\n      <th>261829</th>\n      <th>265115</th>\n      <th>265313</th>\n      <th>266226</th>\n      <th>269566</th>\n      <th>274308</th>\n    </tr>\n    <tr>\n      <th>ISBN</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>006101351X</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>9.0</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>014025448X</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>8.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>014028009X</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>034540288X</th>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>038079487X</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 100 columns</p>\n</div>","text/plain":"User-ID     6251    6575    7346    11601   11676   13552   14521   16795   \\\nISBN                                                                         \n006101351X     NaN     NaN     1.0     NaN     9.0     NaN     6.0     NaN   \n014025448X     NaN     NaN     NaN     1.0     NaN     NaN     NaN     NaN   \n014028009X     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n034540288X     1.0     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n038079487X     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n\nUser-ID     21014   23768   ...  238781  254465  258534  260897  261829  \\\nISBN                        ...                                           \n006101351X     NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN   \n014025448X     NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN   \n014028009X     NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN   \n034540288X     NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN   \n038079487X     NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN   \n\nUser-ID     265115  265313  266226  269566  274308  \nISBN                                                \n006101351X     NaN     NaN     1.0     NaN     1.0  \n014025448X     8.0     NaN     NaN     NaN     NaN  \n014028009X     NaN     NaN     NaN     NaN     NaN  \n034540288X     NaN     NaN     NaN     NaN     1.0  \n038079487X     1.0     NaN     NaN     NaN     NaN  \n\n[5 rows x 100 columns]"},"exec_count":15,"output_type":"execute_result"}},"pos":20,"type":"cell"}
{"cell_type":"code","exec_count":16,"id":"e7e48b","input":"#Create a dummy ratings matrix with all null values imputed to 0\nr_matrix_item_dummy = r_matrix_item.copy().fillna(0)","metadata":{"hidden":true},"pos":21,"type":"cell"}
{"cell_type":"code","exec_count":17,"id":"137835","input":"# Import cosine_score \nfrom sklearn.metrics.pairwise import cosine_similarity\n\n#Compute the cosine similarity matrix using the dummy ratings matrix\ncosine_sim_item = cosine_similarity(r_matrix_item_dummy, r_matrix_item_dummy)","metadata":{"hidden":true},"pos":22,"type":"cell"}
{"cell_type":"code","exec_count":18,"id":"27c21d","input":"#Convert into pandas dataframe \ncosine_sim_item = pd.DataFrame(cosine_sim_item, index=r_matrix_item.index, columns=r_matrix_item.index)\n\ncosine_sim_item.head(10)","metadata":{"hidden":true},"output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>ISBN</th>\n      <th>006101351X</th>\n      <th>014025448X</th>\n      <th>014028009X</th>\n      <th>034540288X</th>\n      <th>038079487X</th>\n      <th>043935806X</th>\n      <th>044021145X</th>\n      <th>044022165X</th>\n      <th>044023722X</th>\n      <th>044651652X</th>\n      <th>...</th>\n      <th>743418174</th>\n      <th>767902521</th>\n      <th>767905180</th>\n      <th>786868716</th>\n      <th>786881852</th>\n      <th>804106304</th>\n      <th>804114986</th>\n      <th>805063897</th>\n      <th>842329129</th>\n      <th>971880107</th>\n    </tr>\n    <tr>\n      <th>ISBN</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>006101351X</th>\n      <td>1.000000</td>\n      <td>0.005847</td>\n      <td>0.044958</td>\n      <td>0.035806</td>\n      <td>0.000000</td>\n      <td>0.051717</td>\n      <td>0.036379</td>\n      <td>0.032858</td>\n      <td>0.044481</td>\n      <td>0.064775</td>\n      <td>...</td>\n      <td>0.032187</td>\n      <td>0.047741</td>\n      <td>0.009323</td>\n      <td>0.007262</td>\n      <td>0.006989</td>\n      <td>0.058881</td>\n      <td>0.043183</td>\n      <td>0.029235</td>\n      <td>0.006989</td>\n      <td>0.026854</td>\n    </tr>\n    <tr>\n      <th>014025448X</th>\n      <td>0.005847</td>\n      <td>1.000000</td>\n      <td>0.009320</td>\n      <td>0.000000</td>\n      <td>0.044639</td>\n      <td>0.000000</td>\n      <td>0.031109</td>\n      <td>0.000000</td>\n      <td>0.007245</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.033029</td>\n      <td>0.049897</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.071714</td>\n      <td>0.010206</td>\n    </tr>\n    <tr>\n      <th>014028009X</th>\n      <td>0.044958</td>\n      <td>0.009320</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.032617</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.034416</td>\n      <td>...</td>\n      <td>0.030783</td>\n      <td>0.006341</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.391058</td>\n      <td>0.189290</td>\n      <td>0.279600</td>\n      <td>0.000000</td>\n      <td>0.007134</td>\n    </tr>\n    <tr>\n      <th>034540288X</th>\n      <td>0.035806</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.032827</td>\n      <td>0.063500</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.150756</td>\n      <td>...</td>\n      <td>0.044947</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.025351</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.048795</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>038079487X</th>\n      <td>0.000000</td>\n      <td>0.044639</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.055744</td>\n      <td>0.177880</td>\n      <td>0.146524</td>\n      <td>...</td>\n      <td>0.051876</td>\n      <td>0.033748</td>\n      <td>0.063267</td>\n      <td>0.024639</td>\n      <td>0.000000</td>\n      <td>0.049947</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.539464</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>043935806X</th>\n      <td>0.051717</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.032827</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.041690</td>\n      <td>0.005021</td>\n      <td>0.034955</td>\n      <td>0.197952</td>\n      <td>...</td>\n      <td>0.014754</td>\n      <td>0.328266</td>\n      <td>0.000000</td>\n      <td>0.003329</td>\n      <td>0.032035</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.006407</td>\n      <td>0.131306</td>\n    </tr>\n    <tr>\n      <th>044021145X</th>\n      <td>0.036379</td>\n      <td>0.031109</td>\n      <td>0.032617</td>\n      <td>0.063500</td>\n      <td>0.000000</td>\n      <td>0.041690</td>\n      <td>1.000000</td>\n      <td>0.019424</td>\n      <td>0.005635</td>\n      <td>0.191460</td>\n      <td>...</td>\n      <td>0.031395</td>\n      <td>0.038806</td>\n      <td>0.066136</td>\n      <td>0.003220</td>\n      <td>0.006197</td>\n      <td>0.000000</td>\n      <td>0.191460</td>\n      <td>0.000000</td>\n      <td>0.037182</td>\n      <td>0.007938</td>\n    </tr>\n    <tr>\n      <th>044022165X</th>\n      <td>0.032858</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.055744</td>\n      <td>0.005021</td>\n      <td>0.019424</td>\n      <td>1.000000</td>\n      <td>0.020357</td>\n      <td>0.023057</td>\n      <td>...</td>\n      <td>0.175295</td>\n      <td>0.000000</td>\n      <td>0.039823</td>\n      <td>0.031018</td>\n      <td>0.074629</td>\n      <td>0.010480</td>\n      <td>0.046114</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.114708</td>\n    </tr>\n    <tr>\n      <th>044023722X</th>\n      <td>0.044481</td>\n      <td>0.007245</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.177880</td>\n      <td>0.034955</td>\n      <td>0.005635</td>\n      <td>0.020357</td>\n      <td>1.000000</td>\n      <td>0.107019</td>\n      <td>...</td>\n      <td>0.087744</td>\n      <td>0.049298</td>\n      <td>0.057762</td>\n      <td>0.076484</td>\n      <td>0.017319</td>\n      <td>0.012160</td>\n      <td>0.053510</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033276</td>\n    </tr>\n    <tr>\n      <th>044651652X</th>\n      <td>0.064775</td>\n      <td>0.000000</td>\n      <td>0.034416</td>\n      <td>0.150756</td>\n      <td>0.146524</td>\n      <td>0.197952</td>\n      <td>0.191460</td>\n      <td>0.023057</td>\n      <td>0.107019</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.184257</td>\n      <td>0.078507</td>\n      <td>0.015287</td>\n      <td>0.029424</td>\n      <td>0.061978</td>\n      <td>0.090909</td>\n      <td>0.246183</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows Ã— 200 columns</p>\n</div>","text/plain":"ISBN        006101351X  014025448X  014028009X  034540288X  038079487X  \\\nISBN                                                                     \n006101351X    1.000000    0.005847    0.044958    0.035806    0.000000   \n014025448X    0.005847    1.000000    0.009320    0.000000    0.044639   \n014028009X    0.044958    0.009320    1.000000    0.000000    0.000000   \n034540288X    0.035806    0.000000    0.000000    1.000000    0.000000   \n038079487X    0.000000    0.044639    0.000000    0.000000    1.000000   \n043935806X    0.051717    0.000000    0.000000    0.032827    0.000000   \n044021145X    0.036379    0.031109    0.032617    0.063500    0.000000   \n044022165X    0.032858    0.000000    0.000000    0.000000    0.055744   \n044023722X    0.044481    0.007245    0.000000    0.000000    0.177880   \n044651652X    0.064775    0.000000    0.034416    0.150756    0.146524   \n\nISBN        043935806X  044021145X  044022165X  044023722X  044651652X  ...  \\\nISBN                                                                    ...   \n006101351X    0.051717    0.036379    0.032858    0.044481    0.064775  ...   \n014025448X    0.000000    0.031109    0.000000    0.007245    0.000000  ...   \n014028009X    0.000000    0.032617    0.000000    0.000000    0.034416  ...   \n034540288X    0.032827    0.063500    0.000000    0.000000    0.150756  ...   \n038079487X    0.000000    0.000000    0.055744    0.177880    0.146524  ...   \n043935806X    1.000000    0.041690    0.005021    0.034955    0.197952  ...   \n044021145X    0.041690    1.000000    0.019424    0.005635    0.191460  ...   \n044022165X    0.005021    0.019424    1.000000    0.020357    0.023057  ...   \n044023722X    0.034955    0.005635    0.020357    1.000000    0.107019  ...   \n044651652X    0.197952    0.191460    0.023057    0.107019    1.000000  ...   \n\nISBN        743418174  767902521  767905180  786868716  786881852  804106304  \\\nISBN                                                                           \n006101351X   0.032187   0.047741   0.009323   0.007262   0.006989   0.058881   \n014025448X   0.033029   0.049897   0.000000   0.000000   0.000000   0.000000   \n014028009X   0.030783   0.006341   0.000000   0.000000   0.000000   0.391058   \n034540288X   0.044947   0.000000   0.000000   0.025351   0.000000   0.000000   \n038079487X   0.051876   0.033748   0.063267   0.024639   0.000000   0.049947   \n043935806X   0.014754   0.328266   0.000000   0.003329   0.032035   0.000000   \n044021145X   0.031395   0.038806   0.066136   0.003220   0.006197   0.000000   \n044022165X   0.175295   0.000000   0.039823   0.031018   0.074629   0.010480   \n044023722X   0.087744   0.049298   0.057762   0.076484   0.017319   0.012160   \n044651652X   0.000000   0.184257   0.078507   0.015287   0.029424   0.061978   \n\nISBN        804114986  805063897  842329129  971880107  \nISBN                                                    \n006101351X   0.043183   0.029235   0.006989   0.026854  \n014025448X   0.000000   0.000000   0.071714   0.010206  \n014028009X   0.189290   0.279600   0.000000   0.007134  \n034540288X   0.000000   0.000000   0.048795   0.000000  \n038079487X   0.000000   0.000000   0.539464   0.000000  \n043935806X   0.000000   0.000000   0.006407   0.131306  \n044021145X   0.191460   0.000000   0.037182   0.007938  \n044022165X   0.046114   0.000000   0.000000   0.114708  \n044023722X   0.053510   0.000000   0.000000   0.033276  \n044651652X   0.090909   0.246183   0.000000   0.000000  \n\n[10 rows x 200 columns]"},"exec_count":18,"output_type":"execute_result"}},"pos":23,"type":"cell"}
{"cell_type":"code","exec_count":19,"id":"55a0a4","input":"#Item-Based Collaborative Filter using Weighted Mean Ratings\ndef cf_item_wmean(user_id, item_id, ratings_matrix, c_sim_matrix, median_rating):\n    \n    #Check if user exists in r_matrix\n    if user_id in ratings_matrix:\n        \n        #Get the similarity scores for the item in question with every other item\n        sim_scores = c_sim_matrix[item_id]\n        \n        #Get the user ratings for the book in question\n        u_ratings = ratings_matrix[user_id]\n        \n        #Extract the indices containing NaN in the m_ratings series\n        idx = u_ratings[u_ratings.isnull()].index\n        \n        #Drop the NaN values from the m_ratings Series\n        u_ratings = u_ratings.dropna()\n        \n        #Drop the corresponding cosine scores from the sim_scores series\n        sim_scores = sim_scores.drop(idx)\n        \n        #Compute the final weighted mean\n        if sim_scores.sum() > 0:\n            wmean_rating = np.dot(sim_scores, u_ratings)/ sim_scores.sum()\n        else: # the book has zero cosine similarity with other books\n            wmean_rating = median_rating\n    \n    else:\n        #Default to a rating of 6.0 in the absence of any information\n        wmean_rating = median_rating\n    \n    return wmean_rating","metadata":{"hidden":true},"pos":24,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"b16af9","input":"# load the data\nimport pandas as pd\nimport numpy as np\nbx = pd.read_csv('./data/BX-Book-Ratings-3000.csv')\nbx.head()","metadata":{"hidden":true},"output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>User-ID</th>\n      <th>ISBN</th>\n      <th>Book-Rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6251</td>\n      <td>345370775</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6251</td>\n      <td>044021145X</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6251</td>\n      <td>312983271</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6251</td>\n      <td>080410526X</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6251</td>\n      <td>743418174</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   User-ID        ISBN  Book-Rating\n0     6251   345370775            1\n1     6251  044021145X            1\n2     6251   312983271            1\n3     6251  080410526X            1\n4     6251   743418174            1"},"exec_count":2,"output_type":"execute_result"}},"pos":3,"type":"cell"}
{"cell_type":"code","exec_count":20,"id":"9c80d3","input":"score(cf_item_wmean, X_test, r_matrix_item, cosine_sim_item, 6)","metadata":{"hidden":true},"output":{"0":{"data":{"text/plain":"3.4119539180908327"},"exec_count":20,"output_type":"execute_result"}},"pos":25,"type":"cell"}
{"cell_type":"code","exec_count":21,"id":"e534b6","input":"bx.head()","metadata":{"hidden":true},"output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>User-ID</th>\n      <th>ISBN</th>\n      <th>Book-Rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6251</td>\n      <td>345370775</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6251</td>\n      <td>044021145X</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6251</td>\n      <td>312983271</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6251</td>\n      <td>080410526X</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6251</td>\n      <td>743418174</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   User-ID        ISBN  Book-Rating\n0     6251   345370775            1\n1     6251  044021145X            1\n2     6251   312983271            1\n3     6251  080410526X            1\n4     6251   743418174            1"},"exec_count":21,"output_type":"execute_result"}},"pos":28,"type":"cell"}
{"cell_type":"code","exec_count":22,"id":"dd57d2","input":"#Define a Reader object\n#The Reader object helps in parsing the file or dataframe containing ratings\nreader = Reader(rating_scale=(1,11))\n\n#Create the dataset to be used for building the filter\n#data = Dataset.load_from_df(ratings, reader)\ndata = Dataset.load_from_df(bx, reader)\n\n#Define the algorithm object; in this case kNN\nrandom.seed(1)\nnp.random.seed(1)\nknn = KNNBasic(k=5, verbose=False)\n\n#Evaluate the performance in terms of RMSE\nfrom surprise.model_selection import cross_validate\nknn_cv = cross_validate(knn, data, measures=['RMSE'], cv=5, verbose=True)\n#to extract the mean RMSE, we need to get the mean of the test_rmse values\nknn_RMSE = np.mean(knn_cv['test_rmse'])\nprint(f'\\nThe RMSE across five folds was {knn_RMSE}')","metadata":{"hidden":true},"output":{"0":{"name":"stdout","output_type":"stream","text":"Evaluating RMSE of algorithm KNNBasic on 5 split(s).\n\n                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \nRMSE (testset)    3.6956  3.7194  3.8178  3.7849  3.4263  3.6888  0.1384  \nFit time          0.00    0.00    0.00    0.00    0.00    0.00    0.00    \nTest time         0.01    0.01    0.01    0.01    0.01    0.01    0.00    \n\nThe RMSE across five folds was 3.688801997695312\n"}},"pos":29,"scrolled":true,"type":"cell"}
{"cell_type":"code","exec_count":23,"id":"586ca9","input":"#Define a Reader object\n#The Reader object helps in parsing the file or dataframe containing ratings\nreader = Reader(rating_scale=(1,11))\n\n#Create the dataset to be used for building the filter\n#data = Dataset.load_from_df(ratings, reader)\ndata = Dataset.load_from_df(bx, reader)\n\n\nsim_options = {'user_based': False  # compute  similarities between items\n               }\n\n#Define the algorithm object; in this case kNN\nrandom.seed(1)\nnp.random.seed(1)\nknn = KNNBasic(k=5, sim_options=sim_options)\n\n#Evaluate the performance in terms of RMSE\nfrom surprise.model_selection import cross_validate\nknn_cv = cross_validate(knn, data, measures=['RMSE'], cv=5, verbose=True)\n#to extract the mean RMSE, we need to get the mean of the test_rmse values\nknn_RMSE = np.mean(knn_cv['test_rmse'])\nprint(f'\\nThe RMSE across five folds was {knn_RMSE}')\n\n#re-train on the whole dataset\ntrainset = data.build_full_trainset()\nknn.fit(trainset)","output":{"0":{"name":"stdout","output_type":"stream","text":"Computing the msd similarity matrix...\nDone computing similarity matrix.\nComputing the msd similarity matrix...\nDone computing similarity matrix.\nComputing the msd similarity matrix...\nDone computing similarity matrix.\nComputing the msd similarity matrix...\nDone computing similarity matrix.\nComputing the msd similarity matrix...\nDone computing similarity matrix.\nEvaluating RMSE of algorithm KNNBasic on 5 split(s).\n\n                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \nRMSE (testset)    3.4648  3.4779  3.4987  3.4872  3.2932  3.4444  0.0764  \nFit time          0.00    0.00    0.00    0.00    0.00    0.00    0.00    \nTest time         0.03    0.02    0.01    0.01    0.01    0.01    0.01    \n\nThe RMSE across five folds was 3.444369689909382\nComputing the msd similarity matrix...\nDone computing similarity matrix.\n"},"1":{"data":{"text/plain":"<surprise.prediction_algorithms.knns.KNNBasic at 0x7fc526333fd0>"},"exec_count":23,"output_type":"execute_result"}},"pos":31,"type":"cell"}
{"cell_type":"code","exec_count":24,"id":"f2ad9a","input":"# load the data\nimport pandas as pd\nimport numpy as np\nbx = pd.read_csv('./data/BX-Book-Ratings-3000.csv')\nbx.head(5)","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>User-ID</th>\n      <th>ISBN</th>\n      <th>Book-Rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6251</td>\n      <td>345370775</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6251</td>\n      <td>044021145X</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6251</td>\n      <td>312983271</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6251</td>\n      <td>080410526X</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6251</td>\n      <td>743418174</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   User-ID        ISBN  Book-Rating\n0     6251   345370775            1\n1     6251  044021145X            1\n2     6251   312983271            1\n3     6251  080410526X            1\n4     6251   743418174            1"},"exec_count":24,"output_type":"execute_result"}},"pos":34,"type":"cell"}
{"cell_type":"code","exec_count":25,"id":"8efd7c","input":"#Define a Reader object\n#The Reader object helps in parsing the file or dataframe containing ratings\nreader = Reader(rating_scale=(1,11))\n\n#Create the dataset to be used for building the filter\n#data = Dataset.load_from_df(ratings, reader)\ndata = Dataset.load_from_df(bx, reader)\n\n#train a knn item-based collaborative filter - don't set k, just let it pick\nsim_options = {'user_based': False  # compute  similarities between items\n               }\n\n# Retrieve the trainset.\ntrainset = data.build_full_trainset()\n\n#Define the algorithm object; in this case item-based kNNBasic\nrandom.seed(1)\nnp.random.seed(1)\nknn = KNNBasic(sim_options=sim_options)\n\n#fit the data\nknn.fit(trainset)\n\n\n#Build the SVD based Collaborative filter\nsvd = SVD()\nrandom.seed(1)\nnp.random.seed(1)\n\n#fit the data\nsvd.fit(trainset)\n\n#test a couple of predictions (Note that ISBN is a string)\nprint(knn.predict(31315, '446606189'))\nprint(svd.predict(31315, '446606189'))","output":{"0":{"name":"stdout","output_type":"stream","text":"Computing the msd similarity matrix...\n"},"1":{"name":"stdout","output_type":"stream","text":"Done computing similarity matrix.\nuser: 31315      item: 446606189  r_ui = None   est = 8.84   {'actual_k': 30, 'was_impossible': False}\nuser: 31315      item: 446606189  r_ui = None   est = 7.39   {'was_impossible': False}\n"}},"pos":35,"type":"cell"}
{"cell_type":"code","exec_count":26,"id":"f1f2e5","input":"#build the hybrid function\ndef hybrid(ratings, userid, item_algo, user_algo, item_weight, N):\n    '''\n    Parameters\n    ratings: the ratings dataframe we're working with\n    userid: the user for whom we are making predictions\n    item_algo: the trained Surprise item-based collaborative filter\n    user_algo: the trained Surprise user-based collaborative filter\n    N: the number of predictions to return\n    returns\n    a dataframe of top recommendations\n    '''\n\n    #first get a dataframe of unique books\n    sim_items = ratings.copy().drop(columns=['User-ID', 'Book-Rating']).drop_duplicates()\n    #generate the predicted this user's predicted rating for each of them based on the item-based filter\n    sim_items['iPrediction'] = sim_items.apply(lambda x: item_algo.predict(userid, x['ISBN']).est, axis=1)\n\n    #add the predictions based on the user-based collaborative filter\n    sim_items['uPrediction'] = sim_items.apply(lambda x: user_algo.predict(userid, x['ISBN']).est, axis=1)\n\n    #weight the item-based collaborative filter by item_weight and the user-based collaborative filter by 1-item_weight and sum them\n\n    sim_items['finalPrediction'] = sim_items.apply(lambda x: (x['iPrediction'] * item_weight) + (x['uPrediction'] * (1-item_weight)), axis=1)\n\n    #get the top N users who rated this highl\n    sim_items = sim_items.sort_values('finalPrediction', ascending=False)\n    return sim_items.head(N)\n\nhybrid(bx, 31315, knn, svd, .6, 10)    \n","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ISBN</th>\n      <th>iPrediction</th>\n      <th>uPrediction</th>\n      <th>finalPrediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>352</th>\n      <td>446606189</td>\n      <td>8.835329</td>\n      <td>7.387121</td>\n      <td>8.256046</td>\n    </tr>\n    <tr>\n      <th>179</th>\n      <td>440234743</td>\n      <td>8.786790</td>\n      <td>7.333763</td>\n      <td>8.205579</td>\n    </tr>\n    <tr>\n      <th>359</th>\n      <td>142000205</td>\n      <td>8.391583</td>\n      <td>6.847254</td>\n      <td>7.773851</td>\n    </tr>\n    <tr>\n      <th>297</th>\n      <td>786881852</td>\n      <td>6.321247</td>\n      <td>5.910907</td>\n      <td>6.157111</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>440214041</td>\n      <td>5.330645</td>\n      <td>5.854014</td>\n      <td>5.539993</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>312195516</td>\n      <td>3.746369</td>\n      <td>5.470936</td>\n      <td>4.436196</td>\n    </tr>\n    <tr>\n      <th>160</th>\n      <td>312995423</td>\n      <td>4.319208</td>\n      <td>3.501948</td>\n      <td>3.992304</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>60502258</td>\n      <td>3.518893</td>\n      <td>4.631996</td>\n      <td>3.964134</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>61009059</td>\n      <td>3.944621</td>\n      <td>3.961251</td>\n      <td>3.951273</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>380002930</td>\n      <td>3.689170</td>\n      <td>4.232122</td>\n      <td>3.906351</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"          ISBN  iPrediction  uPrediction  finalPrediction\n352  446606189     8.835329     7.387121         8.256046\n179  440234743     8.786790     7.333763         8.205579\n359  142000205     8.391583     6.847254         7.773851\n297  786881852     6.321247     5.910907         6.157111\n60   440214041     5.330645     5.854014         5.539993\n58   312195516     3.746369     5.470936         4.436196\n160  312995423     4.319208     3.501948         3.992304\n35    60502258     3.518893     4.631996         3.964134\n11    61009059     3.944621     3.961251         3.951273\n55   380002930     3.689170     4.232122         3.906351"},"exec_count":26,"output_type":"execute_result"}},"pos":36,"type":"cell"}
{"cell_type":"code","exec_count":3,"id":"62194f","input":"print(\"Mean book rating:     \", '%.2f' % bx['Book-Rating'].mean())","metadata":{"hidden":true},"output":{"0":{"name":"stdout","output_type":"stream","text":"Mean book rating:      2.63\n"}},"pos":4,"type":"cell"}
{"cell_type":"code","exec_count":4,"id":"c23dda","input":"#Import the train_test_split function\nfrom sklearn.model_selection import train_test_split\n\n#Assign X as the original ratings dataframe and y as the user_id column of ratings.\nX = bx.copy()\ny = bx['User-ID']\n\n#Split into training and test datasets, stratified along user_id\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, stratify=y, random_state=42)","metadata":{"hidden":true},"pos":5,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"fccc5c","input":"#verify the median of the data\nprint(f\"The median of this rating range is {np.median(np.arange(np.min(bx['Book-Rating']), (np.max(bx['Book-Rating']) + 1)))}\")\n\n#Define the baseline model to always the scale median.\ndef baseline(user_id, item_id, scale_median,  *args):\n    return scale_median","metadata":{"hidden":true},"output":{"0":{"name":"stdout","output_type":"stream","text":"The median of this rating range is 6.0\n"}},"pos":7,"type":"cell"}
{"cell_type":"code","exec_count":6,"id":"58151a","input":"#Function to compute the RMSE score obtained on the testing set by a model\ndef score(cf_model, X_test, *args):\n    \n    #Construct a list of user-book tuples from the testing dataset\n    id_pairs = zip(X_test[X_test.columns[0]], X_test[X_test.columns[1]])\n    \n    #Predict the rating for every user-item tuple\n    y_pred = np.array([cf_model(user, item, *args) for (user, item) in id_pairs])\n    \n    #Extract the actual ratings given by the users in the test data\n    y_true = np.array(X_test[X_test.columns[2]])\n    \n    #Return the final RMSE score\n    return mean_squared_error(y_true, y_pred, squared=False)","metadata":{"hidden":true},"pos":8,"type":"cell"}
{"cell_type":"code","exec_count":7,"id":"300230","input":"score(baseline, X_test, 6)","metadata":{"hidden":true},"output":{"0":{"data":{"text/plain":"4.703780985075257"},"exec_count":7,"output_type":"execute_result"}},"pos":9,"type":"cell"}
{"cell_type":"code","exec_count":8,"id":"081409","input":"#Build the ratings matrix using pivot_table function\n#r_matrix = X_train.pivot_table(values='Book-Rating', index='User-ID', columns='ISBN')\nr_matrix = X_train.pivot(values='Book-Rating', index='User-ID', columns='ISBN')\n\nr_matrix.head()","metadata":{"hidden":true},"output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>ISBN</th>\n      <th>006101351X</th>\n      <th>014025448X</th>\n      <th>014028009X</th>\n      <th>034540288X</th>\n      <th>038079487X</th>\n      <th>043935806X</th>\n      <th>044021145X</th>\n      <th>044022165X</th>\n      <th>044023722X</th>\n      <th>044651652X</th>\n      <th>...</th>\n      <th>743418174</th>\n      <th>767902521</th>\n      <th>767905180</th>\n      <th>786868716</th>\n      <th>786881852</th>\n      <th>804106304</th>\n      <th>804114986</th>\n      <th>805063897</th>\n      <th>842329129</th>\n      <th>971880107</th>\n    </tr>\n    <tr>\n      <th>User-ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6251</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6575</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7346</th>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>11601</th>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>11676</th>\n      <td>9.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 200 columns</p>\n</div>","text/plain":"ISBN     006101351X  014025448X  014028009X  034540288X  038079487X  \\\nUser-ID                                                               \n6251            NaN         NaN         NaN         1.0         NaN   \n6575            NaN         NaN         NaN         NaN         NaN   \n7346            1.0         NaN         NaN         NaN         NaN   \n11601           NaN         1.0         NaN         NaN         NaN   \n11676           9.0         NaN         NaN         NaN         NaN   \n\nISBN     043935806X  044021145X  044022165X  044023722X  044651652X  ...  \\\nUser-ID                                                              ...   \n6251            NaN         1.0         NaN         NaN         NaN  ...   \n6575            NaN         NaN         NaN         NaN         NaN  ...   \n7346            NaN         NaN         NaN         NaN         NaN  ...   \n11601           NaN         NaN         NaN         NaN         NaN  ...   \n11676           NaN         NaN         NaN         NaN         NaN  ...   \n\nISBN     743418174  767902521  767905180  786868716  786881852  804106304  \\\nUser-ID                                                                     \n6251           1.0        NaN        NaN        1.0        NaN        NaN   \n6575           NaN        NaN        NaN        NaN        NaN        NaN   \n7346           NaN        NaN        NaN        NaN        NaN       10.0   \n11601          NaN        NaN        NaN        NaN        NaN        NaN   \n11676          NaN        NaN        NaN        NaN        NaN        NaN   \n\nISBN     804114986  805063897  842329129  971880107  \nUser-ID                                              \n6251           NaN        NaN        NaN        NaN  \n6575           NaN        NaN        NaN        NaN  \n7346           1.0        NaN        NaN        NaN  \n11601          NaN        NaN        NaN        NaN  \n11676          NaN        NaN        NaN        NaN  \n\n[5 rows x 200 columns]"},"exec_count":8,"output_type":"execute_result"}},"pos":11,"type":"cell"}
{"cell_type":"code","exec_count":9,"id":"c66a8d","input":"#Create a dummy ratings matrix with all null values imputed to 0\nr_matrix_dummy = r_matrix.copy().fillna(0)","metadata":{"hidden":true},"pos":12,"type":"cell"}
{"cell_type":"markdown","id":"4d1c47","input":"# *Self-Assessment: Weighted Mean Item-Based Filter - Solution*","metadata":{"heading_collapsed":true},"pos":19,"type":"cell"}
{"cell_type":"markdown","id":"596fd7","input":"The RMSE for each model used so far are stated below ranked from best to worst:\n\n\n- weighted-mean item-based collaborative filter: RMSE = 3.41 \n\n- kNN-based item-based collaborative filter: (average) RMSE = 3.44\n\n- weighted-mean item-based collaborative filter: RMSE = 3.61 \n\n- kNN-based user-based collaborative filter: (average) RMSE = 3.69 (note that this one will vary slightly if you didn't set a seed or if you use a different seed)\n\n- baseline model: RMSE = 4.70.  ","pos":32,"type":"cell"}
{"cell_type":"markdown","id":"7b3c0b","input":"The RMSE for each model used so far are stated below ranked from best to worst:\n\n- weighted-mean item-based collaborative filter: RMSE = 3.41 \n\n- weighted-mean item-based collaborative filter: RMSE = 3.61 \n\n- kNN-based collaborative filter: (average) RMSE = 3.69 (note that this one will vary slightly if you didn't set a seed or if you use a different seed)\n\n- baseline model: RMSE = 4.70.  \n\n# *Self-Assessment: kNNBasic Item-based Collaborative Filter - Solution*","metadata":{"hidden":true},"pos":30,"type":"cell"}
{"cell_type":"markdown","id":"a22bd4","input":"The weighted-mean item-based collaborative filter is the best so far at RMSE = 3.41.  The weighted-mean item-based collaborative filter had RMSE = 3.61 and the baseline model had RMSE = 4.70.  ","metadata":{"hidden":true},"pos":26,"type":"cell"}
{"cell_type":"markdown","id":"a2ba2d","input":"# *Self-Assessment: Hybrid Recommender*","pos":33,"type":"cell"}
{"cell_type":"markdown","id":"a468cd","input":"# *Self-Assessment: kNN-Based Collaborative Filter - Solution*","metadata":{"heading_collapsed":true},"pos":27,"type":"cell"}
{"cell_type":"markdown","id":"b8a6d2","input":"# *Self-Assessment: Setting up the File*","metadata":{"heading_collapsed":true},"pos":2,"type":"cell"}
{"cell_type":"markdown","id":"c68129","input":"The RMSE with the user-based collaborative filter is 3.61 compared to 4.70 for the baseline model, so predicted ratings are more precise.  ","metadata":{"hidden":true},"pos":18,"type":"cell"}
{"cell_type":"markdown","id":"c9552e","input":"# *Self-Assessment: Baseline RMSE to Assess Model Performance*","metadata":{"heading_collapsed":true},"pos":6,"type":"cell"}
{"cell_type":"markdown","id":"fb19d8","input":"# *Self-Assessment: Weighted Mean User-Based Filter*","metadata":{"heading_collapsed":true},"pos":10,"type":"cell"}
{"cell_type":"markdown","id":"fe7788","input":"# Lesson 14 \\- Self\\-Assessment Solutions\n\n","pos":1,"type":"cell"}
{"id":0,"time":1683229566310,"type":"user"}
{"last_load":1683229566079,"type":"file"}