{"backend_state":"init","connection_file":"/tmp/xdg-runtime-user/jupyter/kernel-e0cd9f49-b067-4890-b96c-49ecab9b8602.json","kernel":"python3","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"234.363px"},"toc_section_display":true,"toc_window_display":true},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"trust":true,"type":"settings"}
{"cell_type":"code","exec_count":1,"id":"f8a408","input":"# EXECUTE FIRST\n\n# computational imports\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom surprise import Reader, Dataset, KNNBasic, NormalPredictor,BaselineOnly,KNNWithMeans,KNNBaseline\nfrom surprise import SVD, SVDpp, NMF, SlopeOne, CoClustering\nfrom surprise.model_selection import cross_validate\nfrom surprise.model_selection import GridSearchCV\nfrom surprise import accuracy\n\nimport random\nfrom ast import literal_eval\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel\n\n# plotting imports\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\nplt.style.use('ggplot')\n# for reading files from urls\nimport urllib.request\n# display imports\nfrom IPython.display import display, IFrame\nfrom IPython.core.display import HTML\n\n# import notebook styling for tables and width etc.\nresponse = urllib.request.urlopen('https://raw.githubusercontent.com/DataScienceUWL/DS775v2/master/ds755.css')\nHTML(response.read().decode(\"utf-8\"));","metadata":{"code_folding":[0]},"pos":0,"type":"cell"}
{"cell_type":"code","exec_count":13,"id":"8b8d82","input":"movies = pd.DataFrame({'movie_id': [11,12,13,14,15],\n                      'title':['Jumanji','Balto','Treasure Island', 'Pocahontas', 'The Wizard of Oz'],\n                      'genres': [['adventure', 'fantasy','family'], ['family', 'animation','adventure'], ['adventure','family'], ['adventure', 'animation', 'drama', 'family'], ['adventure', 'family','fantasy']]\n                      })\n\ndisplay(movies.head())\n\n#generate a rating for each user/movie combination\nratings = pd.DataFrame(np.array(np.meshgrid([1, 2, 3,4,5], [11,12,13,14,15])).T.reshape(-1,2), columns=['user_id', 'movie_id'])\nnp.random.seed(1)\nrandratings = np.random.randint(1,6, ratings.shape[0])\n\nratings['rating'] = randratings\n\n#we have 5 * 5 or 25 rows of data in the ratings, but we'll just look at the first 10\nratings.head(10)","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>movie_id</th>\n      <th>title</th>\n      <th>genres</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>11</td>\n      <td>Jumanji</td>\n      <td>[adventure, fantasy, family]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>12</td>\n      <td>Balto</td>\n      <td>[family, animation, adventure]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13</td>\n      <td>Treasure Island</td>\n      <td>[adventure, family]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>14</td>\n      <td>Pocahontas</td>\n      <td>[adventure, animation, drama, family]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>15</td>\n      <td>The Wizard of Oz</td>\n      <td>[adventure, family, fantasy]</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   movie_id             title                                 genres\n0        11           Jumanji           [adventure, fantasy, family]\n1        12             Balto         [family, animation, adventure]\n2        13   Treasure Island                    [adventure, family]\n3        14        Pocahontas  [adventure, animation, drama, family]\n4        15  The Wizard of Oz           [adventure, family, fantasy]"},"exec_count":13,"output_type":"execute_result"},"1":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>movie_id</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>11</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>12</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>13</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>14</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>15</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2</td>\n      <td>11</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2</td>\n      <td>12</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2</td>\n      <td>13</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2</td>\n      <td>14</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2</td>\n      <td>15</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   user_id  movie_id  rating\n0        1        11       4\n1        1        12       5\n2        1        13       1\n3        1        14       2\n4        1        15       4\n5        2        11       1\n6        2        12       1\n7        2        13       2\n8        2        14       5\n9        2        15       5"},"exec_count":13,"output_type":"execute_result"}},"pos":4,"type":"cell"}
{"cell_type":"code","exec_count":14,"id":"6c231a","input":"#Import the train_test_split function\n# from sklearn.model_selection import train_test_split\n\n#Assign X as the original ratings dataframe and y as the user_id column of ratings.\nX = ratings.copy()\ny = ratings['user_id']\n\n#Split into training and test datasets, stratified along user_id\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, stratify=y, random_state=42)","pos":6,"type":"cell"}
{"cell_type":"code","exec_count":15,"id":"c0534e","input":"#compare X_train to X_test\ndisplay(X_train)\ndisplay(X_test)","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>movie_id</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>12</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>5</td>\n      <td>11</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>5</td>\n      <td>13</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2</td>\n      <td>14</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2</td>\n      <td>11</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>4</td>\n      <td>13</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>5</td>\n      <td>15</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>3</td>\n      <td>11</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2</td>\n      <td>12</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2</td>\n      <td>15</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>13</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>3</td>\n      <td>13</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>4</td>\n      <td>14</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>3</td>\n      <td>14</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>14</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>3</td>\n      <td>12</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>5</td>\n      <td>14</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>4</td>\n      <td>11</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"    user_id  movie_id  rating\n1         1        12       5\n20        5        11       5\n22        5        13       2\n8         2        14       5\n5         2        11       1\n17        4        13       3\n24        5        15       2\n10        3        11       2\n6         2        12       1\n9         2        15       5\n2         1        13       1\n12        3        13       5\n18        4        14       5\n13        3        14       3\n3         1        14       2\n11        3        12       3\n23        5        14       1\n15        4        11       4"},"exec_count":15,"output_type":"execute_result"},"1":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>movie_id</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>16</th>\n      <td>4</td>\n      <td>12</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2</td>\n      <td>13</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>4</td>\n      <td>15</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>5</td>\n      <td>12</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>3</td>\n      <td>15</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>15</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>11</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"    user_id  movie_id  rating\n16        4        12       5\n7         2        13       2\n19        4        15       3\n21        5        12       2\n14        3        15       5\n4         1        15       4\n0         1        11       4"},"exec_count":15,"output_type":"execute_result"}},"pos":8,"type":"cell"}
{"cell_type":"code","exec_count":16,"id":"27dd86","input":"#Import the mean_squared_error function\n# from sklearn.metrics import mean_squared_error\n\n#test data\ntest_y_true = [3, -0.5, 2, 7]\ntest_y_pred = [2.5, 0.0, 2, 8]\n\n#this returns MSE (not what we want)\nprint(mean_squared_error(test_y_true, test_y_pred))\n\n#this returns the root mean squared error (and is what we want to use)\nmean_squared_error(test_y_true, test_y_pred, squared=False)","output":{"0":{"name":"stdout","output_type":"stream","text":"0.375\n"},"1":{"data":{"text/plain":"0.6123724356957945"},"exec_count":16,"output_type":"execute_result"}},"pos":11,"type":"cell"}
{"cell_type":"code","exec_count":17,"id":"f9e398","input":"#explaining getting the median....\nprint('Our min is', np.min(ratings['rating']))\nprint('Our max plus one is', np.max(ratings['rating']) + 1)\nprint('If we do not plus one the arange, we only get our max - 1', np.arange(np.min(ratings['rating']), (np.max(ratings['rating']))))\nprint('If we plus it, we get the actual max', np.arange(np.min(ratings['rating']), (np.max(ratings['rating']) + 1)))\n\nprint('The median of the scale is then ', np.median(np.arange(np.min(ratings['rating']), (np.max(ratings['rating']) + 1))))","output":{"0":{"name":"stdout","output_type":"stream","text":"Our min is 1\nOur max plus one is 6\nIf we do not plus one the arange, we only get our max - 1 [1 2 3 4]\nIf we plus it, we get the actual max [1 2 3 4 5]\nThe median of the scale is then  3.0\n"}},"pos":13,"type":"cell"}
{"cell_type":"code","exec_count":18,"id":"990846","input":"#first determine the median of our ratings (we could have done this by hand, but numpy does it so well... )\nprint(f\"The median of this rating range is {np.median(np.arange(np.min(ratings['rating']), (np.max(ratings['rating']) + 1)))}\")\n\n#define a baseline model to always return the median\ndef baseline(user_id, movie_id, scale_median, *args):\n    '''\n    Parameters:\n    user_id, movie_id, *args: None of which are used, but are included for consistency\n    scale_median: the median of the rating scale\n\n    Returns:\n    the median of the rating scale\n    '''\n    return scale_median","output":{"0":{"name":"stdout","output_type":"stream","text":"The median of this rating range is 3.0\n"}},"pos":15,"type":"cell"}
{"cell_type":"code","exec_count":19,"id":"eea916","input":"#Function to compute the RMSE score obtained on the testing set by a model\ndef score(cf_model, X_test, *args):\n    '''\n    Parameters:\n    cf_model: This is the model that will be used to predict ratings\n    X_test: This is the data (a dataframe of user_id, item_id, rating) for which we'll predict ratings\n    *args: any additional values that should be passed to the cf_model\n    Returns:\n    The final mean_squared_error\n    '''\n    \n    #Construct a list of user-movie tuples from the testing dataset\n    id_pairs = zip(X_test[X_test.columns[0]], X_test[X_test.columns[1]])\n    \n    #Predict the rating for every user-item tuple\n    y_pred = np.array([cf_model(user, item, *args) for (user, item) in id_pairs])\n    \n    #Extract the actual ratings given by the users in the test data\n    y_true = np.array(X_test[X_test.columns[2]])\n\n    #Return the final RMSE score\n    return mean_squared_error(y_true, y_pred, squared=False)\n                              \n#let's test it with our baseline model\nscore(baseline, X_test, 3)","output":{"0":{"data":{"text/plain":"1.3093073414159542"},"exec_count":19,"output_type":"execute_result"}},"pos":17,"type":"cell"}
{"cell_type":"code","exec_count":20,"id":"7c2b3e","input":"#Build the ratings matrix using pivot_table function\nr_matrix = X_train.pivot_table(values='rating', index='user_id', columns='movie_id')\n\nr_matrix.head()","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>movie_id</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n    </tr>\n    <tr>\n      <th>user_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>5.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>3.0</td>\n      <td>5.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5.0</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"movie_id   11   12   13   14   15\nuser_id                          \n1         NaN  5.0  1.0  2.0  NaN\n2         1.0  1.0  NaN  5.0  5.0\n3         2.0  3.0  5.0  3.0  NaN\n4         4.0  NaN  3.0  5.0  NaN\n5         5.0  NaN  2.0  1.0  2.0"},"exec_count":20,"output_type":"execute_result"}},"pos":20,"type":"cell"}
{"cell_type":"code","exec_count":21,"id":"490856","input":"#User Based Collaborative Filter using Mean Ratings\ndef cf_mean(user_id, item_id, ratings_matrix, scale_median):\n    '''\n    Parameters:\n    user_id: We don't use this variable, but it's passed through for consistency with other models\n    item_id: The item for which we are generating predicted rating\n    ratings_matrix: The matrix of ratings with items as columns and users as rows\n    scale_median: The median value of the rating scale, which will be used as the default value.\n    Returns:\n    The mean rating\n    \n    '''\n    #Check if item_id exists in ratings_matrix\n    if item_id in ratings_matrix:\n        #Compute the mean of all the ratings given to the item\n        mean_rating = ratings_matrix[item_id].mean()\n    \n    else:\n        #Default to a rating of the scale median in the absence of any information\n        mean_rating = scale_median\n    \n    return mean_rating\n","pos":23,"type":"cell"}
{"cell_type":"code","exec_count":22,"id":"ad6b03","input":"cf_mean(0,15,r_matrix, 3)","output":{"0":{"data":{"text/plain":"3.5"},"exec_count":22,"output_type":"execute_result"}},"pos":25,"type":"cell"}
{"cell_type":"code","exec_count":23,"id":"f80ccb","input":"#score all the test data\nscore(cf_mean, X_test, r_matrix, 3)","output":{"0":{"data":{"text/plain":"1.153411090139653"},"exec_count":23,"output_type":"execute_result"}},"pos":27,"type":"cell"}
{"cell_type":"code","exec_count":24,"id":"681c5c","input":"#Create a dummy ratings matrix with all null values imputed to 0\nr_matrix_dummy = r_matrix.copy().fillna(0)\n\n#Compute the cosine similarity matrix using the dummy ratings matrix\ncosine_sim = cosine_similarity(r_matrix_dummy, r_matrix_dummy)\n\n#Convert into pandas dataframe \ncosine_sim = pd.DataFrame(cosine_sim, index=r_matrix.index, columns=r_matrix.index)\n\ncosine_sim.head(10)","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>user_id</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n    </tr>\n    <tr>\n      <th>user_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>1.000000</td>\n      <td>0.379777</td>\n      <td>0.692411</td>\n      <td>0.335659</td>\n      <td>0.125245</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.379777</td>\n      <td>1.000000</td>\n      <td>0.404557</td>\n      <td>0.568737</td>\n      <td>0.475651</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.692411</td>\n      <td>0.404557</td>\n      <td>1.000000</td>\n      <td>0.783880</td>\n      <td>0.575360</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.335659</td>\n      <td>0.568737</td>\n      <td>0.783880</td>\n      <td>1.000000</td>\n      <td>0.751860</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.125245</td>\n      <td>0.475651</td>\n      <td>0.575360</td>\n      <td>0.751860</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"user_id         1         2         3         4         5\nuser_id                                                  \n1        1.000000  0.379777  0.692411  0.335659  0.125245\n2        0.379777  1.000000  0.404557  0.568737  0.475651\n3        0.692411  0.404557  1.000000  0.783880  0.575360\n4        0.335659  0.568737  0.783880  1.000000  0.751860\n5        0.125245  0.475651  0.575360  0.751860  1.000000"},"exec_count":24,"output_type":"execute_result"}},"pos":29,"type":"cell"}
{"cell_type":"code","exec_count":25,"id":"3a4589","input":"#extract the 2 vectors and calculate the dot product\nu = [0.335659, 0.568737, 0.783880]\nr = [5,1,3]\nnp.dot(u,r)","output":{"0":{"data":{"text/plain":"4.598672"},"exec_count":25,"output_type":"execute_result"}},"pos":31,"type":"cell"}
{"cell_type":"code","exec_count":26,"id":"2fba1a","input":"#the dot product is the same as this hand-calculation, which takes the sum of the products of the 2 values at the same index in each vector\nprint((5*.335659) + (1*.56737) + (3*.783880) )\n#the denominator is the sum of the similarity scores\nprint(sum(u))\n#so the full rating ends up being this\nprint(((5*.335659) + (1*.56737) + (3*.783880))/sum(u))","output":{"0":{"name":"stdout","output_type":"stream","text":"4.597305\n1.688276\n2.7230766770362194\n"}},"pos":32,"type":"cell"}
{"cell_type":"code","exec_count":27,"id":"deecaf","input":"r_matrix","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>movie_id</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n    </tr>\n    <tr>\n      <th>user_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>5.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>3.0</td>\n      <td>5.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5.0</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"movie_id   11   12   13   14   15\nuser_id                          \n1         NaN  5.0  1.0  2.0  NaN\n2         1.0  1.0  NaN  5.0  5.0\n3         2.0  3.0  5.0  3.0  NaN\n4         4.0  NaN  3.0  5.0  NaN\n5         5.0  NaN  2.0  1.0  2.0"},"exec_count":27,"output_type":"execute_result"}},"pos":34,"type":"cell"}
{"cell_type":"code","exec_count":28,"id":"31f206","input":"#User Based Collaborative Filter using Weighted Mean Ratings\ndef cf_wmean(user_id, item_id, ratings_matrix, c_sim_matrix, median_rating):\n    \n    #Check if item_id exists in r_matrix\n    if item_id in ratings_matrix:\n        print(f'UserID: {user_id} - ItemID {item_id}')\n        #Get the similarity scores for the user in question with every other user\n        sim_scores = c_sim_matrix[user_id]\n\n        #Get the user ratings for the item in question\n        i_ratings = ratings_matrix[item_id]\n\n        #Extract the indices containing NaN in the i_ratings series\n        idx = i_ratings[i_ratings.isnull()].index\n\n\n        #Drop the NaN values from the i_ratings Series\n        i_ratings = i_ratings.dropna()\n\n        #Drop the corresponding cosine scores from the sim_scores series\n        sim_scores = sim_scores.drop(idx)\n\n        print(f'Final calculation: np.dot({sim_scores.tolist()}, {i_ratings.tolist()})/sum({sim_scores.tolist()})' )\n        #Compute the final weighted mean\n        if sim_scores.sum()>0:\n            wmean_rating = np.dot(sim_scores, i_ratings)/ sim_scores.sum()\n        else:  # user had zero cosine similarity with other users\n            wmean_rating = median_rating\n\n    else:\n        #Default to the median in the absence of any information\n        wmean_rating = median_rating\n    \n    return wmean_rating\n\n\n\n#we can call our score function to get the RMSE\nscore(cf_wmean, X_test, r_matrix, cosine_sim, 3)\n","output":{"0":{"name":"stdout","output_type":"stream","text":"UserID: 4 - ItemID 12\nFinal calculation: np.dot([0.33565855667130945, 0.5687367919007337, 0.783880147156683], [5.0, 1.0, 3.0])/sum([0.33565855667130945, 0.5687367919007337, 0.783880147156683])\nUserID: 2 - ItemID 13\nFinal calculation: np.dot([0.379777262656375, 0.4045566970313675, 0.5687367919007337, 0.47565149415449404], [1.0, 5.0, 3.0, 2.0])/sum([0.379777262656375, 0.4045566970313675, 0.5687367919007337, 0.47565149415449404])\nUserID: 4 - ItemID 15\nFinal calculation: np.dot([0.5687367919007337, 0.7518604376126321], [5.0, 2.0])/sum([0.5687367919007337, 0.7518604376126321])\nUserID: 5 - ItemID 12\nFinal calculation: np.dot([0.12524485821702988, 0.47565149415449404, 0.575359712265399], [5.0, 1.0, 3.0])/sum([0.12524485821702988, 0.47565149415449404, 0.575359712265399])\nUserID: 3 - ItemID 15\nFinal calculation: np.dot([0.4045566970313675, 0.575359712265399], [5.0, 2.0])/sum([0.4045566970313675, 0.575359712265399])\nUserID: 1 - ItemID 15\nFinal calculation: np.dot([0.379777262656375, 0.12524485821702988], [5.0, 2.0])/sum([0.379777262656375, 0.12524485821702988])\nUserID: 1 - ItemID 11\nFinal calculation: np.dot([0.379777262656375, 0.6924107336786997, 0.33565855667130945, 0.12524485821702988], [1.0, 2.0, 4.0, 5.0])/sum([0.379777262656375, 0.6924107336786997, 0.33565855667130945, 0.12524485821702988])\n"},"1":{"data":{"text/plain":"1.2892045169426132"},"exec_count":28,"output_type":"execute_result"}},"pos":35,"type":"cell"}
{"cell_type":"code","exec_count":29,"id":"c5244f","input":"ratings.rating.plot(kind='hist', bins=4, title='Actual Ratings')","output":{"0":{"data":{"text/plain":"<AxesSubplot:title={'center':'Actual Ratings'}, ylabel='Frequency'>"},"exec_count":29,"output_type":"execute_result"},"1":{"data":{"image/png":"8aa65094cdf2ea739cb5e0628456713e4fbf849d","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":29,"metadata":{"image/png":{"height":428,"width":719}},"output_type":"execute_result"}},"pos":39,"type":"cell"}
{"cell_type":"code","exec_count":30,"id":"5e5195","input":"our_seed = 14\n\n#Define a Reader object\n#The Reader object helps in parsing the file or dataframe containing ratings\nreader = Reader(rating_scale=(1,5)) # defaults to (0,5)\n\n#Create the dataset to be used for building the filter\ndata = Dataset.load_from_df(ratings, reader)\n\n#Define the algorithm object; in this case the normal predictor\nalgo = NormalPredictor() \n\n## apply the seeds right before cross validating, use both random and np.random\nrandom.seed(our_seed)\nnp.random.seed(our_seed)\n#This code cross validates (evaluates) the model\nalgo_cv = cross_validate(algo, data, measures=['RMSE'], cv=5, verbose=True)\nprint(algo_cv)\n\n#to extract the mean RMSE, we need to get the mean of the test_rmse values\nalgo_RMSE = np.mean(algo_cv['test_rmse'])\nprint(f'\\nThe RMSE across five folds was {algo_RMSE}')","output":{"0":{"name":"stdout","output_type":"stream","text":"Evaluating RMSE of algorithm NormalPredictor on 5 split(s).\n\n                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \nRMSE (testset)    2.0239  2.4687  1.6427  1.7709  1.4042  1.8621  0.3634  \nFit time          0.00    0.00    0.00    0.00    0.00    0.00    0.00    \nTest time         0.00    0.00    0.00    0.00    0.00    0.00    0.00    \n{'test_rmse': array([2.02385476, 2.46873783, 1.64274102, 1.77092294, 1.40421843]), 'fit_time': (0.00012922286987304688, 5.054473876953125e-05, 6.580352783203125e-05, 4.38690185546875e-05, 4.1961669921875e-05), 'test_time': (5.2928924560546875e-05, 2.9802322387695312e-05, 3.0517578125e-05, 2.8371810913085938e-05, 2.7418136596679688e-05)}\n\nThe RMSE across five folds was 1.8620949953786858\n"}},"pos":40,"type":"cell"}
{"cell_type":"code","exec_count":31,"id":"daf015","input":"#train on the whole dataset\ntrainset = data.build_full_trainset()\nalgo.fit(trainset)","output":{"0":{"data":{"text/plain":"<surprise.prediction_algorithms.random_pred.NormalPredictor at 0x7f44d465a430>"},"exec_count":31,"output_type":"execute_result"}},"pos":41,"type":"cell"}
{"cell_type":"code","exec_count":32,"id":"f326f8","input":"## apply the seeds right predicting, use both random and np.random\nrandom.seed(our_seed)\nnp.random.seed(our_seed)\n#first let's look at a single prediction for a user and item that were both in the dataset\nprint('Known user and item:', algo.predict(1, 11))\n#this user wasn't in the dataset\nprint('Unknown User: ', algo.predict(0, 11))\n#this item wasn't in the data set\nprint('Unknown item', algo.predict(1, 20))","output":{"0":{"name":"stdout","output_type":"stream","text":"Known user and item: user: 1          item: 11         r_ui = None   est = 5.00   {'was_impossible': False}\nUnknown User:  user: 0          item: 11         r_ui = None   est = 3.32   {'was_impossible': False}\nUnknown item user: 1          item: 20         r_ui = None   est = 3.46   {'was_impossible': False}\n"}},"pos":43,"type":"cell"}
{"cell_type":"code","exec_count":33,"id":"47cca4","input":"## apply the seeds right predicting, use both random and np.random\nrandom.seed(our_seed)\nnp.random.seed(our_seed)\n#run some predictions\npred_df = ratings.copy() #make a copy of the ratings that we can add columns to\n\n#get all the predictions (this could be computationally expensive and would probably only be computed occasionally)\npred_df['prediction'] = pred_df.apply(lambda x: algo.predict(x['user_id'], x['movie_id']).est, axis=1) \n\npred_df","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>movie_id</th>\n      <th>rating</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>11</td>\n      <td>4</td>\n      <td>5.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>12</td>\n      <td>5</td>\n      <td>3.318515</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>13</td>\n      <td>1</td>\n      <td>3.460384</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>14</td>\n      <td>2</td>\n      <td>3.091737</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>15</td>\n      <td>4</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2</td>\n      <td>11</td>\n      <td>1</td>\n      <td>3.416534</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2</td>\n      <td>12</td>\n      <td>1</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2</td>\n      <td>13</td>\n      <td>2</td>\n      <td>3.515960</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2</td>\n      <td>14</td>\n      <td>5</td>\n      <td>2.364555</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2</td>\n      <td>15</td>\n      <td>5</td>\n      <td>4.823174</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>3</td>\n      <td>11</td>\n      <td>2</td>\n      <td>2.921188</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>3</td>\n      <td>12</td>\n      <td>3</td>\n      <td>3.221943</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>3</td>\n      <td>13</td>\n      <td>5</td>\n      <td>1.590235</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>3</td>\n      <td>14</td>\n      <td>3</td>\n      <td>4.161235</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>3</td>\n      <td>15</td>\n      <td>5</td>\n      <td>2.930097</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>4</td>\n      <td>11</td>\n      <td>4</td>\n      <td>4.128380</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>4</td>\n      <td>12</td>\n      <td>5</td>\n      <td>4.413755</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>4</td>\n      <td>13</td>\n      <td>3</td>\n      <td>4.364308</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>4</td>\n      <td>14</td>\n      <td>5</td>\n      <td>2.844558</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>4</td>\n      <td>15</td>\n      <td>3</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>5</td>\n      <td>11</td>\n      <td>5</td>\n      <td>1.445729</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>5</td>\n      <td>12</td>\n      <td>2</td>\n      <td>4.451520</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>5</td>\n      <td>13</td>\n      <td>2</td>\n      <td>4.710542</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>5</td>\n      <td>14</td>\n      <td>1</td>\n      <td>5.000000</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>5</td>\n      <td>15</td>\n      <td>2</td>\n      <td>2.377836</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"    user_id  movie_id  rating  prediction\n0         1        11       4    5.000000\n1         1        12       5    3.318515\n2         1        13       1    3.460384\n3         1        14       2    3.091737\n4         1        15       4    1.000000\n5         2        11       1    3.416534\n6         2        12       1    1.000000\n7         2        13       2    3.515960\n8         2        14       5    2.364555\n9         2        15       5    4.823174\n10        3        11       2    2.921188\n11        3        12       3    3.221943\n12        3        13       5    1.590235\n13        3        14       3    4.161235\n14        3        15       5    2.930097\n15        4        11       4    4.128380\n16        4        12       5    4.413755\n17        4        13       3    4.364308\n18        4        14       5    2.844558\n19        4        15       3    1.000000\n20        5        11       5    1.445729\n21        5        12       2    4.451520\n22        5        13       2    4.710542\n23        5        14       1    5.000000\n24        5        15       2    2.377836"},"exec_count":33,"output_type":"execute_result"}},"pos":45,"type":"cell"}
{"cell_type":"code","exec_count":34,"id":"6963db","input":"our_seed = 14\n\n#Define a Reader object\n#The Reader object helps in parsing the file or dataframe containing ratings\nreader = Reader(rating_scale=(1,5)) # defaults to (0,5)\n\n#Create the dataset to be used for building the filter\ndata = Dataset.load_from_df(ratings, reader)\n\n#Define the algorithm object; in this case kNN\nknn = KNNBasic(k=3, verbose=False) #the default for k is 40, we're also setting verbose to False to supress messages\n\n## apply the seeds right before cross validating, use both random and np.random\nrandom.seed(our_seed)\nnp.random.seed(our_seed)\n#This code cross validates (evaluates) the model\nknn_cv = cross_validate(knn, data, measures=['RMSE'], cv=5, verbose=True)\nprint(knn_cv)\n\n#to extract the mean RMSE, we need to get the mean of the test_rmse values\nknn_RMSE = np.mean(knn_cv['test_rmse'])\nprint(f'\\nThe RMSE across five folds was {knn_RMSE}')","output":{"0":{"name":"stdout","output_type":"stream","text":"Evaluating RMSE of algorithm KNNBasic on 5 split(s).\n\n                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \nRMSE (testset)    1.4385  2.6866  2.1752  1.5797  2.2290  2.0218  0.4570  \nFit time          0.00    0.00    0.00    0.00    0.00    0.00    0.00    \nTest time         0.00    0.00    0.00    0.00    0.00    0.00    0.00    \n{'test_rmse': array([1.43851143, 2.6866473 , 2.17523353, 1.57965233, 2.22902989]), 'fit_time': (5.888938903808594e-05, 2.4318695068359375e-05, 3.123283386230469e-05, 2.4080276489257812e-05, 3.266334533691406e-05), 'test_time': (8.749961853027344e-05, 9.679794311523438e-05, 8.726119995117188e-05, 8.606910705566406e-05, 4.9114227294921875e-05)}\n\nThe RMSE across five folds was 2.021814896489965\n"}},"pos":48,"type":"cell"}
{"cell_type":"code","exec_count":35,"id":"36a221","input":"#Define a Reader object\n#The Reader object helps in parsing the file or dataframe containing ratings\nreader = Reader(rating_scale=(1,5)) # defaults to (0,5)\n\n\n#Create the dataset to be used for building the filter\ndata = Dataset.load_from_df(ratings, reader)\n\n#get the raw ratings\nraw_ratings = data.raw_ratings\n\n# shuffle ratings - set the seed here for homework\nrandom.seed(our_seed)\nnp.random.seed(our_seed)\nrandom.shuffle(raw_ratings)\n\n#A = 90% of the data, B = 10% of the data\nthreshold = int(.9 * len(raw_ratings))\nA_raw_ratings = raw_ratings[:threshold]\nB_raw_ratings = raw_ratings[threshold:]\n\ndata.raw_ratings = A_raw_ratings  # data is now the set A\n\n# Select your best algo with grid search.\nprint('Grid Search...')\nparam_grid = {'k': [3,5], 'min_k': [1,3]} #this will all combinations of max k of 3 and 5 and min k of 1 and 3\ngrid_search = GridSearchCV(KNNBasic, param_grid, measures=['rmse'], cv=3)\ngrid_search.fit(data)\n\nknn_gs_algo = grid_search.best_estimator['rmse']\n\n\n# retrain on the whole set A\ntrainset = data.build_full_trainset()\nknn_gs_algo.fit(trainset)\n\n# Compute biased accuracy on A \npredictions = knn_gs_algo.test(trainset.build_testset())\nprint(f'Biased accuracy on A = {accuracy.rmse(predictions)}')\n\n\n# Compute unbiased accuracy on B\ntestset = data.construct_testset(B_raw_ratings)  # testset is now the set B\npredictions = knn_gs_algo.test(testset)\nprint(f'Unbiased accuracy on B = {accuracy.rmse(predictions)}')\n","output":{"0":{"name":"stdout","output_type":"stream","text":"Grid Search...\nComputing the msd similarity matrix...\nDone computing similarity matrix.\nComputing the msd similarity matrix...\nDone computing similarity matrix.\nComputing the msd similarity matrix...\nDone computing similarity matrix.\nComputing the msd similarity matrix...\nDone computing similarity matrix.\nComputing the msd similarity matrix...\nDone computing similarity matrix.\nComputing the msd similarity matrix...\nDone computing similarity matrix.\nComputing the msd similarity matrix...\nDone computing similarity matrix.\nComputing the msd similarity matrix...\nDone computing similarity matrix.\nComputing the msd similarity matrix...\nDone computing similarity matrix.\nComputing the msd similarity matrix...\nDone computing similarity matrix.\nComputing the msd similarity matrix...\nDone computing similarity matrix.\nComputing the msd similarity matrix...\nDone computing similarity matrix.\nComputing the msd similarity matrix...\nDone computing similarity matrix.\nRMSE: 0.4576\nBiased accuracy on A = 0.45757618754681445\nRMSE: 1.3708\nUnbiased accuracy on B = 1.3707724765638012\n"}},"pos":50,"type":"cell"}
{"cell_type":"code","exec_count":36,"id":"9d8d33","input":"#we can see what our best parameters were\ngrid_search.best_params['rmse']","output":{"0":{"data":{"text/plain":"{'k': 3, 'min_k': 3}"},"exec_count":36,"output_type":"execute_result"}},"pos":51,"type":"cell"}
{"cell_type":"code","exec_count":37,"id":"7b66b1","input":"#set our seeds again\nrandom.seed(our_seed)\nnp.random.seed(our_seed)\n\n#reset the data.raw_ratings to 100% of the data\ndata.raw_ratings = raw_ratings\n\n#build a trainset\ntrainset = data.build_full_trainset()\n\n#build the algorithm with our best parameters\nknn_gs_algo = grid_search.best_estimator['rmse']\n\n#fit to the data\nknn_gs_algo.fit(trainset)\n\n#predict user 1, movie 11\nknn_gs_algo.predict(1, 11)","output":{"0":{"name":"stdout","output_type":"stream","text":"Computing the msd similarity matrix...\nDone computing similarity matrix.\n"},"1":{"data":{"text/plain":"Prediction(uid=1, iid=11, r_ui=None, est=4.158597662771285, details={'actual_k': 3, 'was_impossible': False})"},"exec_count":37,"output_type":"execute_result"}},"pos":53,"type":"cell"}
{"cell_type":"code","exec_count":38,"id":"0a2117","input":"#Define the SVD algorithm object\nsvd = SVD()\n## apply the seeds right before cross validating, use both random and np.random\nrandom.seed(our_seed)\nnp.random.seed(our_seed)\n#Evaluate the performance in terms of RMSE\nsvd_cv = cross_validate(svd, data, measures=['RMSE'], cv=5, verbose=True)\n#to extract the mean RMSE, we need to get the mean of the test_rmse values\nsvd_RMSE = np.mean(svd_cv['test_rmse'])\nprint(f'\\nThe RMSE across five folds was {svd_RMSE}')\n\n#train on the whole dataset\ntrainset = data.build_full_trainset()\nsvd.fit(trainset)","output":{"0":{"name":"stdout","output_type":"stream","text":"Evaluating RMSE of algorithm SVD on 5 split(s).\n\n                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \nRMSE (testset)    1.7165  1.2531  1.9141  1.7461  1.1622  1.5584  0.2956  \nFit time          0.00    0.00    0.00    0.00    0.00    0.00    0.00    \nTest time         0.00    0.00    0.00    0.00    0.00    0.00    0.00    \n\nThe RMSE across five folds was 1.55841375329772\n"},"1":{"data":{"text/plain":"<surprise.prediction_algorithms.matrix_factorization.SVD at 0x7f44abcd6130>"},"exec_count":38,"output_type":"execute_result"}},"pos":56,"type":"cell"}
{"cell_type":"code","exec_count":39,"id":"2fd31c","input":"def fetchSimilarityMatrix(df, soupCol, vectorizer, vectorType='Tfidf'):\n    '''\n    Parameters\n    df: the dataframe containing a soup column to tranform\n    soupCol: The string title of the soup column\n    vectorizer: an initialized vectorizer, with all pre-processing you desire\n    vectorType: 'Tfidf' or 'Count' - representing the type of vectorizer you used.\n\n    Returns\n    Sparse Similarity Matrix\n    '''\n\n    # make sure the soup has no NaN\n    df[soupCol] = df[soupCol].fillna('')\n    nmatrix = vectorizer.fit_transform(df[soupCol])\n\n    #apply the appropriate vectorizer\n    if(vectorType=='Tfidf'):\n        print('Using Linear Kernel (Tfidf)')\n        sim =linear_kernel(nmatrix, nmatrix)\n    else:\n        print('Using Cosine_similarity')\n        sim = cosine_similarity(nmatrix, nmatrix)\n    return(sim)\n\ndef content_recommender(df, seed, seedCol, sim_matrix,  topN=5): \n    #get the indices based off the seedCol\n    indices = pd.Series(df.index, index=df[seedCol]).drop_duplicates()\n    \n    # Obtain the index of the item that matches our seed\n    idx = indices[seed]\n    \n    # Get the pairwsie similarity scores of all items and convert to tuples\n    sim_scores = list(enumerate(sim_matrix[idx]))\n    \n    #delete the item that was passed in\n    del sim_scores[idx]\n    \n    # Sort the items based on the similarity scores\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    \n    # Get the scores of the top-n most similar items.\n    sim_scores = sim_scores[:topN]\n    \n    # Get the item indices\n    item_indices = [i[0] for i in sim_scores]\n    \n    # Return the topN most similar items\n    return df.iloc[item_indices].copy()","pos":58,"type":"cell"}
{"cell_type":"code","exec_count":40,"id":"1847f7","input":"#make a soup from genres\nmovies['soup'] = movies['genres'].apply(lambda x: ' '.join(x))\n\n#set up a count vectorizer\ncount = CountVectorizer(lowercase=True, stop_words='english')\n\n#fetch our similarity matrix\nsim = fetchSimilarityMatrix(movies, 'soup', count, 'Count')\n\n#display it\npd.DataFrame(sim, index=movies['title'], columns=movies['title'])","output":{"0":{"name":"stdout","output_type":"stream","text":"Using Cosine_similarity\n"},"1":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>title</th>\n      <th>Jumanji</th>\n      <th>Balto</th>\n      <th>Treasure Island</th>\n      <th>Pocahontas</th>\n      <th>The Wizard of Oz</th>\n    </tr>\n    <tr>\n      <th>title</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Jumanji</th>\n      <td>1.000000</td>\n      <td>0.666667</td>\n      <td>0.816497</td>\n      <td>0.577350</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>Balto</th>\n      <td>0.666667</td>\n      <td>1.000000</td>\n      <td>0.816497</td>\n      <td>0.866025</td>\n      <td>0.666667</td>\n    </tr>\n    <tr>\n      <th>Treasure Island</th>\n      <td>0.816497</td>\n      <td>0.816497</td>\n      <td>1.000000</td>\n      <td>0.707107</td>\n      <td>0.816497</td>\n    </tr>\n    <tr>\n      <th>Pocahontas</th>\n      <td>0.577350</td>\n      <td>0.866025</td>\n      <td>0.707107</td>\n      <td>1.000000</td>\n      <td>0.577350</td>\n    </tr>\n    <tr>\n      <th>The Wizard of Oz</th>\n      <td>1.000000</td>\n      <td>0.666667</td>\n      <td>0.816497</td>\n      <td>0.577350</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"title              Jumanji     Balto  Treasure Island  Pocahontas  \\\ntitle                                                               \nJumanji           1.000000  0.666667         0.816497    0.577350   \nBalto             0.666667  1.000000         0.816497    0.866025   \nTreasure Island   0.816497  0.816497         1.000000    0.707107   \nPocahontas        0.577350  0.866025         0.707107    1.000000   \nThe Wizard of Oz  1.000000  0.666667         0.816497    0.577350   \n\ntitle             The Wizard of Oz  \ntitle                               \nJumanji                   1.000000  \nBalto                     0.666667  \nTreasure Island           0.816497  \nPocahontas                0.577350  \nThe Wizard of Oz          1.000000  "},"exec_count":40,"output_type":"execute_result"}},"pos":60,"type":"cell"}
{"cell_type":"code","exec_count":41,"id":"f05f19","input":"results = content_recommender(movies, 'Jumanji', 'title', sim, 3)\nresults","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>movie_id</th>\n      <th>title</th>\n      <th>genres</th>\n      <th>soup</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4</th>\n      <td>15</td>\n      <td>The Wizard of Oz</td>\n      <td>[adventure, family, fantasy]</td>\n      <td>adventure family fantasy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13</td>\n      <td>Treasure Island</td>\n      <td>[adventure, family]</td>\n      <td>adventure family</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>12</td>\n      <td>Balto</td>\n      <td>[family, animation, adventure]</td>\n      <td>family animation adventure</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   movie_id             title                          genres  \\\n4        15  The Wizard of Oz    [adventure, family, fantasy]   \n2        13   Treasure Island             [adventure, family]   \n1        12             Balto  [family, animation, adventure]   \n\n                         soup  \n4    adventure family fantasy  \n2            adventure family  \n1  family animation adventure  "},"exec_count":41,"output_type":"execute_result"}},"pos":62,"type":"cell"}
{"cell_type":"code","exec_count":42,"id":"73be2d","input":"results['est_rating'] = results.apply(lambda x: knn_gs_algo.predict(1, x['movie_id']).est, axis=1)\nresults.sort_values('est_rating', ascending=False)\n","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>movie_id</th>\n      <th>title</th>\n      <th>genres</th>\n      <th>soup</th>\n      <th>est_rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>12</td>\n      <td>Balto</td>\n      <td>[family, animation, adventure]</td>\n      <td>family animation adventure</td>\n      <td>4.524207</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>15</td>\n      <td>The Wizard of Oz</td>\n      <td>[adventure, family, fantasy]</td>\n      <td>adventure family fantasy</td>\n      <td>3.507513</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13</td>\n      <td>Treasure Island</td>\n      <td>[adventure, family]</td>\n      <td>adventure family</td>\n      <td>1.509182</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   movie_id             title                          genres  \\\n1        12             Balto  [family, animation, adventure]   \n4        15  The Wizard of Oz    [adventure, family, fantasy]   \n2        13   Treasure Island             [adventure, family]   \n\n                         soup  est_rating  \n1  family animation adventure    4.524207  \n4    adventure family fantasy    3.507513  \n2            adventure family    1.509182  "},"exec_count":42,"output_type":"execute_result"}},"pos":64,"type":"cell"}
{"cell_type":"code","exec_count":43,"id":"1cbca2","input":"def hybrid(user,contentRecs, predCol,algorithm, N):\n    '''\n    Parameters\n    user: the user for whom we are making predictions\n    contentRecs: the dataframe of items (already limited by whatever content recommender you've used)\n    predCol: the column in contentRecs on which we'll be making predictions (the itemID column)\n    algorithm: a trained Surprise model that will be used for making predictions\n    N: the number of predictions to return\n    Returns\n    a pandas dataframe containing everything in the contentRecs dataframe, plus the estimated rating for the user requested\n    '''\n    \n    #generated predicted ratings\n    contentRecs['est_rating'] = contentRecs.apply(lambda x: algorithm.predict(user, x[predCol]).est, axis=1)\n    #sort the results\n    contentRecs = contentRecs.sort_values('est_rating', ascending=False)\n\n    #return the finalN number of results\n    return contentRecs.head(N)\n\n#here we're calling it with user 1, the results from our prior content_recommender function call, the 'movie_id' is the column we want ratings for using the 'knn_gs_algo' model and returning 2 recommendations\nhybrid(1, results, 'movie_id', knn_gs_algo, 2)","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>movie_id</th>\n      <th>title</th>\n      <th>genres</th>\n      <th>soup</th>\n      <th>est_rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>12</td>\n      <td>Balto</td>\n      <td>[family, animation, adventure]</td>\n      <td>family animation adventure</td>\n      <td>4.524207</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>15</td>\n      <td>The Wizard of Oz</td>\n      <td>[adventure, family, fantasy]</td>\n      <td>adventure family fantasy</td>\n      <td>3.507513</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   movie_id             title                          genres  \\\n1        12             Balto  [family, animation, adventure]   \n4        15  The Wizard of Oz    [adventure, family, fantasy]   \n\n                         soup  est_rating  \n1  family animation adventure    4.524207  \n4    adventure family fantasy    3.507513  "},"exec_count":43,"output_type":"execute_result"}},"pos":66,"type":"cell"}
{"cell_type":"code","exec_count":57,"id":"da8c25","input":"# load the data\nimport pandas as pd\nbx = pd.read_csv('./data/BX-Book-Ratings-3000.csv')","pos":71,"type":"cell"}
{"cell_type":"code","exec_count":58,"id":"35bca7","input":"# enter your code here","pos":72,"type":"cell"}
{"cell_type":"code","exec_count":59,"id":"60d04d","input":"# enter your code here","pos":75,"type":"cell"}
{"cell_type":"code","exec_count":60,"id":"099006","input":"# enter your code here","pos":78,"type":"cell"}
{"cell_type":"code","exec_count":61,"id":"5f4713","input":"# enter your code here","pos":81,"type":"cell"}
{"cell_type":"code","exec_count":62,"id":"00493c","input":"# enter your code here","pos":84,"type":"cell"}
{"cell_type":"code","exec_count":63,"id":"4c2c6c","input":"#enter your code here","pos":86,"type":"cell"}
{"cell_type":"code","exec_count":64,"id":"afdf6d","input":"# enter your code here","pos":89,"type":"cell"}
{"cell_type":"markdown","id":"03191c","input":"### RMSE Metric\n\nOur metric for evaluation will be the Root Mean Squared Error. Banik builds a wrapper function around scikit-learn's mean_squared_error function, but that's unnecessary as of scikit-learn version 0.22.1. The function has a parameter we can use to tell it to return the RMSE instead of the MSE.","pos":10,"type":"cell"}
{"cell_type":"markdown","id":"04f336","input":"### Mean","pos":21,"type":"cell"}
{"cell_type":"markdown","id":"0678c6","input":"We already have a variety of trained algorithms we could use to predict how a certain user would rate these movies. Let's add a prediction for user 1 to our results dataframe using our trained knn_gs_algo, and sort in descending order by those predictions","pos":63,"type":"cell"}
{"cell_type":"markdown","id":"068370","input":"With the cosine similarity matrix in hand, we can set up the weighted mean function. This function needs 2 additional arguments - the rating_matrix and the cosine similarity matrix (c_sim_matrix).","pos":33,"type":"cell"}
{"cell_type":"markdown","id":"092ba8","input":"### *Self-Assessment: kNN-Based Collaborative Filter*","pos":82,"type":"cell"}
{"cell_type":"markdown","id":"09f6d9","input":"With the data loaded, our job is to predict the rating, given a user and a movie. We will do this as a regression problem. In some instances, we could view this as categorical data instead of numerical data, because we have discrete values from 1 to 5. But, since this is ordinal data (the order of the numbers has meaning), we'll treat it as continuous data. We want our regressor to \"understand\" that a mistaken rating of 1 when it should be 5 is a bigger mistake than a rating of 4 would be. Classification problems don't understand that nuance.\n\nLet's split the data into train and test sets. Banik uses a hack here to stratify on the user. Stratifying on the user ensures that we have some of each user's ratings in both the train and the test set.","pos":5,"type":"cell"}
{"cell_type":"markdown","id":"0d205f","input":"Use the *surprise* library in Python to build an kNNBasic collaborative filtering model for the BX-Books ratings.  Compute the average RMSE for this model from 5 cross-validations, using a k of 5. Do not tune the hyperparameters. Compare it to the RMSEs of the baseline, weighted mean user-based, and weighted mean item-based models previously obtained. Use a seed of 1.\n\n\n","pos":83,"type":"cell"}
{"cell_type":"markdown","id":"1001fd","input":"Another kind of hybrid recommender system is one which uses multiple recommenders, but weights the recommendations of each recommender to achieve a final set of recommendations. For this self assessment, you're going to write a function that takes a ratings dataframe, a userid, 2 trained Surprise models, a weight (that's less than 1 - meaning it will be a decimal representing the percent of confidence we have in that algorithm) for your first recommender, and a number indicating how many recommendations to return. \n\nYour recommender should do the following:\n* generate a dataframe of unique items from the ratings dataframe (there are multiple ways to accomplish this)\n* generate predicted rating for each combination of passed in userid and item using the first Surprise model (add a column to your unique items dataframe)\n* generate predicted rating for each combination of passed in userid and item using the second Surprise model (add another column to your unique items dataframe)\n* generate a final rating that multiplies the predicted rating of each model by their weight and adds them. (Use a lambda function and create a finalRating column.) Note that your weights should add up to 1, so your weight for your second recommender will be 1-the weight for your first recommender.\n* sort your unique items by the final rating and return the top N recommendations with their ratings.\n\nTo test your function:\n* Use userid 31315\n* Use an item-based KNNBasic algorithm. Do not set k (just let it use the default). Train it on the complete dataset (do not cross validate).\n* Use an SVD algorithm with all the default parameters. Train it on the entire dataset. (Do not cross-validate.)\n* Weight the KNNBasic algorithm with .6. \n* Return the top 10 book recommendations.\n","pos":88,"type":"cell"}
{"cell_type":"markdown","id":"107640","input":"### Weighted Mean\nWeighted mean is going to give more weight to the users that are more similar to each other. We'll do this using cosine similarity. Let's look at the function from the book:\n\n\n$$r_{u, m}=\\frac{\\sum_{u^{\\prime}, u^{\\prime} \\neq u} \\operatorname{sim}\\left(u, u^{\\prime}\\right) \\cdot r_{u^{\\prime}, m}}{\\sum_{u^{\\prime}, u^{\\prime} \\neq u}\\left|\\operatorname{sim}\\left(u, u^{\\prime}\\right)\\right|}$$\n\n\nWhat this says is that the rating for each user-item combination will be the **dot product** of two vectors: \n* the similarity scores between this user and other users\n* the ratings of other users \n\nand this will be divided by the sum of the similarity scores. To calculate this value, we need a cosine similarity matrix between our user's ratings.\n\n","pos":28,"type":"cell"}
{"cell_type":"markdown","id":"27de09","input":"All of the above models were relatively simple and straightforward calculations, even if the code to call them was a little convoluted. \n\nMachine learning algorithms, on the other hand, can give us a more powerful approach, with more complicated calculations. But the <a href=\"http://surpriselib.com/\">Surprise package</a> makes the code to call them surprisingly simple.\n\nWe're providing some sample code below and a walkthrough video to introduce  you to using the surprise package:\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/pLDf62ztgE4\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>","pos":37,"type":"cell"}
{"cell_type":"markdown","id":"2cf0fd","input":"We're going to hand-code a series of models. All our models will take in a user_id and a movie_id, and attempt to predict the rating. (Generalizing this, we could say that they take in a user_id and an item_id, as movies is just one thing we could use this for.)\n\n\nLet's define a baseline model. Our hand-coded baseline model always returns the MEDIAN of our ratings scale (not the median of all of our user's ratings). In other words, our baseline model is trying to be as noncommittal as possible. Later you'll see how to do a different baseline model with the Surprise package that uses a random rating based on a normal distribution. \n\nLet's walk through how to get the median of our scale using Numpy.","pos":12,"type":"cell"}
{"cell_type":"markdown","id":"339a80","input":"<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/H91hfqNk71Y\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n\n\n## Set Up\n\n### Defining Data\nIn Chapter 6, Banik uses the movielens dataset to explore collaborative filtering. We're going to use what's called a \"toy\" dataset, which is just a very small dataset. This makes it easier to see what's happening at each step, though our predictions will be worse because we have much less data to go on.\n","pos":3,"type":"cell"}
{"cell_type":"markdown","id":"36ea5e","input":"Given this data, we'd want to recommend 'Balto' or 'The Wizard of Oz.' Would we want to recommend 'Treasure Island?' It's similar to 'Jumanji,' but our recommender estimates we wouldn't rate it very highly.\n\n## Putting it together\nLet's wrap up what we did above in a nice hybrid function. There's multiple ways we could do this. We'll show you one of them. In this approach, we're doing our content recommender outside our hybrid and passing in the resulting recommendations. In the homework, we'll ask you to pass in all the parameters needed to run your content recommender inside your hybrid function. \n","pos":65,"type":"cell"}
{"cell_type":"markdown","id":"39a8fd","input":"We can generate predictions for our entire dataframe by using a lambda function on each row of data. (Note that the first row of the dataframe matches our hand-coded prediction for user 1 and movie 11 above.)","pos":44,"type":"cell"}
{"cell_type":"markdown","id":"3f0527","input":"### *Self-Assessment: Weighted Mean Item-Based Filter*","pos":79,"type":"cell"}
{"cell_type":"markdown","id":"46c3ab","input":"Build a ratings matrix from the data frame of users, books, and ratings and build a user-based collaborative filtering model that weights mean rank using cosine similarity among users.  Fit the model on the training set and compute the RMSE for this model using the test set and compare it to the RMSE of the baseline model.  Is it better than baseline?  (*i.e.* is the RMSE smaller?)","pos":77,"type":"cell"}
{"cell_type":"markdown","id":"537587","input":"# Collaborative Filters\n","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"53948b","input":"The file **BX-Book-Ratings-3000.csv** (found in the presentation download for this lesson) is loaded here for you, though you may need to change the file path unless you create the same folder structure. Note that book ratings have been adjusted so the scale goes from 1 to 11.   \n\nRun the cell below it to load the file and then do the following:\n\n* display the first 5 lines of the data (get familiar with the data frame)\n* calculate the mean book rating for all books (just to get an idea)\n* split the data set so that 70\\% of a users ratings are in the training set and 30\\% are in the testing set","pos":70,"type":"cell"}
{"cell_type":"markdown","id":"54d607","input":"## Basic Models\n","pos":18,"type":"cell"}
{"cell_type":"markdown","id":"58fb3d","input":"### *Self-Assessment: KNNBasic Item-based Collaborative Filter*\nSurprise makes it easy to switch between user-based and item-based collaborative filtering. By default it's always using user-based. To switch it to item-based, you need to set the sim_options dictionary key \"user_based\" to False\n\nSet up a sim_options dictionary that sets user_based to false and k to 5. Use those sim_options to instantiate your KNNBasic algorithm and run 5-fold cross validation on all the data.\n\nConsult <a href=\"https://surprise.readthedocs.io/en/stable/prediction_algorithms.html#similarity-measures-configuration\">the documentation for examples</a>.\n","pos":85,"type":"cell"}
{"cell_type":"markdown","id":"67011b","input":"With our similarity matrix in hand, we can fetch our most similar movies. Let's fetch the 3 most similar movies to Jumanji. Since we have such a small dataset, that means we're only eliminating one. If you look at the dataframe above, can you predict which one will be eliminated?","pos":61,"type":"cell"}
{"cell_type":"markdown","id":"738956","input":"Let's look at what the cf_mean() function would return for movie 15. Movie 15 has 2 ratings: [5,2]. You can see that it returns the average of the ratings, or 3.5.","pos":24,"type":"cell"}
{"cell_type":"markdown","id":"7b6b23","input":"When Banik looked at demographics, he was using explicit data to determine what makes people \"similar\" and assuming that if they were similar in that respect, their taste in movies would also be similar as well. That might be a faulty assumption. Gender, occupation, or other simple characteristics may not have any bearing on how people rate movies. But, there might be some underlying trends in the data that do result in commonalities in ratings.\n\nK Nearest Neighbors tries to uncover these commonalities by training a model on some data and identifying clusters of users of users that are \"near\" one another.\n\nSpecifically, what this algorithm does is:\n- Find the k-nearest neighbors that have rated movie m\n- Outputs the average rating of the k users for the movie m\n\nThe <a href=\"https://surprise.readthedocs.io/en/stable/knn_inspired.html\">documentation for KNNBasic</a> goes over all the parameters you can set when you're setting up the algorithm.\n\nNote that in this toy set, since we only have a handful of neighbors, we will need to decrease the number of neighbors (k) that the algorithm takes into consideration. Otherwise, we'll just be getting the mean of all the considered ratings in each fold.","pos":47,"type":"cell"}
{"cell_type":"markdown","id":"87c900","input":"Next we need a way to score our model.\n\nHere's where we diverge from Banik's approach just a bit. Instead of relying on global variables, we will explicitly pass in our data for our scoring model. Note we're again using the special parameter \\*args. This tells our scoring function to accept any optional arguments we might need, and we'll pass those right along to our model.\n\nWe are also going to follow the example of the Surprise package and assume that our data has 3 columns in this order:\n* the user id\n* the item id\n* the rating\n\n(This means that the score method will work for any dataframe that's set up that way, regardless of the column names. It's the order that matters, not the names of the columns.)\n\nWe'll also use sklearn's built in RMSE function.\n\nHere's the complete function.","pos":16,"type":"cell"}
{"cell_type":"markdown","id":"88dd7b","input":"Since we have such a small dataset, we can explore what's in our training and test data. You can see that every user is in both the training and test data, though not in equal measure.\n","pos":7,"type":"cell"}
{"cell_type":"markdown","id":"8b973a","input":"Interestingly, our RMSE for the K-nearest neighbors algorithm is actually worse than our normal predictor. This is most likely because we have a tiny dataset, or we're using the wrong k value for this data.\n\nWe can use grid search to identify the best k for this set of data. We'll set up this grid search to get an unbiased accuracy metric.","pos":49,"type":"cell"}
{"cell_type":"markdown","id":"8cfb1f","input":"We're going to alter Banik's function so that it also accepts optional arguments. We don't need any for this function, but later we will need additional arguments and this keeps our coding consistent.","pos":14,"type":"cell"}
{"cell_type":"markdown","id":"9510a9","input":"We covered this in the video, but here's a breakdown of how we'd calculate the rating for user 4 and movie 12 (2 in the video, but we updated the IDs). We'll fold this into our weighted mean function below, but we're pulling it out here just for clarity.","pos":30,"type":"cell"}
{"cell_type":"markdown","id":"99187b","input":"### *Self-Assessment: Setting up the File*","pos":69,"type":"cell"}
{"cell_type":"markdown","id":"9adceb","input":"### *Self-Assessment: Hybrid Recommender*","pos":87,"type":"cell"}
{"cell_type":"markdown","id":"a54594","input":"### K Nearest Neighbors","pos":46,"type":"cell"}
{"cell_type":"markdown","id":"a8c2d8","input":"#### Calling predictions\nWe can use our trained model to predict ratings for any user/item combination. Remember that for this particular algorithm, even unknown users or unknown items will get individual rating estimates, since the algorithm is just returning a random number anyway.","pos":42,"type":"cell"}
{"cell_type":"markdown","id":"aa5cd2","input":"### Singular-value Decomposition (SVD)","pos":54,"type":"cell"}
{"cell_type":"markdown","id":"b74e6a","input":"### Baseline: Normal Predictor\nSurprise includes several baseline predictors. We'll take a look at the normal predictor. The normal predictor simply predicts a random number within your rating scale, and assumes that the ratings come from a normal distribution. If you look at the histogram of our ratings below, you can see that it's unlikely that our ratings follow a normal distribution. In fact, they don't. We generated them from a discrete uniform distribution - which is a distribution in which each of the numbers is equally likely. Given what we know about our actual ratings, we would not expect the normal predictor baseline to be a good baseline for our data. But, we'll use it anyway just to give you a feel for it.","pos":38,"type":"cell"}
{"cell_type":"markdown","id":"b7eb22","input":"The first thing we need to do is to decide which data to use for our content recommender. Since we're dealing with a toy dataset anyway, let's keep this simple and just use the genres as our soup. All we'll need to do to create our soup columns is to join the genres into a string. We'll use a simple count vectorizer with 'english' stopwords and no restriction on the max number of features.","pos":59,"type":"cell"}
{"cell_type":"markdown","id":"ba3dfe","input":"Create a new ratings matrix from the data frame of users, books, and ratings with the rows defined by books (*i.e.* items) and columns defined by users to build an item-based collaborative filtering model that weights mean rank using cosine similarity among items.  Fit the model on the training set and compute the RMSE for this model on the test and compare it to the RMSEs of the baseline and weighted mean user-based models.  Is this one better than baseline?","pos":80,"type":"cell"}
{"cell_type":"markdown","id":"be34ba","input":"Build a baseline model that assigns a neutral rating and compute the RMSE of these simple \"predictions\" using the testing set. Make sure to make this model accept \\*args so that it aligns with more complicated models.\n\nA neutral rating would occur at the midpoint of the rating scale.  Calculate the median of the rating scale to determine what the baseline model should return.","pos":74,"type":"cell"}
{"cell_type":"markdown","id":"c7c9f6","input":"<font size=18>Week 14: Recommender Systems 2</font>","pos":1,"type":"cell"}
{"cell_type":"markdown","id":"c96d76","input":"# Self Assessment\nFollow the examples and use the code files provided from from chapters 5-7 in **Hands-On Recommendation Systems with Python** by Rounak Banik to do the following self-assessment exercises.  \n\nThe self-assessments in this lesson will be using a subset of data from the Book-Crossing dataset.  Click <a href = http://www2.informatik.uni-freiburg.de/~cziegler/BX/> here </a> for more details on the Book-Crossing dataset.","pos":67,"type":"cell"}
{"cell_type":"markdown","id":"cb046f","input":"Everything we've done so far is just setting us up to be able to use something more than our baseline model to do some real user-based collaborative filtering. Now let's try out some basic approaches and compare them to our baseline model.\n\nBefore we can start, though, we need to do yet more data wrangling. We need a matrix that has movies as columns and users as rows, with each user's rating for that movie at the intersection. Note that although we know that every user has rated every movie, we don't have all that data in our training set, so we still end up with some NaN values.","pos":19,"type":"cell"}
{"cell_type":"markdown","id":"cc9b91","input":"# Hybrid Recommenders\n\nHybrid recommenders are probably the most common recommenders you'll see in the real world, and there are many approaches to building hybrid recommenders. Banik specifically walks through a one type of recommender that combines a content-based recommender with a collaborative filter. This is a relatively simplistic hybrid recommender, but it's a good place to start, since it combines the strengths of two different recommender types. \n\nAs you might guess, we have some suggested updates to his code. All that work you did last week to set up functions is really going to help you this week. Let's pull in the functions we used last week.\n","pos":57,"type":"cell"}
{"cell_type":"markdown","id":"cea0ec","input":"The theory behind SVD is covered in Banik's book. The very high-level concept is that it's a method that allows you to reduce the dimensions of a sparse matrix and \"fill in the blanks\" with predictions. Under the hood, the algorithm uses stochastic gradient descent to attempt to minimize errors. We don't expect you to understand all the intricacies. We would like you to understand a couple of the hyperparameters you can tune, which are some of the same hyperparameters in every stochastic gradient descent algorithm.\n\n* n_epochs: this is the number of times the minimization steps are performed. The higher the number of times, the longer the algorithm will work to find the minimum error. The default is 20.\n* learning rate: this is a number that determines how much to change the model each iteration. Think of it as how big of a step the model takes in each iteration. Too large and you may never find your minimum. Too small and your model will be very slow and could get stuck in a local minimum. The default is .005.\n* regularization: this is a penalty term applied to prevent model overfitting. The default is .02.\n\nWe won't demonstrate using these hyperparameters in the lesson, but you'll need them for the homework. Functionally, you'd use them the same way we used k and min_k with the basicKNN algorithm.\n\nThe code to do simple cross validation with SVD itself is extremely simple, once you've already got a suprise data object set up. Read the <a href=\"https://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVD\">full documentation</a> if you're curious.","pos":55,"type":"cell"}
{"cell_type":"markdown","id":"d01cd7","input":"## User-Based Collaborative Filter","pos":68,"type":"cell"}
{"cell_type":"markdown","id":"d42fbf","input":"### *Self-Assessment: Baseline RMSE to Assess Model Performance*","pos":73,"type":"cell"}
{"cell_type":"markdown","id":"da5213","input":"### *Self-Assessment: Weighted Mean User-Based Filter*","pos":76,"type":"cell"}
{"cell_type":"markdown","id":"e87d7b","input":"## Model Based Approaches","pos":36,"type":"cell"}
{"cell_type":"markdown","id":"ea5822","input":"Note that our mean function requires the ratings_matrix argument. Here's where that \\*args parameter comes in. We can pass r_matrix to our score function and it gets passed along to our cf_user_mean model.","pos":22,"type":"cell"}
{"cell_type":"markdown","id":"fa23b3","input":"If we use or score function to get the predicted rating for the entire matrix, we can get the RMSE.","pos":26,"type":"cell"}
{"cell_type":"markdown","id":"fdc12a","input":"The variables y_train and y_test won't actually be used in our code. They're just used as a way to stratify the data. Typically you'd see y as the variable you're trying to predict. That's not how we're doing it here, since our X_train and X_test data are actually dataframes that contain both what we're using to make predictions (user_id and movie_id combination) and what we're predicting (rating). (It's a bit weird. We know.)","pos":9,"type":"cell"}
{"cell_type":"markdown","id":"fdea6e","input":"Using the grid search, we were able to get a better RMSE. Extracting the best parameters shows us that the best combination of parameters was a max k of 3 and a min k of three. By default, KNNBasic uses a min k of 1, so our grid search found that always including 3 neighbors gave a better result than sometimes including fewer than 3 neighbors.\n\n\nIf we wanted to use this model for predictions, we'd want to retrain the model, using the best parameters, on all of our available data. We can do that by setting the data.raw_ratings back to the complete raw ratings, setting up a full trainset, instantiating our model with the best params, and fitting it on the full trainset.","pos":52,"type":"cell"}
{"id":0,"time":1625075925381,"type":"user"}
{"last_load":1625075926399,"type":"file"}