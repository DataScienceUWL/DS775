{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# EXECUTE FIRST\n",
    "\n",
    "# computational imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from sklearn.feature_extraction import text\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet as wn\n",
    "import string\n",
    "\n",
    "# plotting imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# display imports\n",
    "from IPython.display import display, IFrame\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<p><font size=18>Week 13: Recommender Systems 1</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Recommender Systems are a complicated topic. This week's lesson and next week's lesson are intended to just give you a small taste of what recommender systems can do. To truly implement a recommender system, you'd want to dive much deeper. It's a topic worthy of more study. Recommender systems are all around you. Let's explore one recommender system you're probably familiar with, Netflix. \n",
    "\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/L0qVVRJoCf0\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "\n",
    "This week we'll look at two types of recommenders: Knowledge-Based Recommenders and Content-Based Recommenders. These systems are both used to address the \"cold-start\" problem. The cold-start problem is the idea that when you first launch a recommender site, you probably don't have data about what a user likes or how your items have been rated by other users. This lack of data makes it difficult to return really good recommendations, as most of the best recommenders rely on having ratings data. It also makes it impossible to quantify the quality of your recommendations. If you were building recommenders like this in real life, you might want to do some user-testing to validate if your recommender system is making good suggestions. Here, we'll just rely on our own best judgement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Knowledge-Based Recommender\n",
    "\n",
    "The knowledge-based recommender is just a simple ranked chart of data built using some input from the user. Banik describes it as a recommender that:\n",
    "1. Gets user input on their preferences.\n",
    "2. Extracts all the records that match the conditions set by the user.\n",
    "3. Calculates a score to sort the records.\n",
    "4. Returns the sorted results.\n",
    "\n",
    "\n",
    "## The Data\n",
    "Like most data science projects, preparing/wrangling/cleaning your data for your recommender system can be the hardest part. In the lessons and homework, we'll be providing you data that has already been simplified to some extent, so we can focus more on the concepts of recommender systems. For our knowledge-based recommender, we're going to be using the movies_metadata_clean.csv file in the data directory of the lesson. If you'd like to see how we got to this file from the movies_metadata.csv file used in the book, please review the \"DataCleaning\" notebook in the extras directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the dataframe is (5000, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>overview</th>\n",
       "      <th>revenue</th>\n",
       "      <th>runtime</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>862</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>30000000.0</td>\n",
       "      <td>['Animation', 'Comedy', 'Family']</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>373554033</td>\n",
       "      <td>81.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>5415</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8844</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>65000000.0</td>\n",
       "      <td>['Adventure', 'Fantasy', 'Family']</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>262797249</td>\n",
       "      <td>104.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2413</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15602</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['Romance', 'Comedy']</td>\n",
       "      <td>A family wedding reignites the ancient feud be...</td>\n",
       "      <td>0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>92</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31357</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>16000000.0</td>\n",
       "      <td>['Comedy', 'Drama', 'Romance']</td>\n",
       "      <td>Cheated on, mistreated and stepped on, the wom...</td>\n",
       "      <td>81452156</td>\n",
       "      <td>127.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>34</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11862</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['Comedy']</td>\n",
       "      <td>Just when George Banks has recovered from his ...</td>\n",
       "      <td>76578911</td>\n",
       "      <td>106.0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>173</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                        title      budget  \\\n",
       "0    862                    Toy Story  30000000.0   \n",
       "1   8844                      Jumanji  65000000.0   \n",
       "2  15602             Grumpier Old Men         0.0   \n",
       "3  31357            Waiting to Exhale  16000000.0   \n",
       "4  11862  Father of the Bride Part II         0.0   \n",
       "\n",
       "                               genres  \\\n",
       "0   ['Animation', 'Comedy', 'Family']   \n",
       "1  ['Adventure', 'Fantasy', 'Family']   \n",
       "2               ['Romance', 'Comedy']   \n",
       "3      ['Comedy', 'Drama', 'Romance']   \n",
       "4                          ['Comedy']   \n",
       "\n",
       "                                            overview    revenue  runtime  \\\n",
       "0  Led by Woody, Andy's toys live happily in his ...  373554033     81.0   \n",
       "1  When siblings Judy and Peter discover an encha...  262797249    104.0   \n",
       "2  A family wedding reignites the ancient feud be...          0    101.0   \n",
       "3  Cheated on, mistreated and stepped on, the wom...   81452156    127.0   \n",
       "4  Just when George Banks has recovered from his ...   76578911    106.0   \n",
       "\n",
       "   vote_average  vote_count  year  \n",
       "0           7.7        5415  1995  \n",
       "1           6.9        2413  1995  \n",
       "2           6.5          92  1995  \n",
       "3           6.1          34  1995  \n",
       "4           5.7         173  1995  "
      ]
     },
     "execution_count": 2,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in the file\n",
    "df = pd.read_csv('data/movies_metadata_clean.csv')\n",
    "print(f'The shape of the dataframe is {df.shape}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can see that genres looks like a list. To get Python to treat it like a list, we'll need to apply literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Convince Python that this column should be treated like a list, not a string.\n",
    "df['genres'] = df['genres'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Avoiding \"Explode\"\n",
    "Banik would have you \"explode\" the genres. He does this to make it easier to filter on the genres column, but it has an unfortunate side effect of making duplicate rows per movie. It's easy to avoid that by using a different method of filtering. Let's see how that works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 5 movies are:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>overview</th>\n",
       "      <th>revenue</th>\n",
       "      <th>runtime</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>862</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>30000000.0</td>\n",
       "      <td>[Animation, Comedy, Family]</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>373554033</td>\n",
       "      <td>81.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>5415</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8844</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>65000000.0</td>\n",
       "      <td>[Adventure, Fantasy, Family]</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>262797249</td>\n",
       "      <td>104.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2413</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15602</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Romance, Comedy]</td>\n",
       "      <td>A family wedding reignites the ancient feud be...</td>\n",
       "      <td>0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>92</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31357</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>16000000.0</td>\n",
       "      <td>[Comedy, Drama, Romance]</td>\n",
       "      <td>Cheated on, mistreated and stepped on, the wom...</td>\n",
       "      <td>81452156</td>\n",
       "      <td>127.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>34</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11862</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>Just when George Banks has recovered from his ...</td>\n",
       "      <td>76578911</td>\n",
       "      <td>106.0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>173</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                        title      budget  \\\n",
       "0    862                    Toy Story  30000000.0   \n",
       "1   8844                      Jumanji  65000000.0   \n",
       "2  15602             Grumpier Old Men         0.0   \n",
       "3  31357            Waiting to Exhale  16000000.0   \n",
       "4  11862  Father of the Bride Part II         0.0   \n",
       "\n",
       "                         genres  \\\n",
       "0   [Animation, Comedy, Family]   \n",
       "1  [Adventure, Fantasy, Family]   \n",
       "2             [Romance, Comedy]   \n",
       "3      [Comedy, Drama, Romance]   \n",
       "4                      [Comedy]   \n",
       "\n",
       "                                            overview    revenue  runtime  \\\n",
       "0  Led by Woody, Andy's toys live happily in his ...  373554033     81.0   \n",
       "1  When siblings Judy and Peter discover an encha...  262797249    104.0   \n",
       "2  A family wedding reignites the ancient feud be...          0    101.0   \n",
       "3  Cheated on, mistreated and stepped on, the wom...   81452156    127.0   \n",
       "4  Just when George Banks has recovered from his ...   76578911    106.0   \n",
       "\n",
       "   vote_average  vote_count  year  \n",
       "0           7.7        5415  1995  \n",
       "1           6.9        2413  1995  \n",
       "2           6.5          92  1995  \n",
       "3           6.1          34  1995  \n",
       "4           5.7         173  1995  "
      ]
     },
     "execution_count": 4,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Family filter values \n",
      " 0     True\n",
      "1     True\n",
      "2    False\n",
      "3    False\n",
      "4    False\n",
      "Name: genres, dtype: bool\n",
      "Drama filter values \n",
      "0    False\n",
      "1    False\n",
      "2    False\n",
      "3     True\n",
      "4    False\n",
      "Name: genres, dtype: bool\n",
      "These movies have drama OR family.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>overview</th>\n",
       "      <th>revenue</th>\n",
       "      <th>runtime</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>862</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>30000000.0</td>\n",
       "      <td>[Animation, Comedy, Family]</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>373554033</td>\n",
       "      <td>81.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>5415</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8844</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>65000000.0</td>\n",
       "      <td>[Adventure, Fantasy, Family]</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>262797249</td>\n",
       "      <td>104.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2413</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31357</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>16000000.0</td>\n",
       "      <td>[Comedy, Drama, Romance]</td>\n",
       "      <td>Cheated on, mistreated and stepped on, the wom...</td>\n",
       "      <td>81452156</td>\n",
       "      <td>127.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>34</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id              title      budget                        genres  \\\n",
       "0    862          Toy Story  30000000.0   [Animation, Comedy, Family]   \n",
       "1   8844            Jumanji  65000000.0  [Adventure, Fantasy, Family]   \n",
       "3  31357  Waiting to Exhale  16000000.0      [Comedy, Drama, Romance]   \n",
       "\n",
       "                                            overview    revenue  runtime  \\\n",
       "0  Led by Woody, Andy's toys live happily in his ...  373554033     81.0   \n",
       "1  When siblings Judy and Peter discover an encha...  262797249    104.0   \n",
       "3  Cheated on, mistreated and stepped on, the wom...   81452156    127.0   \n",
       "\n",
       "   vote_average  vote_count  year  \n",
       "0           7.7        5415  1995  \n",
       "1           6.9        2413  1995  \n",
       "3           6.1          34  1995  "
      ]
     },
     "execution_count": 4,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These movies have (Romance and Comedy) OR more than 5000 votes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>overview</th>\n",
       "      <th>revenue</th>\n",
       "      <th>runtime</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>862</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>30000000.0</td>\n",
       "      <td>[Animation, Comedy, Family]</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>373554033</td>\n",
       "      <td>81.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>5415</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15602</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Romance, Comedy]</td>\n",
       "      <td>A family wedding reignites the ancient feud be...</td>\n",
       "      <td>0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>92</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31357</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>16000000.0</td>\n",
       "      <td>[Comedy, Drama, Romance]</td>\n",
       "      <td>Cheated on, mistreated and stepped on, the wom...</td>\n",
       "      <td>81452156</td>\n",
       "      <td>127.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>34</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id              title      budget                       genres  \\\n",
       "0    862          Toy Story  30000000.0  [Animation, Comedy, Family]   \n",
       "2  15602   Grumpier Old Men         0.0            [Romance, Comedy]   \n",
       "3  31357  Waiting to Exhale  16000000.0     [Comedy, Drama, Romance]   \n",
       "\n",
       "                                            overview    revenue  runtime  \\\n",
       "0  Led by Woody, Andy's toys live happily in his ...  373554033     81.0   \n",
       "2  A family wedding reignites the ancient feud be...          0    101.0   \n",
       "3  Cheated on, mistreated and stepped on, the wom...   81452156    127.0   \n",
       "\n",
       "   vote_average  vote_count  year  \n",
       "0           7.7        5415  1995  \n",
       "2           6.5          92  1995  \n",
       "3           6.1          34  1995  "
      ]
     },
     "execution_count": 4,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's fetch just the first 5 rows of our dataframe\n",
    "snip = df[:5]\n",
    "print('The first 5 movies are:')\n",
    "display(snip)\n",
    "\n",
    "#let's create a filter that will be True if \"Family\" is in the list of genres for each movie\n",
    "hasFamilyFilter = snip['genres'].apply(lambda x: \"Family\" in x)\n",
    "print(f'Family filter values \\n {hasFamilyFilter}')\n",
    "\n",
    "#let's create a filter that will be True if \"Drama\" is in the list of genres of each movie\n",
    "hasDramaFilter = snip['genres'].apply(lambda x: \"Drama\" in x)\n",
    "print(f'Drama filter values \\n{hasDramaFilter}')\n",
    "\n",
    "#let's filter our dataset to just those movies that have Family OR Drama. Note the placement of the parenthesis\n",
    "print('These movies have drama OR family.')\n",
    "display(snip[(hasFamilyFilter) | (hasDramaFilter)])\n",
    "\n",
    "#let's filter our dataset to just those movies that have Comedy AND Romance OR have a vote_count > 5000.\n",
    "#let's use variables for our two genres\n",
    "selected1 = 'Romance'\n",
    "selected2 = 'Comedy'\n",
    "\n",
    "#instead of creating stand-alone filters, we'll filter \"on the fly\" using the apply right in the filter\n",
    "#again, pay attention to where the parentheses go\n",
    "print('These movies have (Romance and Comedy) OR more than 5000 votes.')\n",
    "snip[(snip['vote_count'] > 5000) | \n",
    "     ((snip['genres'].apply(lambda x: selected1 in x)) & \n",
    "      (snip['genres'].apply(lambda x: selected2 in x)))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Fetching unique values from a column of lists\n",
    "The one other thing we need to be able to do where the exploding helps is to get the list of unique genres. Let's look at how we could do that. We'll do it slightly differently than Banik. And, we'll first break it down into steps so you can see what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\n",
      "           0        1        2\n",
      "0  Animation   Comedy   Family\n",
      "1  Adventure  Fantasy   Family\n",
      "2    Romance   Comedy      NaN\n",
      "3     Comedy    Drama  Romance\n",
      "4     Comedy      NaN      NaN\n",
      "Step 2\n",
      "0  0    Animation\n",
      "   1       Comedy\n",
      "   2       Family\n",
      "1  0    Adventure\n",
      "   1      Fantasy\n",
      "   2       Family\n",
      "2  0      Romance\n",
      "   1       Comedy\n",
      "3  0       Comedy\n",
      "   1        Drama\n",
      "   2      Romance\n",
      "4  0       Comedy\n",
      "dtype: object\n",
      "Step 3\n",
      "['Animation' 'Comedy' 'Family' 'Adventure' 'Fantasy' 'Romance' 'Drama']\n",
      "Step 3 is a \n",
      "<class 'numpy.ndarray'>\n",
      "Step 4\n",
      "Animation, Comedy, Family, Adventure, Fantasy, Romance, Drama\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Animation, Comedy, Family, Adventure, Fantasy, Romance, Drama'"
      ]
     },
     "execution_count": 5,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#in steps\n",
    "#convert the genres list in a series of columns\n",
    "step1 = snip.apply(lambda x:pd.Series(x['genres']),axis=1)\n",
    "print(f\"Step 1\\n{step1}\")\n",
    "\n",
    "#this step converts the rows into columns and \"stacks\" them all together\n",
    "step2 = step1.stack()\n",
    "print(f\"Step 2\\n{step2}\")\n",
    "\n",
    "#let's get just the unique values from this series\n",
    "step3 = step2.unique()\n",
    "print(f\"Step 3\\n{step3}\")\n",
    "print(f\"Step 3 is a \\n{type(step3)}\")\n",
    "\n",
    "#numpy arrays can be joined just like lists, so let's join it to create a comma-delimited string\n",
    "step4 = ', '.join(step3)\n",
    "print(f\"Step 4\\n{step4}\")\n",
    "\n",
    "\n",
    "#let's do it all in one step\n",
    "allGenres = ', '.join(snip.apply(lambda x:pd.Series(x['genres']),axis=1).stack().unique())\n",
    "allGenres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### *Self-Assessment: Modularize Fetching Unique Items*\n",
    "\n",
    "Getting a unique list of items from a Pandas column that contains lists is exactly the kind of thing you might have to do fairly often. Even though Pandas makes this fairly easy, let's modularize the code. Write a function that takes in a dataframe, the name of the column from which to pull unique lists, a type parameter to determine if you should return a numpy array or a string, and a True/False sort parameter to determine if the <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.sort.html\">returned list should be sorted</a>. Use the provided code below to get you started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Food</th>\n",
       "      <th>Flavors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cake</td>\n",
       "      <td>[Chocolate, Vanilla, Marble]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pie</td>\n",
       "      <td>[Apple, Chocolate, Cherry]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ice Cream</td>\n",
       "      <td>[Vanilla, Cherry, Mint]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Food                       Flavors\n",
       "0       Cake  [Chocolate, Vanilla, Marble]\n",
       "1        Pie    [Apple, Chocolate, Cherry]\n",
       "2  Ice Cream       [Vanilla, Cherry, Mint]"
      ]
     },
     "execution_count": 6,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each flavor cell is already of type <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "#this is a test dataframe to use\n",
    "sa1_df = pd.DataFrame({\n",
    "        'Food': ['Cake', 'Pie', 'Ice Cream'],\n",
    "        'Flavors': [['Chocolate','Vanilla', 'Marble'], ['Apple', 'Chocolate', 'Cherry'], ['Vanilla', 'Cherry', 'Mint']]\n",
    "    })\n",
    "display(sa1_df)\n",
    "\n",
    "#note that Python already thinks we have lists in the flavor column, so we don't need literal_eval\n",
    "print(f\"Each flavor cell is already of type {type(sa1_df['Flavors'][0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (1715688343.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_799/1715688343.py\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    getUniqueListFromColumn(sa1_df, 'Flavors', 'string', True)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def getUniqueListFromColumn(df, col, returntype = 'string', sort=True):\n",
    "    #enter your code here\n",
    "    \n",
    "\n",
    "#This line calls your code. Try it with array instead of string, or not sorted, too\n",
    "getUniqueListFromColumn(sa1_df, 'Flavors', 'string', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Creating a metric\n",
    "Knowledge-based recommenders rely on some sort of score for returning their results. There's no right answer for what the score is or how it should be calculated. You'll need to consider the data that you're working with and decide how to calculate a meaningful score. If you are running an e-commerce site, you might use products with the highest percent off sale. If you're recommending library books, you might use a score built off of the number of times the book has been checked out, weighted by the number of weeks since it was first in circulation.\n",
    "\n",
    "For this example, we're interested in highly-rated movies. But, it makes sense to consider both the average rating for a movie and the number of people that rated the movie. Say you have a rating scale from 1 to 5 stars. Your highest average rating would be 5 - a perfect score. But does a rating of 5 by one user mean the same thing as a rating of 5 by 100 users? Probably not.\n",
    "\n",
    "Banik solves this problem by using the IMDB weighted rating. \n",
    "\n",
    "$$Weighted Rating (WR) = \\left(\\frac{v}{v+m} * R\\right) + \\left(\\frac{m}{v+m}*C\\right)$$\n",
    " \n",
    "Where:\n",
    "* v is the number of votes garnered by the movie\n",
    "* m is the minimum number of votes required for the movie to be in the chart (the prerequisite)\n",
    "* R is the mean rating of the movie\n",
    "* C is the mean rating of all the movies in the dataset\n",
    "\n",
    "Banik chose the 80th percentile for the minimum number of votes to be included in the recommender. \n",
    "\n",
    "Note that Banik chooses m (our minimum number of votes) based on the whole dataset, because IMDB sets this as the minimum threshold for being included in the ratings. So m is both a part of the metric AND a filter.\n",
    "\n",
    "Let's also base C on the whole dataset to start.\n",
    "\n",
    "Let's fetch C and m and filter to movies that have vote_counts greater than or equal to the 80th quantile. (This is equivalent to getting the top 20% of votes.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C is 6.069160000000003\n",
      "m is 255.20000000000027\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fetch C from the whole dataset\n",
    "C = df['vote_average'].mean()\n",
    "print(f\"C is {C}\")\n",
    "\n",
    "#fetch m from the whole dataset\n",
    "m = df['vote_count'].quantile(.8)\n",
    "print(f\"m is {m}\")\n",
    "\n",
    "#filter to movies that have greater than or equal to 80% of the votes\n",
    "df = df[df['vote_count'] >= m]\n",
    "\n",
    "#see how many movies are left.\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's write our function to do scoring. Note that unlike Banik's metric, our version takes in x (the row of data) and m & C. Passing all the variables you need into the the function is a best practice that Banik does not follow, but we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def weighted_rating(x, m, C):\n",
    "    v = x['vote_count']\n",
    "    R = x['vote_average']\n",
    "    # Compute the weighted score\n",
    "    return (v/(v+m) * R) + (m/(m+v) * C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's apply the score to the unfiltered dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>overview</th>\n",
       "      <th>revenue</th>\n",
       "      <th>runtime</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "      <th>year</th>\n",
       "      <th>score1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>862</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>30000000.0</td>\n",
       "      <td>[Animation, Comedy, Family]</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>373554033</td>\n",
       "      <td>81.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>5415</td>\n",
       "      <td>1995</td>\n",
       "      <td>7.626600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8844</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>65000000.0</td>\n",
       "      <td>[Adventure, Fantasy, Family]</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>262797249</td>\n",
       "      <td>104.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2413</td>\n",
       "      <td>1995</td>\n",
       "      <td>6.820534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>949</td>\n",
       "      <td>Heat</td>\n",
       "      <td>60000000.0</td>\n",
       "      <td>[Action, Crime, Drama, Thriller]</td>\n",
       "      <td>Obsessive master thief, Neil McCauley leads a ...</td>\n",
       "      <td>187436818</td>\n",
       "      <td>170.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>1886</td>\n",
       "      <td>1995</td>\n",
       "      <td>7.505628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>710</td>\n",
       "      <td>GoldenEye</td>\n",
       "      <td>58000000.0</td>\n",
       "      <td>[Adventure, Action, Thriller]</td>\n",
       "      <td>James Bond must unmask the mysterious head of ...</td>\n",
       "      <td>352194034</td>\n",
       "      <td>130.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>1194</td>\n",
       "      <td>1995</td>\n",
       "      <td>6.506521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>21032</td>\n",
       "      <td>Balto</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Family, Animation, Adventure]</td>\n",
       "      <td>An outcast half-wolf risks his life to prevent...</td>\n",
       "      <td>11348324</td>\n",
       "      <td>78.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>423</td>\n",
       "      <td>1995</td>\n",
       "      <td>6.712105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id      title      budget                            genres  \\\n",
       "0     862  Toy Story  30000000.0       [Animation, Comedy, Family]   \n",
       "1    8844    Jumanji  65000000.0      [Adventure, Fantasy, Family]   \n",
       "5     949       Heat  60000000.0  [Action, Crime, Drama, Thriller]   \n",
       "9     710  GoldenEye  58000000.0     [Adventure, Action, Thriller]   \n",
       "12  21032      Balto         0.0    [Family, Animation, Adventure]   \n",
       "\n",
       "                                             overview    revenue  runtime  \\\n",
       "0   Led by Woody, Andy's toys live happily in his ...  373554033     81.0   \n",
       "1   When siblings Judy and Peter discover an encha...  262797249    104.0   \n",
       "5   Obsessive master thief, Neil McCauley leads a ...  187436818    170.0   \n",
       "9   James Bond must unmask the mysterious head of ...  352194034    130.0   \n",
       "12  An outcast half-wolf risks his life to prevent...   11348324     78.0   \n",
       "\n",
       "    vote_average  vote_count  year    score1  \n",
       "0            7.7        5415  1995  7.626600  \n",
       "1            6.9        2413  1995  6.820534  \n",
       "5            7.7        1886  1995  7.505628  \n",
       "9            6.6        1194  1995  6.506521  \n",
       "12           7.1         423  1995  6.712105  "
      ]
     },
     "execution_count": 10,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['score1'] = df.apply(weighted_rating, args=(m,C), axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "What happens if use C from just the filtered dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C is 6.805500000000003\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>overview</th>\n",
       "      <th>revenue</th>\n",
       "      <th>runtime</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "      <th>year</th>\n",
       "      <th>score1</th>\n",
       "      <th>score2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>862</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>30000000.0</td>\n",
       "      <td>[Animation, Comedy, Family]</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>373554033</td>\n",
       "      <td>81.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>5415</td>\n",
       "      <td>1995</td>\n",
       "      <td>7.626600</td>\n",
       "      <td>7.659741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8844</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>65000000.0</td>\n",
       "      <td>[Adventure, Fantasy, Family]</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>262797249</td>\n",
       "      <td>104.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2413</td>\n",
       "      <td>1995</td>\n",
       "      <td>6.820534</td>\n",
       "      <td>6.890962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>949</td>\n",
       "      <td>Heat</td>\n",
       "      <td>60000000.0</td>\n",
       "      <td>[Action, Crime, Drama, Thriller]</td>\n",
       "      <td>Obsessive master thief, Neil McCauley leads a ...</td>\n",
       "      <td>187436818</td>\n",
       "      <td>170.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>1886</td>\n",
       "      <td>1995</td>\n",
       "      <td>7.505628</td>\n",
       "      <td>7.593389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>710</td>\n",
       "      <td>GoldenEye</td>\n",
       "      <td>58000000.0</td>\n",
       "      <td>[Adventure, Action, Thriller]</td>\n",
       "      <td>James Bond must unmask the mysterious head of ...</td>\n",
       "      <td>352194034</td>\n",
       "      <td>130.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>1194</td>\n",
       "      <td>1995</td>\n",
       "      <td>6.506521</td>\n",
       "      <td>6.636188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>21032</td>\n",
       "      <td>Balto</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Family, Animation, Adventure]</td>\n",
       "      <td>An outcast half-wolf risks his life to prevent...</td>\n",
       "      <td>11348324</td>\n",
       "      <td>78.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>423</td>\n",
       "      <td>1995</td>\n",
       "      <td>6.712105</td>\n",
       "      <td>6.989183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id      title      budget                            genres  \\\n",
       "0     862  Toy Story  30000000.0       [Animation, Comedy, Family]   \n",
       "1    8844    Jumanji  65000000.0      [Adventure, Fantasy, Family]   \n",
       "5     949       Heat  60000000.0  [Action, Crime, Drama, Thriller]   \n",
       "9     710  GoldenEye  58000000.0     [Adventure, Action, Thriller]   \n",
       "12  21032      Balto         0.0    [Family, Animation, Adventure]   \n",
       "\n",
       "                                             overview    revenue  runtime  \\\n",
       "0   Led by Woody, Andy's toys live happily in his ...  373554033     81.0   \n",
       "1   When siblings Judy and Peter discover an encha...  262797249    104.0   \n",
       "5   Obsessive master thief, Neil McCauley leads a ...  187436818    170.0   \n",
       "9   James Bond must unmask the mysterious head of ...  352194034    130.0   \n",
       "12  An outcast half-wolf risks his life to prevent...   11348324     78.0   \n",
       "\n",
       "    vote_average  vote_count  year    score1    score2  \n",
       "0            7.7        5415  1995  7.626600  7.659741  \n",
       "1            6.9        2413  1995  6.820534  6.890962  \n",
       "5            7.7        1886  1995  7.505628  7.593389  \n",
       "9            6.6        1194  1995  6.506521  6.636188  \n",
       "12           7.1         423  1995  6.712105  6.989183  "
      ]
     },
     "execution_count": 11,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fetch c from the already filtered data\n",
    "C2 = df['vote_average'].mean()\n",
    "print(f\"C is {C2}\")\n",
    "\n",
    "df['score2'] = df.apply(weighted_rating, args=(m,C2), axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "You can see that it does indeed make a difference in the score. But would it make a difference in our recommendations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### *Self-Assessment: Load and Display*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Load the data set **ted_clean.csv** and display the first 5 rows. This data set can be found in the data folder for this lesson.  More information about this data set <a href = https://www.kaggle.com/rounakbanik/ted-talks> here </a>.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### *Self-Assessment: Pandas*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "How many talks are in the TED Talks data frame?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### *Self-Assessment: Prerequisites*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Select TED talks with these prerequisites:\n",
    "\n",
    "1. talks with duration of at least 5 minutes (i.e. 300 seconds)\n",
    "2. talks with only 1 speaker\n",
    "3. talks in the top 90\\% of views (exclude the bottom 10\\%)\n",
    "\n",
    "Also inspect the number of talks that made the cut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### *Self-Assessment: Compute a Metric, Sort and Print*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the absence of numerical ratings here, use the ratio of the number of comments per 1000 views as a metric to sort the TED talks and print the 10 with the highest ratios.  \n",
    "\n",
    "Display only the description, the main speaker, and the number of views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Getting User Input\n",
    "Now that we've seen the pieces of the knowledge-based recommender, all we need to do is wrap the pieces in a function that accepts user input.\n",
    "\n",
    "Our function will take in a cleaned dataframe and a percentile to use for m. By default, the percentile will be .8. \n",
    "\n",
    "Note that the only changes we're making here from Banik's metric is to adjust how we do the genres filter. We'll follow Banik's lead by calculated m after we filter the data to the user's selections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def build_chart(gen_df, percentile=0.8):\n",
    "    \n",
    "    #Ask for preferred genres\n",
    "    print(\"Input preferred genre\")\n",
    "    genre = input()\n",
    "    \n",
    "    #Ask for lower limit of duration\n",
    "    print(\"Input shortest duration\")\n",
    "    low_time = int(input())\n",
    "    \n",
    "    #Ask for upper limit of duration\n",
    "    print(\"Input longest duration\")\n",
    "    high_time = int(input())\n",
    "    \n",
    "    #Ask for lower limit of timeline\n",
    "    print(\"Input earliest year\")\n",
    "    low_year = int(input())\n",
    "    \n",
    "    #Ask for upper limit of timeline\n",
    "    print(\"Input latest year\")\n",
    "    high_year = int(input())\n",
    "    \n",
    "    #Define a new movies variable to store the preferred movies. Copy the contents of gen_df to movies\n",
    "    movies = gen_df.copy()\n",
    "    \n",
    "    #Filter based on the condition\n",
    "    movies = movies[(movies['genres'].apply(lambda x: genre in x)) & #updated filtering based on a list.\n",
    "                    (movies['runtime'] >= low_time) & \n",
    "                    (movies['runtime'] <= high_time) & \n",
    "                    (movies['year'] >= low_year) & \n",
    "                    (movies['year'] <= high_year)]\n",
    "    \n",
    "    #Compute the values of C and m for the filtered movies\n",
    "    C = movies['vote_average'].mean()\n",
    "    m = movies['vote_count'].quantile(percentile)\n",
    "    \n",
    "    #Only consider movies that have higher than m votes. Save this in a new dataframe q_movies\n",
    "    q_movies = movies.copy().loc[movies['vote_count'] >= m]\n",
    "    \n",
    "    #Calculate score using the IMDB formula\n",
    "    q_movies['score'] = q_movies.apply(lambda x: (x['vote_count']/(x['vote_count']+m) * x['vote_average']) \n",
    "                                       + (m/(m+x['vote_count']) * C)\n",
    "                                       ,axis=1)\n",
    "\n",
    "    #Sort movies in descending order of their scores\n",
    "    q_movies = q_movies.sort_values('score', ascending=False)\n",
    "    \n",
    "    return q_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We'll test our code by requesting movies that have the genres 'Family' and a runtime between 80 and 120 minutes and a year between 1980 and 2000. We'll return the output to a variable so we can review it in different ways. (Don't worry if your saved code has extra boxes or looks out of order. That's just a bug in CoCalc's output saving.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cocalc": {
     "outputs": {
      "1": {
       "name": "input",
       "opts": {
        "password": false,
        "prompt": ""
       },
       "output_type": "stream"
      },
      "2": {
       "name": "input",
       "opts": {
        "password": false,
        "prompt": ""
       },
       "output_type": "stream"
      },
      "3": {
       "name": "input",
       "opts": {
        "password": false,
        "prompt": ""
       },
       "output_type": "stream",
       "value": "80"
      },
      "4": {
       "name": "input",
       "opts": {
        "password": false,
        "prompt": ""
       },
       "output_type": "stream"
      },
      "5": {
       "name": "input",
       "opts": {
        "password": false,
        "prompt": ""
       },
       "output_type": "stream",
       "value": "120"
      },
      "6": {
       "name": "input",
       "opts": {
        "password": false,
        "prompt": ""
       },
       "output_type": "stream"
      },
      "7": {
       "name": "input",
       "opts": {
        "password": false,
        "prompt": ""
       },
       "output_type": "stream",
       "value": "1980"
      },
      "8": {
       "name": "input",
       "opts": {
        "password": false,
        "prompt": ""
       },
       "output_type": "stream"
      },
      "9": {
       "name": "input",
       "opts": {
        "password": false,
        "prompt": ""
       },
       "output_type": "stream",
       "value": ""
      }
     }
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input preferred genre\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": " "
    }
   ],
   "source": [
    "out_movies = build_chart(df, .8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We have two other versions of scores in the dataset. Let's compare the rankings if we sort in different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#set the score rank column\n",
    "out_movies['scoreRank'] = np.arange(len(out_movies))\n",
    "#sort by score1 and set the score1rank column\n",
    "out_movies = out_movies.sort_values('score1', ascending=False)\n",
    "out_movies['score1Rank'] = np.arange(len(out_movies))\n",
    "#sort by score2 and set the score2rank column\n",
    "out_movies = out_movies.sort_values('score2', ascending=False)\n",
    "out_movies['score2Rank'] = np.arange(len(out_movies))\n",
    "#resort by score\n",
    "out_movies = out_movies.sort_values('score', ascending=False)\n",
    "\n",
    "#display the final result with just name and scores\n",
    "out_movies[['title','score1Rank', 'score2Rank', 'scoreRank' ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Did the different methods of computing the score impact the recommendation results in a significant way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### *Self-Assessment: Create the Knowledge-Based Recommender*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For this example we will use the TED Talks data set that you have already loaded to build a knowledge-based recommender by soliciting the desired publication year and word rating from the user. We've already extracted the year from the publish date and set up the ratings column as a list of words. (You'll still need to apply literal_eval to it, if you didn't already. Do that outside of your function in a separate cell. If you apply literal_eval twice, it will cause an error.) \n",
    "\n",
    "1. Print a list of the descriptive word ratings for the user to choose from. (*Hint: follow the directions in this lesson.*)\n",
    "\n",
    "2. Ask the user to enter answers to the following questions:\n",
    "\n",
    "    - Enter a descriptive word for rating.\n",
    "    - Enter the earliest year published for the talk (between 2006 and 2017).\n",
    "    - Enter the latest year published for the talk (between 2006 and 2017).\n",
    "\n",
    "3. Consider only talks with the top 90% of views (after filtering based on user preferences).\n",
    "\n",
    "4. Display the top 5 recommended talks according to the \"comments per 1000 views\" ratio (calculated AFTER doing steps 2 & 3).\n",
    "\n",
    "5. Display only the main speaker, the name of the talk, the year published, and the comments per thousand views ratio.\n",
    "\n",
    "6.  Show the results for the word rating \"obnoxious\" and published years between 2009 and 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Content-Based Recommender\n",
    "\n",
    "There are a number of different approaches to doing content-based recommenders. What they all share is that they use some method to:\n",
    "* select and clean text\n",
    "* convert text to a numerical representation\n",
    "* use the numerical representation of the text to assign similarity scores between each pair or set of items.\n",
    "\n",
    "Once our pairs of items are scored, given one \"seed\" item, we can then recommend items that are similar to that item.\n",
    "\n",
    "Let's break this down into steps. \n",
    "\n",
    "## Selecting and Cleaning Text\n",
    "Banik talks about either using metadata or descriptions as the text that represents your item. This is a bit of a simplification. Really you could use any text, or any combination of text, that you have available to you. You should put time and effort into deciding what text you'll use. Consider which textual elements best represent the essence of the items you're recommending. Here's where that user testing comes into play. Maybe you write two recommenders - one using metadata and one using descriptions and you get your family and friends to sit down and review the recommendations that they receive using each approach. Is one better than the other? Does one recommend entirely inappropriate things? (We've all heard the stories about recommender systems gone wrong.)\n",
    "\n",
    "Whatever text you use, you most likely want to do some pre-processing of the text first. This step is referred to as \"cleaning\" the text. There are many, many possible steps you can take here. We'll demonstrate just a few of them. Banik doesn't really discuss this bit, but we think it's too important to gloss over. Here's a non-exhaustive list of things to consider doing to your text, once you've decided which bits of text you're going to use.\n",
    "\n",
    "1. Converting it to lowercase.\n",
    "2. Removing punctuation.\n",
    "3. Stemming or Lemmatization.\n",
    "4. Using N-grams or combining words.\n",
    "5. Identifying and using specific parts of speech.\n",
    "\n",
    "Entire courses have been written about natural language processing. We can't get into all the details, so we'll focus on just a few steps that you can take that integrate well with scikit learn's CountVectorizer and TfidfVectorizer - converting to lowercase, removing punctuation, lemmatizing (converting words to their \"root\" word), and n-grams. Remember, a CountVectorizer creates a matrix of all the words available in the text, and *counts* each occurrence of each word. Banik does a nice job explaining this so we won't delve too deeply into it here. We'll loop back to TfidfVectorizer in a bit.\n",
    "\n",
    "### Converting to lowercase, removing stop words, and removing punctuation.\n",
    "These are by far the easiest. The vectorizers automatically remove most punctuation. Converting to lowercase is the default setting for both of the vectorizers. And removing a standard set of stop words can be done just by adding one parameter. We'll add both parameters explicitly just so you can see what happens. First we need some data to play with. Let's set that up. Here we're creating a dataframe with three \"documents\" and setting the index of the documents to the ID column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#create a small dataframe\n",
    "df = pd.DataFrame({'ID': ['Doc 1', 'Doc 2', 'Doc 3'], \n",
    "                   'Abstract': ['She is a queen bee.', 'There is no king.', 'The queen rules her bees.']})\n",
    "\n",
    "#use the document name as the index\n",
    "corpus_index = df['ID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "When we set up the CountVectorizer, we'll tell it that we want it to convert everything to lowercase and we want to remove stop words. Then we'll fit_transform the text using the Count Vectorizer. That's where the magic happens. Calling fit_transform and passing in our data will create the sparse matrix of words and counts. Finally, we'll just convert it to a dense matrix for easy display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#create a Count Vectorizer that explicitly converts to lowercase and removes stopwords.\n",
    "vec1 = CountVectorizer(lowercase=True, stop_words='english')\n",
    "\n",
    "#Construct the required count matrix by applying the fit_transform method on the Abstract\n",
    "count_matrix = vec1.fit_transform(df['Abstract'])\n",
    "\n",
    "#display the matrix as a pandas dataframe\n",
    "pd.DataFrame(count_matrix.todense(), index=corpus_index, columns=vec1.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "You can see that after creating the matrix, we end up with five \"features\" - five unique words that hopefully provide some meaning. All of the prepositions, pronouns and articles were removed by applying the stop words. \n",
    "\n",
    "#### N-Grams\n",
    "What if we were interested in 2-word phrases. \"Queen bee\" has a different meaning than either queen or bee alone. Multi-word phrases are called n-grams, and the \"n\" can be replaced with any number. We could have bigrams (2-word phrases), trigrams (3-word phrases), and so-on. To use n-grams with either of our scikit-learn vectorizers, we add the ngram_range parameter, and we give it a tuple indicating the shortest and longest phrase we're interested in considering. Let's try an n-gram of 1 or 2 word phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#create a Count Vectorizer that explicitly converts to lowercase and removes stopwords and looks at 1 and 2 word n-grams.\n",
    "vec2 = CountVectorizer(lowercase=True, stop_words='english', ngram_range=(1,2))\n",
    "\n",
    "#Construct the required count matrix by applying the fit_transform method on the Abstract\n",
    "count_matrix = vec2.fit_transform(df['Abstract'])\n",
    "\n",
    "#display the matrix as a pandas dataframe\n",
    "pd.DataFrame(count_matrix.todense(), index=corpus_index, columns=vec2.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "By adding in n-grams, we went from 5 features to 8 features. Note that this is a very small corpus of text. If we were analyzing large bodies of text, adding in n-grams can increase your feature space very quickly. Be cautious about using n-grams if you're working with large datasets. But, if you know your text has a lot of meaningful phrases, it might be an approach worth considering, particularly if you're getting poor results from single words.\n",
    "\n",
    "#### Lemmatization\n",
    "Finally, let's look at lemmatization. Lemmatizing a word means reducing it to its root word. For example, the root word for the verb \"is\" is \"be.\" Roots for nouns are generally the singular form of the word. Root forms of verbs tend to be the present-tense form. Adding lemmatization is a bit more complicated. We need to create a custom tokenizer. The tokenizer is what the vectorizers use to chunk up the text into tokens (typically single words). We'll create a tokenizer that converts the words to their lemmas (root words) before the vectorizer creates the matrix. Don't panic. We're giving you the code here. There are two bits - one cell that you'd only need once and shouldn't have to alter at all, and then the code to actually set up the vectorizer, and call the fit_transform, which would only require passing in different data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#################################\n",
    "# This cell does all the set up work - you only need to run this once per notebook\n",
    "#################################\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "#Create a helper function to get part of speech\n",
    "def get_wordnet_pos(word, pretagged = False):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    if pretagged:\n",
    "        tag = word[1].upper() \n",
    "    else:\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    \n",
    "    tag_dict = {\"J\": wn.ADJ,\n",
    "                \"N\": wn.NOUN,\n",
    "                \"V\": wn.VERB,\n",
    "                \"R\": wn.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wn.NOUN)\n",
    "\n",
    "#create a tokenizer that uses lemmatization (word shortening)\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        \n",
    "        #get the sentences\n",
    "        sents = sent_tokenize(articles)\n",
    "        #get the parts of speech for sentence tokens\n",
    "        sent_pos = [nltk.pos_tag(word_tokenize(s)) for s in sents]\n",
    "        #flatten the list\n",
    "        pos = [item for sublist in sent_pos for item in sublist]\n",
    "        #lemmatize based on POS (otherwise, all words are nouns)\n",
    "        lems = [self.wnl.lemmatize(t[0], get_wordnet_pos(t, True)) for t in pos if t[0] not in string.punctuation]\n",
    "        #clean up in-word punctuation\n",
    "        lems_clean = [''.join(c for c in s if c not in string.punctuation) for s in lems]\n",
    "        return lems_clean \n",
    "\n",
    "\n",
    "    \n",
    "#lemmatize the stop words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_stop_words = [lemmatizer.lemmatize(w) for w in text.ENGLISH_STOP_WORDS]\n",
    "#extend the stop words with any other words you want to add, these are bits of contractions\n",
    "lemmatized_stop_words.extend(['ve','nt','ca','wo','ll'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#create the vectorizer using the lemmaTokenizer - note that we also need the lemmatized version of the stop words for this to work\n",
    "vec3 = CountVectorizer(tokenizer=LemmaTokenizer(), lowercase=True, stop_words=lemmatized_stop_words) \n",
    "\n",
    "#Construct the required count matrix by applying the fit_transform method on the Abstract\n",
    "count_matrix3 = vec3.fit_transform(df['Abstract'])\n",
    "\n",
    "pd.DataFrame(count_matrix3.todense(), index=corpus_index, columns=vec3.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now you can see we have just 4 features, with both document 1 and document 3 containing a reference to \"bee.\" Lemmatization might be important if you're including free-form text. You would not need to use lemmatization if you were using standard metadata, such as keywords or genres. When using that kind of standard vocabulary, you should already have a smaller number of features and the words should be consistent across your rows of data.\n",
    "\n",
    "Like many things in Data Science, there's no one \"right\" answer about how much to pre-process your data. Sometimes, the best thing you can do is try out different approaches and see which ones seem to work the best.\n",
    "\n",
    "### Term Frequency-Inverse Document Frequency\n",
    "The Term Frequency Inverse Document Frequency Vectorizer works exactly the same way as the Count Vectorizer. Let's do some data cleaning of the \"overview\" column of our snip data set and create a Tfidf matrix. Since this is free-form text, we'll lemmatize the words, remove stop words and punctuation and convert to lowercase. Let's also use the max_features parameter (available in both Tfidf and Count vectorizers) to limit the features to the top 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#remember what's in snip\n",
    "snip = snip.copy()\n",
    "display(snip)\n",
    "\n",
    "#Define a TF-IDF Vectorizer Object. Use the LemmaTokenizer defined above, convert to lowercase, and remove stopwords.\n",
    "tfidf = TfidfVectorizer(tokenizer=LemmaTokenizer(), lowercase=True, stop_words=lemmatized_stop_words, max_features = 100)\n",
    "\n",
    "#if we had any empty overview fields, we'd need to replace NaN with an empty string. \n",
    "# we don't in our snip dataset, but we'll step through it as a good practice\n",
    "snip['overview'] = snip['overview'].fillna('')\n",
    "\n",
    "#Construct the required TF-IDF matrix by applying the fit_transform method on the overview feature\n",
    "tfidf_matrix = tfidf.fit_transform(snip['overview'])\n",
    "#Output the shape of tfidf_matrix\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Because we have such a small matrix, we can actually print it and take a peek at what's going on in there. In order to print a tfidf matrix, we need to convert it to something that can be displayed.\n",
    "\n",
    "*Note: You won't need to do this bit in your homework or self-assessment, but it's helpful in understanding what we have.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#this extracts all the words (features) in the matrix - we'll use this for our columns\n",
    "feature_names = tfidf.get_feature_names()\n",
    "#this extracts the IDs of the movies - we'll use this for our rows\n",
    "corpus_index = snip['title']\n",
    "#this puts both into a dataframe. \n",
    "#The tfidf_matrix is usually a sparse matrix, meaning not all row/col combinations have a value. Using todense() puts a zero in that row/col slot\n",
    "pd.DataFrame(tfidf_matrix.todense(), index=corpus_index, columns=feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "When we print out this matrix, we can see that \"afraid\" is an important word in the description of Toy Story, but not in any of our other movies. But \"wedding\" is important in both Grumpier Old Men and Father of the Bride.\n",
    "\n",
    "If we create a heatmap of the features, we can visually scan to see which movies are more similar. Similar movies would have similar patterns of colors. Which movies look most similar? Which movies look the most different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "ax = sns.heatmap(pd.DataFrame(tfidf_matrix.todense(), index=snip['title'], columns=feature_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Computing Cosine Similarity\n",
    "Visually looking for similarities doesn't really work well for very many items. Instead, let's determine how similar each movie is to each other movie mathematically. We'll use cosine similarity to do that. Cosine similarity compares the direction of vectors in multi-dimensional space. The less angle between each vector, the higher the cosine-similarity score. What now? Yep, it's a lot to wrap your head around. Just remember that each row of our matrix can be thought of as a vector that can be plotted in multi-dimensional space. It's easier if we think about a 2 dimensional problem. Let's say we had 2 characteristics and 4 movies that we've scored based on those characteristics. If we plot those on a graph and draw a line from the origin (0,0) through their score points, we'd have a vector for each of the characteristics pointing in specific directions. Let's look at an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "V = np.array([[12,1], [4,1], [4,7], [-12,-1]])\n",
    "origin = np.array([[0, 0, 0, 0],[0, 0, 0, 0]]) # origin point\n",
    "\n",
    "plt.quiver(*origin, V[:,0], V[:,1], color=['r','b','g','black'], scale=31)\n",
    "plt.ylabel('Action')\n",
    "plt.xlabel('Comedy')\n",
    "plt.title('Movies rated as Action or Comedy')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "If we imagine here the the X axis represents \"Comedy\" and the Y axis represents \"Action\", our green arrow might represent a movie that is action-packed and a little bit funny. Our red arrow might be something with just a little action and a lot of comedy. Our blue movie is somewhere right in between. Our black arrow is the exact opposite of our red arrow, and probably represents a slow-moving, serious movie. \n",
    "\n",
    "When we quantify this with cosine similarity, we end up with a number for each pair of movies that's somewhere between -1 (total opposites) and 1 (exactly the same). Let's look at the cosine similiarity between some of our paris of movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "print('Remember - the higher the number, the closer the vectors.')\n",
    "print(f'The similarity between the red and blue vectors is {round(1 - distance.cosine(V[0], V[1]), 3)}')\n",
    "print(f'The similarity between the blue and green vectors is {round(1 - distance.cosine(V[1], V[2]), 3)}')\n",
    "print(f'The similarity between the red and green vectors is {round(1 - distance.cosine(V[0], V[2]), 3)}')\n",
    "print(f'The similarity between the red and black vectors is {round(1 - distance.cosine(V[0], V[3]), 3)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Of note here is the the *length* of the vector doesn't matter when we're dealing with cosine similarity - only the angle between the two vectors matters. So, our red and blue movies score nearly a perfect match, even though the red movie is arguably a more comedic movie than the blue movie. That's just how these similarity measures work.\n",
    "\n",
    "### Dot Product (linear_kernel) vs. Cosine Similarity\n",
    "Banik briefly mentions this, but it bears repeating. If you have data that's already normalized so that all the scores share the same magnitude, such as what happens with TfidfVectorizer (where each score is between 0 and 1), you can use the computationally cheaper dot product calculation using linear_kernel(). If you're using the CountVectorizer, you have to use the cosine similarity function - cosine_similarity() -  which will normalize your data before calculating your vector distances.  e\n",
    "\n",
    "If you want to dive into the math of all this, the book \"Practical Recommender Systems\" by Kim Falk does a nice job of explaining and diving much deeper into these concepts.\n",
    "\n",
    "With our snip dataset, we've constructed our vector matrix using Tfidf, so we can use the linear_kernel()/dot product method to get our similiarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Compute the cosine similarity matrix\n",
    "sim_matrix = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "#let's look at what we've got.\n",
    "dpdf = pd.DataFrame(sim_matrix, columns=snip['title'], index=snip['title'])\n",
    "cm = sns.color_palette(\"Blues\", as_cmap=True)\n",
    "dpdf.style.set_caption('Dot Product with most similar movies highlighted.')\\\n",
    "    .background_gradient(cmap=cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Note that what we get is a matrix (which we've converted to a colored dataframe for display) that scores each movie in relation to another movie. A movie scored with itself will always be a one. Note that in our dataset here, Waiting to Exhale and Grumpier Old Men are our least similar two movies. Does that track with what you see in the heatmap above? Does that track with what you know of those two movies?\n",
    "\n",
    "Which two movies are most similar?\n",
    "\n",
    "## Using the Similarity Matrix\n",
    "Now that we have this matrix, we need to be able to use it to identify similar movies. Let's break down some of what Banik does.\n",
    "\n",
    "### Getting Index from Title\n",
    "Banik uses a reverse mapping of indexes and titles to fetch data from the cosine similarity matrix. Let's take a look at what that is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#create the reverse mapping\n",
    "indices = pd.Series(snip.index, index=snip['title']).drop_duplicates()\n",
    "#print it \n",
    "print(f'The index series looks like this: \\n{indices}')\n",
    "\n",
    "#if I wanted to get the index from the title I would do this:\n",
    "print(f'The index for Waiting to Exhale is: {indices[\"Waiting to Exhale\"]}')\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's also break down what's going on with converting our cosine similarity to a list of tuples. \n",
    "\n",
    "The first thing we're doing is getting the row from the matrix that corresponds to the movie we want to review. Let's say we want to review Grumpier Old Men. Let's use the reverse mapping to get the index, and then fetch that row from our matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "idx = indices[\"Grumpier Old Men\"]\n",
    "sim_matrix[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The <a href=\"https://book.pythontips.com/en/latest/enumerate.html\">enumerate function</a> loops over some iterable object and returns a counter and the value for each item in the iterable. We can see that what cosine_sim[2] returns is an array, which is an iterable object. We can't directly print the results from enumerate, so we have to wrap it in a list function.\n",
    "\n",
    "What this results in is a list of tuples that correspond to the column number and the cosine similarity score for each movie that we compared to Grumpier Old Men. Which column number would be Grumpier Old Men compared with itself?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "sim_scores = list(enumerate(sim_matrix[idx]))\n",
    "sim_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can see that the most similar movie to Grumpier Old Men is... Grumpier Old Men. This makes sense - it's the same movie! We don't want that movie in our results, though. Since we know this is a balanced matrix (the indexes are the same for the columns and for the rows), we can just delete the item with our Grumpier Old Men index. Remember, that's 2. Let's see how that works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "del sim_scores[idx]\n",
    "sim_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Great. That got rid of the tuple that corresponded to the column Grumpier Old Men.\n",
    "\n",
    "The next thing we do is to sort this list by the score (the second bit of the tuple). We're using a lambda function to do that. Let's see what we get when we sort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "sim_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can see that column 4 and column 1 of our matrix contain our 2 most similar movies. But, what movies are those? We need to go back to our dataframe to figure that out. Let's extract just the indices for our top 2 movies. Finally, we'll use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html\">iloc</a> to find the corresponding movie titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "top_two = [i[0] for i in sim_scores[0:2]]\n",
    "print(f'The top two indices are: {top_two}')\n",
    "\n",
    "snip.iloc[top_two]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Recommender Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's wrap this up in a function. We're going to do this slightly differently than Banik did. \n",
    "* We'll avoid giving it any variable defaults (which is a good practice unless you're hard-coding the defaults).\n",
    "* We'll pass in the string to identify the seed column and get the indices mapping inside the function. (The seed column is whatever column we're using to identify the item we're interested in using to recommend other items. In this case, it's a movie title. But it could be a place name, or a recipe title, or whatever, depending on our recommender.)\n",
    "* We'll also pass in the number of results to return. This is hard-coded number, so we will set a default for that.\n",
    "* We'll delete the passed-in movie explicitly, instead of assuming it's the first after sorting\n",
    "* We'll return the whole dataframe, not just the titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def content_recommender(df, seed, seedCol, sim_matrix,  topN=2): \n",
    "    #get the indices based off the seedCol\n",
    "    indices = pd.Series(df.index, index=df[seedCol]).drop_duplicates()\n",
    "    \n",
    "    # Obtain the index of the item that matches our seed\n",
    "    idx = indices[seed]\n",
    "    \n",
    "    # Get the pairwsie similarity scores of all items and convert to tuples\n",
    "    sim_scores = list(enumerate(sim_matrix[idx]))\n",
    "    \n",
    "    #delete the item that was passed in\n",
    "    del sim_scores[idx]\n",
    "    \n",
    "    # Sort the items based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get the scores of the top-n most similar items.\n",
    "    sim_scores = sim_scores[:topN]\n",
    "    \n",
    "    # Get the item indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    \n",
    "    # Return the topN most similar items\n",
    "    return df.iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's test our recommender with Grumpier Old Men and our snipped dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "content_recommender(snip, 'Grumpier Old Men', 'title', sim_matrix, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Content-Based Recommender Self Assessment\n",
    "\n",
    "For this example we will use the TED Talks data set that you have already loaded to build a content-based recommender based on the descriptions of the talks.  This will correspond to the **plot description-based recommender**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### *Self-Assessment: TF-IDF Vectors*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "From the original TED Talks data frame that use in this lesson, create the TF-IDF (term frequency - inverse document frequency) matrix from the descriptions of the talks.  The TF-IDF is high where a rare term is present or frequent in a document and TF-IDF is near zero where a term is absent from a document, or abundant across all documents.\n",
    "\n",
    "The feature name in the data frame is **description**.\n",
    "\n",
    "Preprocess the description column by using lower case letters, removing punctuation, removing the default English stop words, and using only bigrams.\n",
    "\n",
    "Output the shape of the TF-IDF matrix you create. The number of rows corresponds to the number of TED talks in the data frame and the number of columns represents the number of unique terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### *Self-Assessment: Create the Content-Based Recommender Based on Dot Product*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Compute the dot product score for all of the TED talks in the data frame. Next build the recommender to request the name of a TED talk in the data frame and provide the top 5 recommended talks based on the similarity of the descriptions with the name of the talk supplied.\n",
    "\n",
    "Show that it works by getting the top 5 recommended talks that are similar to the talk named \"Tyler Cowen: Be suspicious of simple stories\" (from the **name** column of the data frame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Content-Based Recommender using MetaData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We mentioned at the beginning that you could use any text you have available. But so far, we've only demonstrated using free-form text - like a description, synopsis, or overview. You can also use metadata that comes from more defined and finite lists. Banik demonstrates this with keywords and credits and we have that code for you in the Content Based Recommenders file in this directory. \n",
    "\n",
    "There are some things consider when using the metadata to generate your list of features. First, if you have multiple metadata fields, you'll probably want to combine them. Banik calls this combined list of words the \"soup.\" I think he's thinking of alphabet soup here, since we're throwing all the words together and stirring them up. That might be fine if each of your categorical lists of words are single words. But if you have categories that include phrases, you should consider pre-processing the phrases in such a way that the spaces between your words in the phrases are removed. For instance, remembering what you've learned so far, think about what would happen if you had the following records:\n",
    "\n",
    "| Business      | Keywords                 |\n",
    "| ------------- | ------------------------ |\n",
    "| McDonald's    | Fast Food, High Volume   |\n",
    "| MJ's Finest   | High Prices, Upscale |\n",
    "| Panera        | Fast Casual              |\n",
    "\n",
    "If we were to vectorize the keywords as-is, we'd end up with the following words: fast, food, high, volume, prices, upscale, casual. But we'd lose the differentiation between high-volume and high-prices and fast-food and fast-casual. Banik uses \"sanitizing\" to prevent this kind of ambiguity. The sanitizing function is available in the book and in the Content Recommender chapter notebook.\n",
    "\n",
    "We can also demonstrate the basic principles with our snip dataset. We already have our genres in a list and none of our snip movies have more than 3 genres and each of our genres are only 1 word. So we don't have to generate lists or sanitize anything. We simply need to create a soup of our overview and our genres.\n",
    "\n",
    "Note: genres is a list, so we'll need to use ' '.join() to turn it into a string. Overview is a string, so we just need to add that string onto the end of the string created after ' '.join()ing the genres. Be sure to add a space in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#reminder again - what's in snip\n",
    "snip = snip.copy()\n",
    "display(snip)\n",
    "\n",
    "#Function that creates a soup out of the desired metadata\n",
    "def create_soup(x):\n",
    "    return ' '.join(x['genres']) + ' ' + x['overview'] \n",
    "\n",
    "#create a column with the soup in it    \n",
    "snip['soup'] = snip.apply(create_soup, axis=1)   \n",
    "\n",
    "\n",
    "print(f'The soup for Toy Story is: \\n{snip[\"soup\"][0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Banik used a count vectorizer instead of a tf-idf vectorizer for his metadata recommender. He does this because using a tf-idf would downweight actors that appear in more than one movie. The same thing would happen with genres. So we'll follow suit and use the count vectorizer here. We'll remove stopwords and convert to lowercase and remove punctuation, but we won't use n-grams or lemmatization here.\n",
    "\n",
    "You can read <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">the documentation for count_vectorizer</a> to learn more about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "\n",
    "count = CountVectorizer(stop_words='english', lowercase=True)\n",
    "count_matrix = count.fit_transform(snip['soup'])\n",
    "\n",
    "#Compute the cosine similarity score \n",
    "cosine_sim2 = cosine_similarity(count_matrix, count_matrix)\n",
    "\n",
    "\n",
    "#call our same function, using the same movie. \n",
    "content_recommender(snip, 'Grumpier Old Men', 'title', cosine_sim2, topN=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Even with this tiny dataset, switching between TF-IDF and CountVectorizer and the linear kernel and the cosine-similarity changed our top two results.\n",
    "\n",
    "It's worth trying different approaches with your data to determine the right fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## *Self-Assessment: Metadata Recommender*\n",
    "\n",
    "Using the Ted Talks data, create a content recommender using the \"ratings\" and \"tags\" columns as your features. These columns include multi-word phrases, so use Banik's sanitize function on those two columns. Use a count vectorizer, removing English stop words and converting to lowercase, to create your vectorized matrix. Use cosine_similarity to compute your similarity matrix. Return the top 5 talks most closely related to the **title** \"Humble plants that hide surprising secrets.\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "260.517px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}