{"backend_state":"running","connection_file":"/tmp/xdg-runtime-user/jupyter/kernel-14d04b2e-30c6-42c4-9d51-f29258d85590.json","kernel":"python3","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1693264288691,"exec_count":4,"id":"072dc3","input":"#import the diamonds csv file\ndiamonds = pd.read_csv('data/diamonds_transformed.csv')\ndiamonds","kernel":"python3","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cut_Fair</th>\n      <th>Cut_Good</th>\n      <th>Cut_Ideal</th>\n      <th>Cut_Signature-Ideal</th>\n      <th>Cut_Very Good</th>\n      <th>Color_D</th>\n      <th>Color_E</th>\n      <th>Color_F</th>\n      <th>Color_G</th>\n      <th>Color_H</th>\n      <th>...</th>\n      <th>Carat Weight_13.0</th>\n      <th>Carat Weight_2.0</th>\n      <th>Carat Weight_3.0</th>\n      <th>Carat Weight_4.0</th>\n      <th>Carat Weight_5.0</th>\n      <th>Carat Weight_6.0</th>\n      <th>Carat Weight_7.0</th>\n      <th>Carat Weight_8.0</th>\n      <th>Carat Weight_9.0</th>\n      <th>Price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5169</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3470</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3183</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4370</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3171</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5995</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>6250</td>\n    </tr>\n    <tr>\n      <th>5996</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5328</td>\n    </tr>\n    <tr>\n      <th>5997</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>6157</td>\n    </tr>\n    <tr>\n      <th>5998</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>11206</td>\n    </tr>\n    <tr>\n      <th>5999</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>30507</td>\n    </tr>\n  </tbody>\n</table>\n<p>6000 rows Ã— 41 columns</p>\n</div>","text/plain":"      Cut_Fair  Cut_Good  Cut_Ideal  Cut_Signature-Ideal  Cut_Very Good  \\\n0          0.0       0.0        1.0                  0.0            0.0   \n1          0.0       0.0        1.0                  0.0            0.0   \n2          0.0       0.0        1.0                  0.0            0.0   \n3          0.0       0.0        1.0                  0.0            0.0   \n4          0.0       0.0        1.0                  0.0            0.0   \n...        ...       ...        ...                  ...            ...   \n5995       0.0       0.0        1.0                  0.0            0.0   \n5996       0.0       0.0        0.0                  0.0            1.0   \n5997       0.0       0.0        1.0                  0.0            0.0   \n5998       0.0       0.0        0.0                  1.0            0.0   \n5999       0.0       0.0        1.0                  0.0            0.0   \n\n      Color_D  Color_E  Color_F  Color_G  Color_H  ...  Carat Weight_13.0  \\\n0         0.0      0.0      0.0      0.0      1.0  ...                0.0   \n1         0.0      0.0      0.0      0.0      1.0  ...                0.0   \n2         0.0      0.0      0.0      0.0      1.0  ...                0.0   \n3         0.0      1.0      0.0      0.0      0.0  ...                0.0   \n4         0.0      0.0      0.0      1.0      0.0  ...                0.0   \n...       ...      ...      ...      ...      ...  ...                ...   \n5995      1.0      0.0      0.0      0.0      0.0  ...                0.0   \n5996      1.0      0.0      0.0      0.0      0.0  ...                0.0   \n5997      1.0      0.0      0.0      0.0      0.0  ...                0.0   \n5998      0.0      0.0      0.0      1.0      0.0  ...                0.0   \n5999      0.0      1.0      0.0      0.0      0.0  ...                0.0   \n\n      Carat Weight_2.0  Carat Weight_3.0  Carat Weight_4.0  Carat Weight_5.0  \\\n0                  0.0               0.0               0.0               1.0   \n1                  0.0               0.0               0.0               0.0   \n2                  0.0               0.0               0.0               0.0   \n3                  1.0               0.0               0.0               0.0   \n4                  0.0               0.0               0.0               0.0   \n...                ...               ...               ...               ...   \n5995               0.0               0.0               1.0               0.0   \n5996               0.0               0.0               1.0               0.0   \n5997               0.0               0.0               1.0               0.0   \n5998               0.0               0.0               0.0               0.0   \n5999               0.0               0.0               0.0               0.0   \n\n      Carat Weight_6.0  Carat Weight_7.0  Carat Weight_8.0  Carat Weight_9.0  \\\n0                  0.0               0.0               0.0               0.0   \n1                  0.0               0.0               0.0               0.0   \n2                  0.0               0.0               0.0               0.0   \n3                  0.0               0.0               0.0               0.0   \n4                  0.0               0.0               0.0               0.0   \n...                ...               ...               ...               ...   \n5995               0.0               0.0               0.0               0.0   \n5996               0.0               0.0               0.0               0.0   \n5997               0.0               0.0               0.0               0.0   \n5998               0.0               1.0               0.0               0.0   \n5999               0.0               0.0               0.0               0.0   \n\n      Price  \n0      5169  \n1      3470  \n2      3183  \n3      4370  \n4      3171  \n...     ...  \n5995   6250  \n5996   5328  \n5997   6157  \n5998  11206  \n5999  30507  \n\n[6000 rows x 41 columns]"},"exec_count":4}},"pos":6,"start":1693264288543,"state":"done","type":"cell"}
{"cell_type":"code","end":1693264295923,"exec_count":5,"id":"128f91","input":"#in x get all columns except Price column\nX = diamonds.loc[:, diamonds.columns != 'Price']\n#use Price column as target variable\ny= diamonds['Price']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)","kernel":"python3","pos":10,"start":1693264295909,"state":"done","type":"cell"}
{"cell_type":"code","end":1693264300757,"exec_count":6,"id":"b8991d","input":"# Create a model object (hyperparameters would be declared here, but we are using default values) \nmodel_lr = LinearRegression()\n\n# Fit the model by calling its fit() method\nmodel_lr.fit(X_train, y_train)","kernel":"python3","output":{"0":{"data":{"text/plain":"LinearRegression()"},"exec_count":6}},"pos":15,"start":1693264300262,"state":"done","type":"cell"}
{"cell_type":"code","end":1693264305756,"exec_count":7,"id":"94c2d7","input":"score_training = model_lr.score(X_train,y_train)\nprint(f\"Model r-squared score from training data {score_training:.4f}\")","kernel":"python3","output":{"0":{"name":"stdout","text":"Model r-squared score from training data 0.8731\n"}},"pos":17,"scrolled":true,"start":1693264305746,"state":"done","type":"cell"}
{"cell_type":"code","end":1693264310182,"exec_count":8,"id":"f199bb","input":"# Step 4 - assess model quality on test data\nscore_test = model_lr.score(X_test,y_test)\nprint(f\"Model r-squared score from test data: {score_test:.4f}\")","kernel":"python3","output":{"0":{"name":"stdout","text":"Model r-squared score from test data: 0.8742\n"}},"pos":19,"start":1693264310176,"state":"done","type":"cell"}
{"cell_type":"code","end":1693264314910,"exec_count":9,"id":"078ff0","input":"# Make predictions\ny_pred = model_lr.predict(X_test)\n\n# The plot is optional, but it gives an idea of the model accuracy, \n# in a perfect model the points would line up along the diagonal (y=x)\n# import matplotlib.pyplot as plt\nplt.figure(figsize=(9,6))\nplt.plot(y_test,y_pred,'k.')\nplt.xlabel('Test Values')\nplt.ylabel('Predicted Values');","kernel":"python3","output":{"0":{"data":{"image/png":"321017041426b74d377585fdf36f7964dd7c4d4d","text/plain":"<Figure size 648x432 with 1 Axes>"},"metadata":{"image/png":{"height":370,"width":582}}}},"pos":21,"start":1693264314249,"state":"done","type":"cell"}
{"cell_type":"code","end":1693264325141,"exec_count":10,"id":"036f15","input":"# Assess accuracy on test-data.\n\n# from sklearn.metrics import mean_squared_error\nmse = mean_squared_error(y_test,y_pred)\nrmse = np.sqrt(mse)\nprint(f\"Mean squared error on test data: {mse:.2f}\")\nprint(f\"Root mean squared error on test data: {rmse:.2f}\")","kernel":"python3","output":{"0":{"name":"stdout","text":"Mean squared error on test data: 13563408.00\nRoot mean squared error on test data: 3682.85\n"}},"pos":23,"start":1693264325135,"state":"done","type":"cell"}
{"cell_type":"code","end":1693264329186,"exec_count":11,"id":"898a4b","input":"# Here is all the code in one cell with most of it wrapped into a function for reuse\n\n# from sklearn.linear_model import LinearRegression\nmodel_lr = LinearRegression()\nmodel_lr.fit(X_train,y_train) # this could be inside the function below too\n\ndef my_regression_results(model):\n    score_test = model.score(X_test,y_test)\n    print('Model r-squared score from test data: {:0.4f}'.format(score_test))\n\n    y_pred = model.predict(X_test)\n    # import matplotlib.pyplot as plt\n    plt.figure(figsize=(9,6))\n    plt.plot(y_test,y_pred,'k.')\n    plt.xlabel('Test Values')\n    plt.ylabel('Predicted Values');\n\n    # from sklearn.metrics import mean_squared_error\n    mse = mean_squared_error(y_test,y_pred)\n    rmse = np.sqrt(mse)\n    print('Mean squared error on test data: {:0.2f}'.format(mse))\n    print('Root mean squared error on test data: {:0.2f}'.format(rmse))\n    return (round(rmse, 2))\n    \nmy_regression_results(model_lr)","kernel":"python3","output":{"0":{"name":"stdout","text":"Model r-squared score from test data: 0.8742\nMean squared error on test data: 13563408.00\nRoot mean squared error on test data: 3682.85\n"},"1":{"data":{"text/plain":"3682.85"},"exec_count":11},"2":{"data":{"image/png":"321017041426b74d377585fdf36f7964dd7c4d4d","text/plain":"<Figure size 648x432 with 1 Axes>"},"metadata":{"image/png":{"height":370,"width":582}}}},"pos":25,"start":1693264328633,"state":"done","type":"cell"}
{"cell_type":"code","end":1693264336489,"exec_count":12,"id":"4764ab","input":"# from sklearn.ensemble import RandomForestRegressor\n\nrf_model = RandomForestRegressor(random_state=0)\nrf_model.fit(X_train,y_train)\n\nrf_rmse = my_regression_results(rf_model)","kernel":"python3","output":{"0":{"name":"stdout","text":"Model r-squared score from test data: 0.9674\nMean squared error on test data: 3517749.65\nRoot mean squared error on test data: 1875.57\n"},"1":{"data":{"image/png":"aaf8b5ccf2809452ef3f990b4138a207944f8e6d","text/plain":"<Figure size 648x432 with 1 Axes>"},"metadata":{"image/png":{"height":370,"width":582}}}},"pos":29,"start":1693264334843,"state":"done","type":"cell"}
{"cell_type":"code","end":1693264343936,"exec_count":13,"id":"1a857a","input":"# import xgboost as xgb\n\nxgbr_model = xgb.XGBRegressor(objective ='reg:squarederror')\nxgbr_model.fit(X_train,y_train)\n\nxg_default_rmse = my_regression_results(xgbr_model)","kernel":"python3","output":{"0":{"name":"stdout","text":"Model r-squared score from test data: 0.9690\nMean squared error on test data: 3347681.78\nRoot mean squared error on test data: 1829.67\n"},"1":{"data":{"image/png":"7fb59bf6f11ee71c3926c3288914d36b1b9a5141","text/plain":"<Figure size 648x432 with 1 Axes>"},"metadata":{"image/png":{"height":370,"width":589}}}},"pos":33,"start":1693264341927,"state":"done","type":"cell"}
{"cell_type":"code","end":1693264353042,"exec_count":14,"id":"255469","input":"# from sklearn.model_selection import cross_val_score, KFold\nscores = cross_val_score(xgbr_model, X=X_train, y=y_train, cv = 3)\nprint(f\"The average score across the folds is {scores.mean():.4f}\")","kernel":"python3","output":{"0":{"name":"stdout","text":"The average score across the folds is 0.9718\n"}},"pos":37,"start":1693264349902,"state":"done","type":"cell"}
{"cell_type":"code","end":1693264355823,"exec_count":15,"id":"df05cb","input":"# from pprint import pprint\npprint(xgbr_model.get_xgb_params())","kernel":"python3","output":{"0":{"name":"stdout","text":"{'base_score': 0.5,\n 'booster': 'gbtree',\n 'colsample_bylevel': 1,\n 'colsample_bynode': 1,\n 'colsample_bytree': 1,\n 'gamma': 0,\n 'gpu_id': -1,\n 'interaction_constraints': '',\n 'learning_rate': 0.300000012,\n 'max_delta_step': 0,\n 'max_depth': 6,\n 'min_child_weight': 1,\n 'monotone_constraints': '()',\n 'n_jobs': 0,\n 'num_parallel_tree': 1,\n 'objective': 'reg:squarederror',\n 'random_state': 0,\n 'reg_alpha': 0,\n 'reg_lambda': 1,\n 'scale_pos_weight': 1,\n 'subsample': 1,\n 'tree_method': 'exact',\n 'validate_parameters': 1,\n 'verbosity': None}\n"}},"pos":41,"start":1693264355810,"state":"done","type":"cell"}
{"cell_type":"code","end":1693264764953,"exec_count":16,"id":"5ccb8d","input":"# run GridSearchCV with our xgbr_model to find better hyperparameters\n# from sklearn.model_selection import GridSearchCV\n\n# define the grid\nparams = {\n    \"learning_rate\": [0.01, 0.1],\n    \"max_depth\": [3, 5],\n    \"n_estimators\": [100,150],\n    \"subsample\": [0.8, 1],\n    \"min_child_weight\": [1, 3],\n    \"reg_lambda\": [1, 3],\n    \"reg_alpha\": [1, 3]\n}\n\n# setup the grid search\ngrid_search = GridSearchCV(xgbr_model,\n                           param_grid=params,\n                           cv=3,\n                           verbose=1,\n                           return_train_score=True)\n\ngrid_search.fit(X_train, y_train)","kernel":"python3","output":{"0":{"name":"stdout","text":"Fitting 3 folds for each of 128 candidates, totalling 384 fits\n"},"1":{"data":{"text/plain":"GridSearchCV(cv=3,\n             estimator=XGBRegressor(base_score=0.5, booster='gbtree',\n                                    colsample_bylevel=1, colsample_bynode=1,\n                                    colsample_bytree=1, gamma=0, gpu_id=-1,\n                                    importance_type='gain',\n                                    interaction_constraints='',\n                                    learning_rate=0.300000012, max_delta_step=0,\n                                    max_depth=6, min_child_weight=1,\n                                    missing=nan, monotone_constraints='()',\n                                    n_estimators=100, n_jobs=0,\n                                    num_parallel_tree=1, random_state=0,\n                                    reg_alpha=0, reg_lambda=1,\n                                    scale_pos_weight=1, subsample=1,\n                                    tree_method='exact', validate_parameters=1,\n                                    verbosity=None),\n             param_grid={'learning_rate': [0.01, 0.1], 'max_depth': [3, 5],\n                         'min_child_weight': [1, 3], 'n_estimators': [100, 150],\n                         'reg_alpha': [1, 3], 'reg_lambda': [1, 3],\n                         'subsample': [0.8, 1]},\n             return_train_score=True, verbose=1)"},"exec_count":16}},"pos":45,"start":1693264379882,"state":"done","type":"cell"}
{"cell_type":"code","end":1693264847764,"exec_count":17,"id":"69dc7b","input":"grid_search.best_params_","kernel":"python3","output":{"0":{"data":{"text/plain":"{'learning_rate': 0.1,\n 'max_depth': 5,\n 'min_child_weight': 1,\n 'n_estimators': 150,\n 'reg_alpha': 3,\n 'reg_lambda': 1,\n 'subsample': 0.8}"},"exec_count":17}},"pos":47,"start":1693264847757,"state":"done","type":"cell"}
{"cell_type":"code","end":1693264853130,"exec_count":18,"id":"24b562","input":"gs_rmse = my_regression_results(grid_search)","kernel":"python3","output":{"0":{"name":"stdout","text":"Model r-squared score from test data: 0.9739\n"},"1":{"name":"stdout","text":"Mean squared error on test data: 2809765.71\nRoot mean squared error on test data: 1676.24\n"},"2":{"data":{"image/png":"c7c37b35e6b96c9d1ffce8180e16f34168fcadee","text/plain":"<Figure size 648x432 with 1 Axes>"},"metadata":{"image/png":{"height":370,"width":589}}}},"pos":49,"start":1693264852331,"state":"done","type":"cell"}
{"cell_type":"code","end":1693264857657,"exec_count":19,"id":"8e77c6","input":"#a function to track the results of our different hyperparameter optimization approaches\ndef track_results(approachName, params, rmse, fits, current, show=True):\n    current['Approach'].append(approachName)\n    current['RMSE'].append(rmse)\n    current['Fits'].append(fits)\n    for k in params.keys():\n        current[k].append(params[k])\n    if show:\n        df = pd.DataFrame(current)\n        df = df.sort_values('RMSE', ascending=True)\n        display(df)\n    return current    \nsetup = {'Approach': [],\n         'learning_rate': [],\n         'max_depth': [],\n         'min_child_weight': [],\n         'n_estimators': [],\n         'reg_alpha': [],\n         'reg_lambda': [],\n         'subsample': [],\n         'Fits': [],\n         'RMSE': []\n        } \n\ndefaults = {\n        'learning_rate': .1,\n        'max_depth': 3,\n        'min_child_weight': 1,\n        'n_estimators': 100,\n        'reg_alpha': 0,\n        'reg_lambda': 1,\n        'subsample': 1,\n}\n\ncurrent = track_results('Default XGBoost', defaults, 1829.67, 1, setup, show=False  )\ncurrent = track_results('Grid Search', grid_search.best_params_, 1676.24,384, current, show=True )","kernel":"python3","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Approach</th>\n      <th>learning_rate</th>\n      <th>max_depth</th>\n      <th>min_child_weight</th>\n      <th>n_estimators</th>\n      <th>reg_alpha</th>\n      <th>reg_lambda</th>\n      <th>subsample</th>\n      <th>Fits</th>\n      <th>RMSE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>Grid Search</td>\n      <td>0.1</td>\n      <td>5</td>\n      <td>1</td>\n      <td>150</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0.8</td>\n      <td>384</td>\n      <td>1676.24</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Default XGBoost</td>\n      <td>0.1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>100</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>1829.67</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"          Approach  learning_rate  max_depth  min_child_weight  n_estimators  \\\n1      Grid Search            0.1          5                 1           150   \n0  Default XGBoost            0.1          3                 1           100   \n\n   reg_alpha  reg_lambda  subsample  Fits     RMSE  \n1          3           1        0.8   384  1676.24  \n0          0           1        1.0     1  1829.67  "}}},"pos":51,"start":1693264857645,"state":"done","type":"cell"}
{"cell_type":"code","end":1693264971061,"exec_count":20,"id":"5ddbe2","input":"# from sklearn.model_selection import RandomizedSearchCV\n# from scipy.stats import uniform, randint\n\nparams = {\n    \"learning_rate\": [0.001, 0.01, 0.1, 0.5, 1.],\n    \"max_depth\": randint(1, 10),\n    \"n_estimators\": randint(10, 150),\n    \"subsample\": uniform(0.05, 0.95),  # so uniform on [.05,.05+.95] = [.05,1.]\n    \"min_child_weight\": randint(1, 20),\n    \"reg_alpha\": uniform(0, 5),\n    \"reg_lambda\": uniform(0, 5)\n}\n\nrandom_search = RandomizedSearchCV(\n    xgbr_model,\n    param_distributions=params,\n    random_state=8675309,\n    n_iter=25,\n    cv=3,\n    verbose=1,\n    return_train_score=True)\n\nrandom_search.fit(X_train, y_train)","kernel":"python3","output":{"0":{"name":"stdout","text":"Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"},"1":{"data":{"text/plain":"RandomizedSearchCV(cv=3,\n                   estimator=XGBRegressor(base_score=0.5, booster='gbtree',\n                                          colsample_bylevel=1,\n                                          colsample_bynode=1,\n                                          colsample_bytree=1, gamma=0,\n                                          gpu_id=-1, importance_type='gain',\n                                          interaction_constraints='',\n                                          learning_rate=0.300000012,\n                                          max_delta_step=0, max_depth=6,\n                                          min_child_weight=1, missing=nan,\n                                          monotone_constraints='()',\n                                          n_estimators=100, n_jobs=0,\n                                          num_par...\n                                        'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fa678fb8b50>,\n                                        'reg_alpha': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fa6790cde50>,\n                                        'reg_lambda': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fa6790cd910>,\n                                        'subsample': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fa678fb81c0>},\n                   random_state=8675309, return_train_score=True, verbose=1)"},"exec_count":20}},"pos":54,"start":1693264900374,"state":"done","type":"cell"}
{"cell_type":"code","end":1693264977961,"exec_count":21,"id":"9b85cb","input":"random_search.best_params_","kernel":"python3","output":{"0":{"data":{"text/plain":"{'learning_rate': 0.5,\n 'max_depth': 8,\n 'min_child_weight': 13,\n 'n_estimators': 54,\n 'reg_alpha': 4.273531344366107,\n 'reg_lambda': 0.3614847715291919,\n 'subsample': 0.9827539273587139}"},"exec_count":21}},"pos":56,"start":1693264977943,"state":"done","type":"cell"}
{"cell_type":"code","end":1693264979855,"exec_count":22,"id":"a6fecd","input":"my_regression_results(random_search)","kernel":"python3","output":{"0":{"name":"stdout","text":"Model r-squared score from test data: 0.9676\nMean squared error on test data: 3497319.70\nRoot mean squared error on test data: 1870.11\n"},"1":{"data":{"text/plain":"1870.11"},"exec_count":22},"2":{"data":{"image/png":"16a29f84a24a04b7bb7f7a21d754d1aa91666e46","text/plain":"<Figure size 648x432 with 1 Axes>"},"metadata":{"image/png":{"height":370,"width":582}}}},"pos":57,"start":1693264979133,"state":"done","type":"cell"}
{"cell_type":"code","end":1693264985466,"exec_count":23,"id":"e4a1bf","input":"#track current results\ncurrent = track_results('Random Search', random_search.best_params_, 1870.11, 75, current, True )","kernel":"python3","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Approach</th>\n      <th>learning_rate</th>\n      <th>max_depth</th>\n      <th>min_child_weight</th>\n      <th>n_estimators</th>\n      <th>reg_alpha</th>\n      <th>reg_lambda</th>\n      <th>subsample</th>\n      <th>Fits</th>\n      <th>RMSE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>Grid Search</td>\n      <td>0.1</td>\n      <td>5</td>\n      <td>1</td>\n      <td>150</td>\n      <td>3.000000</td>\n      <td>1.000000</td>\n      <td>0.800000</td>\n      <td>384</td>\n      <td>1676.24</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Default XGBoost</td>\n      <td>0.1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>100</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1</td>\n      <td>1829.67</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Random Search</td>\n      <td>0.5</td>\n      <td>8</td>\n      <td>13</td>\n      <td>54</td>\n      <td>4.273531</td>\n      <td>0.361485</td>\n      <td>0.982754</td>\n      <td>75</td>\n      <td>1870.11</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"          Approach  learning_rate  max_depth  min_child_weight  n_estimators  \\\n1      Grid Search            0.1          5                 1           150   \n0  Default XGBoost            0.1          3                 1           100   \n2    Random Search            0.5          8                13            54   \n\n   reg_alpha  reg_lambda  subsample  Fits     RMSE  \n1   3.000000    1.000000   0.800000   384  1676.24  \n0   0.000000    1.000000   1.000000     1  1829.67  \n2   4.273531    0.361485   0.982754    75  1870.11  "}}},"pos":58,"start":1693264985411,"state":"done","type":"cell"}
{"cell_type":"code","end":1693265134971,"exec_count":24,"id":"e036e8","input":"from skopt import BayesSearchCV\nfrom skopt.space import Real, Categorical, Integer\n\nparams = {\n    \"learning_rate\": Categorical([0.001, 0.01, 0.1, 0.5, 1.]), #use categorical here\n    \"max_depth\": (1, 10), #this will optimize all integers 1-10 with equal probability\n    \"n_estimators\": (10, 150),\n    \"subsample\": (0.05, .95),\n    \"min_child_weight\": (1, 20),\n    \"reg_alpha\": (0, 5, 'uniform'),\n    \"reg_lambda\": (0, 5, 'uniform')\n}\n\nbayes_search = BayesSearchCV(\n    xgbr_model,   \n    search_spaces=params,\n    random_state=8675309,\n    n_iter=25,\n    cv=3,\n    verbose=1,\n    n_jobs=1,\n    return_train_score=True)\n\n\nbayes_search.fit(X_train, y_train)\n","kernel":"python3","output":{"0":{"name":"stdout","text":"Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"},"1":{"name":"stdout","text":"Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"},"10":{"name":"stdout","text":"Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"},"11":{"name":"stdout","text":"Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"},"12":{"name":"stdout","text":"Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"},"13":{"name":"stdout","text":"Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"},"14":{"name":"stdout","text":"Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"},"15":{"name":"stdout","text":"Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"},"16":{"name":"stdout","text":"Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"},"17":{"name":"stdout","text":"Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"},"18":{"name":"stdout","text":"Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"},"19":{"name":"stdout","text":"Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"},"2":{"name":"stdout","text":"Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"},"20":{"name":"stdout","text":"Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"},"21":{"name":"stdout","text":"Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"},"22":{"name":"stdout","text":"Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"},"23":{"name":"stdout","text":"Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"},"24":{"name":"stdout","text":"Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"},"25":{"data":{"text/plain":"BayesSearchCV(cv=3,\n              estimator=XGBRegressor(base_score=0.5, booster='gbtree',\n                                     colsample_bylevel=1, colsample_bynode=1,\n                                     colsample_bytree=1, gamma=0, gpu_id=-1,\n                                     importance_type='gain',\n                                     interaction_constraints='',\n                                     learning_rate=0.300000012,\n                                     max_delta_step=0, max_depth=6,\n                                     min_child_weight=1, missing=nan,\n                                     monotone_constraints='()',\n                                     n_estimators=100, n_jobs=0,\n                                     num_parallel...\n                                     tree_method='exact', validate_parameters=1,\n                                     verbosity=None),\n              n_iter=25, random_state=8675309, return_train_score=True,\n              search_spaces={'learning_rate': Categorical(categories=(0.001, 0.01, 0.1, 0.5, 1.0), prior=None),\n                             'max_depth': (1, 10), 'min_child_weight': (1, 20),\n                             'n_estimators': (10, 150),\n                             'reg_alpha': (0, 5, 'uniform'),\n                             'reg_lambda': (0, 5, 'uniform'),\n                             'subsample': (0.05, 0.95)},\n              verbose=1)"},"exec_count":24},"3":{"name":"stdout","text":"Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"},"4":{"name":"stdout","text":"Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"},"5":{"name":"stdout","text":"Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"},"6":{"name":"stdout","text":"Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"},"7":{"name":"stdout","text":"Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"},"8":{"name":"stdout","text":"Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"},"9":{"name":"stdout","text":"Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"}},"pos":62,"start":1693264997643,"state":"done","type":"cell"}
{"cell_type":"code","end":1693265141371,"exec_count":25,"id":"5f1b21","input":"print('Total iterations:', 75)\nbayes_search.best_params_","kernel":"python3","output":{"0":{"name":"stdout","text":"Total iterations: 75\n"},"1":{"data":{"text/plain":"OrderedDict([('learning_rate', 0.5),\n             ('max_depth', 6),\n             ('min_child_weight', 11),\n             ('n_estimators', 111),\n             ('reg_alpha', 0),\n             ('reg_lambda', 1),\n             ('subsample', 0.95)])"},"exec_count":25}},"pos":63,"start":1693265141256,"state":"done","type":"cell"}
{"cell_type":"code","end":1693265146755,"exec_count":26,"id":"df64c2","input":"my_regression_results(bayes_search)","kernel":"python3","output":{"0":{"name":"stdout","text":"Model r-squared score from test data: 0.9677\nMean squared error on test data: 3481666.20\nRoot mean squared error on test data: 1865.92\n"},"1":{"data":{"text/plain":"1865.92"},"exec_count":26},"2":{"data":{"image/png":"e231f9b90349c3f19652269ff1899e4f4a027aec","text/plain":"<Figure size 648x432 with 1 Axes>"},"metadata":{"image/png":{"height":370,"width":582}}}},"pos":64,"start":1693265146262,"state":"done","type":"cell"}
{"cell_type":"code","end":1693265154144,"exec_count":27,"id":"fc6659","input":"#track current results\ncurrent = track_results('Bayes Search', bayes_search.best_params_, 1865.92, 75, current, True )","kernel":"python3","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Approach</th>\n      <th>learning_rate</th>\n      <th>max_depth</th>\n      <th>min_child_weight</th>\n      <th>n_estimators</th>\n      <th>reg_alpha</th>\n      <th>reg_lambda</th>\n      <th>subsample</th>\n      <th>Fits</th>\n      <th>RMSE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>Grid Search</td>\n      <td>0.1</td>\n      <td>5</td>\n      <td>1</td>\n      <td>150</td>\n      <td>3.000000</td>\n      <td>1.000000</td>\n      <td>0.800000</td>\n      <td>384</td>\n      <td>1676.24</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Default XGBoost</td>\n      <td>0.1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>100</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1</td>\n      <td>1829.67</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Bayes Search</td>\n      <td>0.5</td>\n      <td>6</td>\n      <td>11</td>\n      <td>111</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.950000</td>\n      <td>75</td>\n      <td>1865.92</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Random Search</td>\n      <td>0.5</td>\n      <td>8</td>\n      <td>13</td>\n      <td>54</td>\n      <td>4.273531</td>\n      <td>0.361485</td>\n      <td>0.982754</td>\n      <td>75</td>\n      <td>1870.11</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"          Approach  learning_rate  max_depth  min_child_weight  n_estimators  \\\n1      Grid Search            0.1          5                 1           150   \n0  Default XGBoost            0.1          3                 1           100   \n3     Bayes Search            0.5          6                11           111   \n2    Random Search            0.5          8                13            54   \n\n   reg_alpha  reg_lambda  subsample  Fits     RMSE  \n1   3.000000    1.000000   0.800000   384  1676.24  \n0   0.000000    1.000000   1.000000     1  1829.67  \n3   0.000000    1.000000   0.950000    75  1865.92  \n2   4.273531    0.361485   0.982754    75  1870.11  "}}},"pos":65,"start":1693265154137,"state":"done","type":"cell"}
{"cell_type":"code","end":1693265351577,"exec_count":28,"id":"6b559c","input":"from tpot import TPOTRegressor\n\ntpot_config = {\n    'xgboost.XGBRegressor': {\n        'n_estimators': [100],\n        'max_depth': range(1, 11),\n        'learning_rate': np.append(np.array([.001,.01]),np.arange(0.05,1.05,.05)),\n        'subsample': np.arange(0.05, 1.01, 0.05),\n        'min_child_weight': range(1, 21),\n        'reg_alpha': np.arange(1.0,5.25,.25),\n        'reg_lambda': np.arange(1.0,5.25,.25),\n        'nthread': [1],\n        'objective': ['reg:squarederror']\n    }\n}\n\ntpot = TPOTRegressor(scoring = 'r2',\n                     generations=10,\n                     population_size=10,\n                     verbosity=2,\n                     config_dict=tpot_config,\n                     cv=3,\n                     template='Regressor', #no stacked models\n                     random_state=8675309)\n\ntpot.fit(X_train, y_train)\ntpot.export('tpot_XGBregressor.py') # export the model","kernel":"python3","metadata":{"hidden":true},"output":{"0":{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b2c6b51e3ca04aa58fc51a7bcabff217","version_major":2,"version_minor":0},"text/plain":"Optimization Progress:   0%|          | 0/110 [00:00<?, ?pipeline/s]"}},"1":{"name":"stdout","text":"\nGeneration 1 - Current best internal CV score: 0.9669359796971312\n"},"10":{"name":"stdout","text":"\nGeneration 10 - Current best internal CV score: 0.974643870779841\n"},"11":{"name":"stdout","text":"\nBest pipeline: XGBRegressor(input_matrix, learning_rate=0.1, max_depth=8, min_child_weight=1, n_estimators=100, nthread=1, objective=reg:squarederror, reg_alpha=3.25, reg_lambda=4.0, subsample=0.7500000000000001)\n"},"2":{"name":"stdout","text":"\nGeneration 2 - Current best internal CV score: 0.9739527739793802\n"},"3":{"name":"stdout","text":"\nGeneration 3 - Current best internal CV score: 0.9739527739793802\n"},"4":{"name":"stdout","text":"\nGeneration 4 - Current best internal CV score: 0.9739527739793802\n"},"5":{"name":"stdout","text":"\nGeneration 5 - Current best internal CV score: 0.9739527739793802\n"},"6":{"name":"stdout","text":"\nGeneration 6 - Current best internal CV score: 0.9740139708348456\n"},"7":{"name":"stdout","text":"\nGeneration 7 - Current best internal CV score: 0.9740139708348456\n"},"8":{"name":"stdout","text":"\nGeneration 8 - Current best internal CV score: 0.974643870779841\n"},"9":{"name":"stdout","text":"\nGeneration 9 - Current best internal CV score: 0.974643870779841\n"}},"pos":69,"start":1693265162388,"state":"done","type":"cell"}
{"cell_type":"code","end":1693265373613,"exec_count":29,"id":"33c9d6","input":"my_regression_results(tpot)","kernel":"python3","metadata":{"hidden":true},"output":{"0":{"name":"stdout","text":"Model r-squared score from test data: 0.9726\nMean squared error on test data: 2958126.24\nRoot mean squared error on test data: 1719.92\n"},"1":{"data":{"text/plain":"1719.92"},"exec_count":29},"2":{"data":{"image/png":"c4f297452f2ec34e64d3ab4ea7cf9322963e4ded","text/plain":"<Figure size 648x432 with 1 Axes>"},"metadata":{"image/png":{"height":370,"width":582}}}},"pos":71,"start":1693265373094,"state":"done","type":"cell"}
{"cell_type":"code","end":1693265383625,"exec_count":30,"id":"91cc9e","input":"tpot_best_params = {'learning_rate':0.1, 'max_depth':8, 'min_child_weight':1, 'n_estimators':100,   'reg_alpha':3.25, 'reg_lambda':4.0, 'subsample':0.7500000000000001}\n\ncurrent = track_results('TPOT', tpot_best_params, 1719.92,330, current, True )","kernel":"python3","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Approach</th>\n      <th>learning_rate</th>\n      <th>max_depth</th>\n      <th>min_child_weight</th>\n      <th>n_estimators</th>\n      <th>reg_alpha</th>\n      <th>reg_lambda</th>\n      <th>subsample</th>\n      <th>Fits</th>\n      <th>RMSE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>Grid Search</td>\n      <td>0.1</td>\n      <td>5</td>\n      <td>1</td>\n      <td>150</td>\n      <td>3.000000</td>\n      <td>1.000000</td>\n      <td>0.800000</td>\n      <td>384</td>\n      <td>1676.24</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TPOT</td>\n      <td>0.1</td>\n      <td>8</td>\n      <td>1</td>\n      <td>100</td>\n      <td>3.250000</td>\n      <td>4.000000</td>\n      <td>0.750000</td>\n      <td>330</td>\n      <td>1719.92</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Default XGBoost</td>\n      <td>0.1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>100</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1</td>\n      <td>1829.67</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Bayes Search</td>\n      <td>0.5</td>\n      <td>6</td>\n      <td>11</td>\n      <td>111</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.950000</td>\n      <td>75</td>\n      <td>1865.92</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Random Search</td>\n      <td>0.5</td>\n      <td>8</td>\n      <td>13</td>\n      <td>54</td>\n      <td>4.273531</td>\n      <td>0.361485</td>\n      <td>0.982754</td>\n      <td>75</td>\n      <td>1870.11</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"          Approach  learning_rate  max_depth  min_child_weight  n_estimators  \\\n1      Grid Search            0.1          5                 1           150   \n4             TPOT            0.1          8                 1           100   \n0  Default XGBoost            0.1          3                 1           100   \n3     Bayes Search            0.5          6                11           111   \n2    Random Search            0.5          8                13            54   \n\n   reg_alpha  reg_lambda  subsample  Fits     RMSE  \n1   3.000000    1.000000   0.800000   384  1676.24  \n4   3.250000    4.000000   0.750000   330  1719.92  \n0   0.000000    1.000000   1.000000     1  1829.67  \n3   0.000000    1.000000   0.950000    75  1865.92  \n2   4.273531    0.361485   0.982754    75  1870.11  "}}},"pos":73,"start":1693265383618,"state":"done","type":"cell"}
{"cell_type":"code","end":1693265946653,"exec_count":31,"id":"1a4e70","input":"# from tpot import TPOTRegressor\n\ntpot = TPOTRegressor(scoring = 'r2',\n                     generations=5,\n                     population_size=20,\n                     verbosity=2,\n                     cv=3,\n                     random_state=8675309)\n\ntpot.fit(X_train, y_train)\nprint(tpot.score(X_test, y_test))\ntpot.export('tpot_optimal_pipeline.py')","kernel":"python3","metadata":{"code_folding":[],"hidden":true},"output":{"0":{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"40514922e0684eccabf1be881477bc62","version_major":2,"version_minor":0},"text/plain":"Optimization Progress:   0%|          | 0/120 [00:00<?, ?pipeline/s]"}},"1":{"name":"stdout","text":"\nGeneration 1 - Current best internal CV score: 0.9680748947092271\n"},"2":{"name":"stdout","text":"\nGeneration 2 - Current best internal CV score: 0.9702235067536723\n"},"3":{"name":"stdout","text":"\nGeneration 3 - Current best internal CV score: 0.9702235067536723\n"},"4":{"name":"stdout","text":"\nGeneration 4 - Current best internal CV score: 0.9702235067536723\n"},"5":{"name":"stdout","text":"\nGeneration 5 - Current best internal CV score: 0.9702235067536723\n"},"6":{"name":"stdout","text":"\nBest pipeline: RidgeCV(PolynomialFeatures(input_matrix, degree=2, include_bias=False, interaction_only=False))\n"},"7":{"name":"stdout","text":"0.9692425005449955\n"}},"pos":77,"start":1693265487013,"state":"done","type":"cell"}
{"cell_type":"code","end":1693266008675,"exec_count":32,"id":"fcbe38","input":"my_regression_results(tpot)","kernel":"python3","metadata":{"hidden":true},"output":{"0":{"name":"stdout","text":"Model r-squared score from test data: 0.9692\nMean squared error on test data: 3317223.64\nRoot mean squared error on test data: 1821.32\n"},"1":{"data":{"text/plain":"1821.32"},"exec_count":32},"2":{"data":{"image/png":"528a3c070ed4f88756168ca86db10dd2ed2dd999","text/plain":"<Figure size 648x432 with 1 Axes>"},"metadata":{"image/png":{"height":370,"width":582}}}},"pos":78,"start":1693266008215,"state":"done","type":"cell"}
{"cell_type":"code","end":1693266017221,"exec_count":33,"id":"63fd71","input":"#ExtraTreesRegressor(LassoLarsCV(input_matrix, normalize=False), bootstrap=True, max_features=0.6000000000000001, min_samples_leaf=1, min_samples_split=6, n_estimators=100)\n\ntpot_best_params = {'learning_rate':'N/A', 'max_depth':'N/A', 'min_child_weight':'N/A', 'n_estimators':100,   'reg_alpha':'N/A', 'reg_lambda':'N/A', 'subsample': 'N/A'}\n\ncurrent = track_results('TPOT-AutoML', tpot_best_params, 1821.32,360, current, True )","kernel":"python3","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Approach</th>\n      <th>learning_rate</th>\n      <th>max_depth</th>\n      <th>min_child_weight</th>\n      <th>n_estimators</th>\n      <th>reg_alpha</th>\n      <th>reg_lambda</th>\n      <th>subsample</th>\n      <th>Fits</th>\n      <th>RMSE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>Grid Search</td>\n      <td>0.1</td>\n      <td>5</td>\n      <td>1</td>\n      <td>150</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0.8</td>\n      <td>384</td>\n      <td>1676.24</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TPOT</td>\n      <td>0.1</td>\n      <td>8</td>\n      <td>1</td>\n      <td>100</td>\n      <td>3.25</td>\n      <td>4.0</td>\n      <td>0.75</td>\n      <td>330</td>\n      <td>1719.92</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>TPOT-AutoML</td>\n      <td>N/A</td>\n      <td>N/A</td>\n      <td>N/A</td>\n      <td>100</td>\n      <td>N/A</td>\n      <td>N/A</td>\n      <td>N/A</td>\n      <td>360</td>\n      <td>1821.32</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Default XGBoost</td>\n      <td>0.1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>100</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1829.67</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Bayes Search</td>\n      <td>0.5</td>\n      <td>6</td>\n      <td>11</td>\n      <td>111</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.95</td>\n      <td>75</td>\n      <td>1865.92</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Random Search</td>\n      <td>0.5</td>\n      <td>8</td>\n      <td>13</td>\n      <td>54</td>\n      <td>4.273531</td>\n      <td>0.361485</td>\n      <td>0.982754</td>\n      <td>75</td>\n      <td>1870.11</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"          Approach learning_rate max_depth min_child_weight  n_estimators  \\\n1      Grid Search           0.1         5                1           150   \n4             TPOT           0.1         8                1           100   \n5      TPOT-AutoML           N/A       N/A              N/A           100   \n0  Default XGBoost           0.1         3                1           100   \n3     Bayes Search           0.5         6               11           111   \n2    Random Search           0.5         8               13            54   \n\n  reg_alpha reg_lambda subsample  Fits     RMSE  \n1         3          1       0.8   384  1676.24  \n4      3.25        4.0      0.75   330  1719.92  \n5       N/A        N/A       N/A   360  1821.32  \n0         0          1         1     1  1829.67  \n3         0          1      0.95    75  1865.92  \n2  4.273531   0.361485  0.982754    75  1870.11  "}}},"pos":80,"start":1693266017214,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"1516e3","input":"","pos":86,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"2ccb62","input":"","pos":87,"type":"cell"}
{"cell_type":"code","exec_count":3,"id":"d57dbc","input":"# EXECUTE FIRST\n# computational imports\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, KFold, RandomizedSearchCV\nimport xgboost as xgb\nfrom scipy.stats import uniform, randint\nfrom tpot import TPOTRegressor\nfrom pprint import pprint\n\n# plotting imports\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\n\n# display imports\nfrom IPython.display import display, IFrame\nfrom IPython.core.display import HTML\n\n# import warnings\nimport warnings\n\n# Note - you may see a warning about \"pandas.Int64Index is deprecated\".  Don't worry about this.  When CoCalc updates the version of xgb that is used, this warning should vanish.","kernel":"python3","metadata":{"code_folding":[0]},"output":{"0":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/xgboost/compat.py:93: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  from pandas import MultiIndex, Int64Index\n"}},"pos":0,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"001b99","input":"This is a nice improvement over the default XGBoost regression model.  Our tuned model now performs better than the linear regression model we saw above.\n\nThe main drawback to grid search is that it can get really expensive if we want to exhaustively search, particularly if the model fits are slow as they can be when large datasets with many predictors are used.\n\nBefore we get too far, let's create a way to track our results. In the next cell, we have a function that will track the approach we're using, the best hyperparameters, and the RMSE. Note that we call it twice - once to enter the data from our default XGBoost model, and once to add the results from our grid search approach. Note that we use show=False the first time, so that we only output a single dataframe of results. We return our current dictionary of results, which we can use each successive time we call the function.","pos":50,"type":"cell"}
{"cell_type":"markdown","id":"0137ff","input":"The random forest model, with default parameters, is much better than the linear regression model.","pos":30,"type":"cell"}
{"cell_type":"markdown","id":"04e353","input":"With this seed, Bayes Search with XGBoost still beat the AutoML with TPOT. If you were to run TPOT again with a different random seed or with different settings (more generations/individuals) you'd very likely find a different model.  It often produces nested models where models are applied in sequence. \n\nUnderstanding the details of nested models and such isn't important here and we don't recommend blindly using AutoML of any sort, but TPOT can provide good starting points and suggestions for models to investigate further.  We're eager to try other AutoML tools to see how they work.","metadata":{"hidden":true},"pos":81,"type":"cell"}
{"cell_type":"markdown","id":"0d1774","input":"We'd like to optimize the scores of our models when applied to data the model hasn't seen. However, the model doesn't see the test data during model training.  To estimate the test data model score we apply k-fold cross validation to the model training process.\n\nThis technique is taught in DS740 and you can learn more in <a href=\"https://machinelearningmastery.com/k-fold-cross-validation/\">this article</a>.  Basically the training data is divided into k subsets and the subsets are used to build models.  The scores from these models are averaged to estimate the score when applied to unseen data.  Cross validation is used by all of our hyperparameter optimization algorithms to estimate the model score and this estimated score is what we try to optimize.  \n\nWe won't really have to do cross-validation directly, but this bit of code shows how we could do it using `sklearn`.  Here we are estimating the test error of our xgboost model using 3-fold cross-validation (3 is the default number of folds for `cross_val_score`, but 5 is more commonly used for hyperparameter optimization):","pos":36,"type":"cell"}
{"cell_type":"markdown","id":"105410","input":"This number doesn't have the same clean interpretation as r-squared in a statistics setting, but it is useful for comparing models if we remember larger scores are better.  (Note:  if we were looking at mean square instead, then smaller would be better.)\n\nFinally we can use our model to make predictions:","pos":20,"type":"cell"}
{"cell_type":"markdown","id":"1753b4","input":"## Setup the data","pos":8,"type":"cell"}
{"cell_type":"markdown","id":"186bb6","input":"Now it's very easy to assess other models by first creating a model object in sklearn and then using our custom function `my_regression_results`. \n","pos":26,"type":"cell"}
{"cell_type":"markdown","id":"1daef1","input":"We've actually used TPOT in a rather narrow way by forcing it to optimize the hyperparameters for one choice of a machine learning model.  However, TPOT is really designed as an auto machine learning tool (AutoML) that tries to figure out optimize the whole machine learning pipeline:  data preprocessing, feature selection, model selection, and hyperparamter tuning.  For real problems this process could take days (see the <a href=\"https://epistasislab.github.io/tpot/using/#what-to-expect-from-automl-software\">TPOT discussion of AutoML</a>.  For this toy problem it doesn't take too long so let's see what it does.  In practice you would want to run the optimization as long as possible by increasing the number of generations and the population size.  \n\nBy specifying `None` for the config_dict parameter <a href=\"https://epistasislab.github.io/tpot/using/#built-in-tpot-configurations\">TPOT defaults</a> to optimizing the whole machine learning pipeline.  We'll turn it loose with a population size of 20 for 5 generations and a cv of 3. This ran in about 10 minutes on CoCalc. (This is far less generations and a much smaller population size that would be desirable in the real world.)","metadata":{"hidden":true},"pos":76,"type":"cell"}
{"cell_type":"markdown","id":"1de6b4","input":"Fitting a model in machine learning is an optimization problem.  In a previous lesson we saw how logistic and linear regression use optimization to find the regression model coefficients to minimize the difference between observed and predicted values of the response variable.\n\nMost machine learning models also come with a bunch of parameters that need to be set which can alter the fit of the model.  For example, here is the `LogisticRegression` class from scikit learn (`sklearn`):\n\n```\nclass sklearn.linear_model.LogisticRegression(penalty=â€™l2â€™, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=â€™warnâ€™, max_iter=100, multi_class=â€™warnâ€™, verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n```\n\nSome of these parameters have to do with exactly what model is fit.  For instance, `penalty` changes the form of regularization added to the objective function to prevent overfitting while `C` changes the strength of the regularization (larger `C` is less regularization).  These extra parameters are usually called hyperparameters and to get the best model they often need to be tuned.  This tuning is another kind of optimization and is usually called \"hyperparameter optimization\" or \"hyperparameter tuning\".  This is a hot area and a little searching with Google will yield a ton of results.  <a href=\"https://medium.com/criteo-labs/hyper-parameter-optimization-algorithms-2fe447525903\">Here is one article</a> that gives an overview of hyperparameter tuning methods (but gets a bit technical at the end).\n\nTo keep everything straight it helps to remember that model parameters and hyperparameters are different.  Hyperparameters are set or determined before the model is fit.  Model parameters are determined during the process of fitting the model to the data.\n\nHere are four kinds of hyperparameter optimization we'll explore:\n\n* **Grid Search:**  choose a list of possible values for each hyperparameter and loop over all the combinations of hyperparameters while fitting a model for each combination.  Return the combination that gives the best model performance.  We'll use `GridSearchCV` from the `sklearn` package.  \n\n* **Random Search:** choose a list of possible values, or a probability distribution, for each hyperparameter.  Choose combinations at random and return the combination that gives the best model performance.  We'll use `RandomSearchCV` from the `sklearn` package.\n\n* **Bayesian Optimization:**  after each iteration an approximate model of the objective function, called a surrogate model, is built and used to suggest a better set of hyperparameters for the next iteration.  This is really useful for objective functions that are expensive to evaluate and for which there is uncertainty or noise in the function values as there always will be when we sample data at random to feed into a machine learning model.  We'll use `BayesianSearchCV` from the `Scikit-Optimize` (skopt) package.  Other popular Python packages include `HyperOpt` and `GPyOpt`.  We aren't going to study the details of Bayesian Optimization in this class, but there are a number of tutorials on the topic.  I think <a href=\"http://krasserm.github.io/2018/03/21/bayesian-optimization/\">this article</a> is an especially good place to start if you want to learn more about the details.  In particular, the section called \"Implementation with NumPy and SciPy\" walks through the process of maximize a simple $y = f(x)$.\n\n* **Genetic Algorithms:**  a population of possible hyperparameter sets is evolved using selection, crossover, and mutation with the goal of identifying hyperparameters that yield trained models with the best performance.  We'll use the TPOT package.  We'll also see that the TPOT package is really an auto machine learning package because it can pick between multiple models using the genetic algorithm.\n\nThere are **other hyperparameter optimization approaches**, but Bayesian Optimization is currently the most popular.  Work through this presentation to get an idea of how hyperparameter optimization works!\n\nIf you haven't taken DS740 - Data Mining and Machine Learning yet, you can still follow along and do the assignment, but you might wish to revisit this assignment after you've completed DS740.  We're going to walk through the four methods of hyperparameter optimization using a problem involving regression.  Your assignment will be to apply the same methods to the classification problem of predicting loan defaults using a subset of the project data from DS705 - Statistical Methods class.","metadata":{"hidden":true},"pos":3,"type":"cell"}
{"cell_type":"markdown","id":"1f5a37","input":"The random search in this case did not find a very good fit. Note that it did not choose any of the default parameters. The default parameters outperformed those chosen by the random search.","pos":59,"type":"cell"}
{"cell_type":"markdown","id":"31827c","input":"Before we try to understand hyperparameter optimization, let's first explore the data we'll be using and the type of machine learning we'll be doing. \n\nWe'll be using a pre-processed diamonds dataset. You can read more about the <a href=\"https://towardsdatascience.com/predicting-diamond-prices-using-basic-measurement-metrics-bc8ba821c8f6\">variables in the diamond dataset in this blog post.</a> We have used Pycaret to <a href=\"https://github.com/pycaret/pycaret/blob/master/tutorials/Regression%20Tutorial%20Level%20Intermediate%20-%20REG102.ipynb\">pre-process the data</a> and output it as a CSV file stored in the data directory. First we'll read in the data.","pos":5,"type":"cell"}
{"cell_type":"markdown","id":"355192","input":"The best hyperparameter values are stored in the grid_search object as a dictionary:","pos":46,"type":"cell"}
{"cell_type":"markdown","id":"4606b8","input":"In this instance, TPOT chose an ExtraTreesRegressor. Since the parameters are different for that model, we can't just include the parameters in our results the way we have before. But, we can still track our RMSE and fits.","pos":79,"type":"cell"}
{"cell_type":"markdown","id":"48db88","input":"## AutoML with TPOT","metadata":{"heading_collapsed":true},"pos":75,"type":"cell"}
{"cell_type":"markdown","id":"493ff6","input":"We'll train a few different models just to get an idea of how things work and to establish a baseline.  By default these models fit the data by minimizing the mean squared error which is the average squared difference between the predicted target values and the actual target values. A closely related quantity is r-squared which is the proportion of the total variation in the target values that is captured by the predicted values.\n\n**Our goal, for each regression model, is to maximize r-squared as measured on the test data.**","pos":12,"type":"cell"}
{"cell_type":"markdown","id":"497ea9","input":"Random Forest regression uses an ensemble of decision trees and combines their final output to build a predictive model. If you haven't taken DS740 yet, that's OK.  All we really need to know is that a Random Forest regressor is a predictive model with a bunch of hyperparameters that can be changed and often are very influential in the model performance.  These models are computationally expensive to train because a typical ensemble uses 10 to 100 or more decision trees.","pos":28,"type":"cell"}
{"cell_type":"markdown","id":"4d7905","input":"You can interpret the rmse to roughly mean that the average error in our predictions is about \\$3682.  Below we condense the code into one cell to make it easier to follow.","pos":24,"type":"cell"}
{"cell_type":"markdown","id":"4e8c02","input":"# Assignment","metadata":{"heading_collapsed":true},"pos":84,"type":"cell"}
{"cell_type":"markdown","id":"4eecee","input":"XGBoost is a decision-tree-based ensemble algorithm that uses a gradient boosting framework to produce some pretty fantastic results.  We won't go into the details, but <a href=\"https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d\">this article</a> has a pretty nice description and talks about a variety of decision-tree based algorithms.  XGBoost is typically quite a bit faster to train than a random forest algorithm, but can still be computationally expensive.","pos":32,"type":"cell"}
{"cell_type":"markdown","id":"522787","input":"The idea behind grid search is to pick a list of potential values for each hyperparameter and then search all the possible combinations doing a k-fold cross validation for each combination.  That means we have to do k * number of combinations model fits.  If we were to run this code\n\n```\nparams = {\n    \"learning_rate\": [0.001, 0.01, 0.1, 0.5, 1.],\n    \"max_depth\": np.arange(1,11),\n    \"n_estimators\": [10,50,100,150],\n    \"subsample\": np.arange(0.05,1.01,0.05),\n    \"min_child_weight\": np.arange(1,21),\n    \"reg_lambda\": np.arange(0,5.5,0.5),\n    \"reg_alpha\": np.arange(0,5.5,0.5)\n    \n}\n\ngrid_search = GridSearchCV(xgb_model, param_grid=params, cv=5, verbose=1, return_train_score=True)\n\ngrid_search.fit(X_train,y_train)\n```\n\nwe have $k=5$ and $5 \\times 10 \\times 4 \\times 20 \\times 20 \\times 10 \\times 10 = 8,000,000$ combinations for a total of 40,000,000 model fits. Even if we could fit 10 models per second, it would still take about 46 days to try all the models.  Nevertheless, GridSearchCV is commonly used by trimming the number of possible values for each parameter to get something manageable.\n\nAs you can see the lists or arrays of values for the hyperparameters are stored in\n a dictionary.  The number of cross-validation folds is set by `cv = 3`. (cv = 5 is better, but we've selected 3 for reduced run time)\n\nTo illustrate how this works we'll pick fewer values for each hyperparameter as shown in the next cell, but we are still doing $2^7 \\times 3 = 384$ model fits which takes several minutes to run on our computers.","pos":44,"type":"cell"}
{"cell_type":"markdown","id":"52abf4","input":"# Hyperparameter Optimization applied to XGBoost Regression","pos":39,"type":"cell"}
{"cell_type":"markdown","id":"6672cb","input":"We really don't need to do much with genetic algorithms to use TPOT, though we can change the usual parameters like population size, mutation probability, and crossover probability.  The software authors recommend leaving the probabilities at their default values - <a href=\"https://epistasislab.github.io/tpot/\"> the documentation is here.</a> \n\nTPOT can actually do much more than optimize hyperparameters for a single model, but it can do that too.  To focus on a single model we set up a nested dictionary like that shown in the code below.    Then we call TPOT and it returns an optimized model in an object that behaves just like objects returned by GridSearchCV and RandomSearchCV.  Additional models could be added as `'model_name':{'param':values,'param':value,...}`.\n\nWe've found that we generally need more model fits to get good results with TPOT than we did with Bayesian Optimization, but it still works really well.  Note that TPOT is maximizing the k-fold cross-validated negative mean square error instead of r-squared, but it gets us to the same place.  Here we iterate for 10 generations with 10 different individual sets of hyperameters in each generation.  For each individual we do 3 model fits (k = 3) and there is an extra round of cross validated fits for the initial population, thus altogether we perform $$(\\mbox{number of folds}) \\times (\\mbox{number of generations + 1}) \\times (\\mbox{population size}) = 3 \\times (10 + 1) \\times 10 = 330 \\mbox{ model fits}.$$","metadata":{"hidden":true},"pos":68,"type":"cell"}
{"cell_type":"markdown","id":"668463","input":"After exploring several different hyperparameter optimization tools, we found that all of them improved the `XGBregressor` model by varying amounts.  Looking just at the hyperparameter optimization of the `xgbr_model` we found that `GridSearchCV` was the most expensive with 960 model fits, but found a very good model.  `RandomSearchCV` and `BayesianOptimizaion` used 125 model fits and 175 model fits, respectively and `BayesianOptimization` identified the model with the lowest MSE on the test data.  However, be careful before concluding that Bayesian Optimization outperforms Random Search.  If you change the random number seeds you'll get different results and Bayesian Optimization will not always be the winner.  However, the consensus is that it works better than Random Search on average.\n\nThe last thing we ran was an AutoML experiment with TPOT which used a genetic algorithm to search over many different models and hyperparameter choices.  The model it identified was a pretty crazy nested model that had a slight performance boost over the the optimized `xgboost` model. Due the complexity of that model it might not be the best choice, but it does provide a starting point for other directions to look.  \n\nIf you're curious to explore further, there are many AutoML tools being developed.  Here are a couple of interesting ones with which you might experiment:\n\n* **AzureML from Microsoft:** Check out <a href=\"https://docs.microsoft.com/en-us/azure/machine-learning/service/tutorial-auto-train-models\">this really cool tutorial</a> on using AutoML for choosing a regression model for predicting taxi fares.  The tutorial uses Python and sklearn so it wouldn't be a stretch to follow along.  Moreover, AzureML provides free credits when you sign up.  Just make sure to complete the \"Clean Up Resources\" section at the end of the tutorial so you don't leave anything running that will use up your free credits!\n\n* **RapidMiner:** We don't have personal experience with this one, but we've only heard good things about it and are eager to check it out.  <a href=\"https://rapidminer.com/educational-program/\">It is free for students.</a> RapidMiner's version of AutoML is a called Auto Model.  You can find a <a href=\"https://docs.rapidminer.com/latest/studio/auto-model/\">tutorial for predicting survival on the Titanic here.</a>\n\n* **Pycaret:** If you're familiar with Caret in R, <a href=\"https://pycaret.org/\">Pycaret</a> should excite you. PyCaret is in development, but new tools are being added all the time. Unfortunately, it doesn't work quite right on CoCalc without downgrading scikit-learn. We encourage you to create your own local environment and play with it on your own, though. There are some good tutorials.\n\nIf you try any other AutoML tools, please tell us about it on Piazza.","metadata":{"hidden":true},"pos":83,"type":"cell"}
{"cell_type":"markdown","id":"681c9c","input":"# But first... The Regression Problem","pos":4,"type":"cell"}
{"cell_type":"markdown","id":"6832d2","input":"## Using RandomizedSearchCV","pos":52,"type":"cell"}
{"cell_type":"markdown","id":"6f12fe","input":"## Using a Genetic Algorithm from TPOT","metadata":{"heading_collapsed":true},"pos":67,"type":"cell"}
{"cell_type":"markdown","id":"6fd721","input":"# Introduction","metadata":{"heading_collapsed":true},"pos":2,"type":"cell"}
{"cell_type":"markdown","id":"79623a","input":"### Linear Regression and sklearn basics","pos":13,"type":"cell"}
{"cell_type":"markdown","id":"807cfe","input":"The best hyperparameters found:","pos":55,"type":"cell"}
{"cell_type":"markdown","id":"8367bd","input":"Depending on the random seeds you've used you might see that Bayesian Optimization does better or worse than other methods.  (Try a few different seeds and compare.)  For the particular seed that we set, Bayes didn't do as well as grid search, and used far fewer fits (75 vs 384). \n\nBoth random search and Bayesian optimization will give better results if they're allowed to run for more iterations.  Bayesian optimization doesn't always beat random or grid search, but common wisdom suggests that it usually works better - see this <a href=\"https://stats.stackexchange.com/questions/302891/hyper-parameters-tuning-random-search-vs-bayesian-optimization\">Stack Exchange post</a> for some nice discussion and references.  Moreover companies are getting into automatic machine learning in a big way and some of the giants, <a href=\"https://cloud.google.com/blog/products/gcp/hyperparameter-tuning-cloud-machine-learning-engine-using-bayesian-optimization\">like Google, are betting on Bayesian Optimization</a>.  ","pos":66,"type":"cell"}
{"cell_type":"markdown","id":"86e41d","input":"From the plot we can see that the model is far from perfect, but it is getting the overall trend right.  One thing to note is that it's doing a pretty poor job at predicting large target values.  \n\nFor a real-world problem we'd want to assess the accuracy of the model predictions on the test data.  For regression problems this is often done with the mean square error or root mean square error (rmse):","pos":22,"type":"cell"}
{"cell_type":"markdown","id":"8a38d6","input":"If we can only afford to fit the model a limited number of times, then one approach is to search randomly instead of exhaustively.  To use `RandomizedSearchCV` we can either specify a probability distribution for each hyperparameter or we can specify a list of values for the hyperparameter in which case a value is chosen from the list assuming all values in the list are equally probable. \n\nFor optimizing our XGBoost model Wwe'll leave the learning rate as a list since we want more small values to choose from than large values.  The other hyperparameters can be specified with distributions.  Note that the uniform distribution specified below is not intuitive.  `uniform(loc,scale)` is uniform on the interval `[loc, loc+scale]`.  For the search below we're going to check just 25 randomly selected sets of hyperparameters as we might for a really expensive model.   `random_state = 8675309` is a random number seed for reproducibility.  Change it and you'll get different results.  ","pos":53,"type":"cell"}
{"cell_type":"markdown","id":"9287c5","input":"Several of the values are different than their default values.  To see if this optimized model is better than the default XGBoost model let's apply it to the test data:\n","pos":48,"type":"cell"}
{"cell_type":"markdown","id":"a5d6e4","input":"**Our objective is to predict the target variable (Price) from the other 40 features.**","pos":7,"type":"cell"}
{"cell_type":"markdown","id":"a8f434","input":"### XGBoost","pos":31,"type":"cell"}
{"cell_type":"markdown","id":"b46e3b","input":"Fortunately the default values shown above work pretty well in many problems.  Some of the hyperparameters don't directly change the model like `nthread` and `verbosity`.  Of the rest we'll pick a subset to optimize.  Some commonly optimized parameters are `n_estimators`, `max_depth`, `learning_rate`, `subsample`, and `min_child_weight` (these are the same ones that are optimized in the `TPOT` package). Two other hyperparameters linked to regularization terms are `reg_lambda` and `reg_alpha` which can be useful to prevent overfitting.\n\nThe table below lists some typical values and default values:\n\nHyperparameter | Default Value | Typical Range\n---- | ---- | ----\nn_estimators | 100 | 10 to 150\nmax_depth | 3 | 1 to 10\nmin_child_weight | 1 | 1 to 20\nlearning_rate | 0.1 | 0.001 to 1\nsubsample | 1 | 0.05 to 1\nreg_lambda | 1 | 0 to 5\nreg_alpha  | 0 | 0 to 5\n\nOf course, we could throw more hyperparameters into the mix, but we'll keep the numbers down to so we can afford to experiment.  \n","pos":42,"type":"cell"}
{"cell_type":"markdown","id":"b79c39","input":"To track our best parameters, you'll need to make a dictionary based on the \"Best pipeline\" printed out from tpot. Unfortunately, we have not found a way to capture this programmatically. But it's pretty easy to cut and paste.","pos":72,"type":"cell"}
{"cell_type":"markdown","id":"b92276","input":"# Bayesian Optimization with Scikit-Optimize\n\n<a href=\"https://scikit-optimize.github.io/stable/\">Scikit-Optimize</a> is an optimization package built on top of scikit-learn. It allows you to tune your hyperparameters using Bayesian optimization, as well as do some visualization of your optimization results.\n\n","pos":60,"type":"cell"}
{"cell_type":"markdown","id":"b95c3b","input":"# Estimating the model score without test data","pos":35,"type":"cell"}
{"cell_type":"markdown","id":"bdf1a3","input":"## Using GridSearchCV","pos":43,"type":"cell"}
{"cell_type":"markdown","id":"c50d4a","input":"In this example TPOT did slightly worse than `BayesianOptimization` and `Grid Search`, but TPOT can do much more as we'll see in the next section.","metadata":{"hidden":true},"pos":74,"type":"cell"}
{"cell_type":"markdown","id":"c7ee0e","input":"### Random Forest Regression","pos":27,"type":"cell"}
{"cell_type":"markdown","id":"c83b7d","input":"In a full machine learning analysis you would want to explore the data, do feature selection and possibly feature engineering including variable rescaling. Since you are not required to know those concepts before this course, we've done that for you. We're going to focus on only hyperparameter optimization.\n\nWe've already loaded the data, but we'll grab our features, `X`, and our target variable `y`.  We could just fit a model to all of the data, but we don't want a model that's just memorized the data and can't generalize to new data (overfitting).  So we usually divide the data into test and training sets.  The training set is used to fit a model, while the test set is used to validate the result.  Typically around 20% of the data is randomly selected for the test set.  We set the random number seed in `train_test_split` for reproducibility.","pos":9,"type":"cell"}
{"cell_type":"markdown","id":"ccdfa9","input":"We'll focus on the XGboost model for regression because it's a pretty amazing model.  If you haven't heard about it, then try to Google a bit or <a href=\"https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/\">check out this article</a> to learn more.  The XGboost model has many hyperparameters:","pos":40,"type":"cell"}
{"cell_type":"markdown","id":"cead0a","input":"**Here is the important bit about cross-validation** for estimating model performance:  *the k-fold cross-validated model score is the quantity we optimize in hyperparameter optimization*.\n\nFor regression problems we are usually minimizing the k-fold cross-validated mean square error.  For classification problems we maximize the k-fold cross-validated accuracy where accuracy is the total number of correct classifications divided by the total number of classifications.  The number of folds used is commonly $k=5$ or $k=10$, but we'll mostly use $k=3$ just to speed things up for learning purposes.","pos":38,"type":"cell"}
{"cell_type":"markdown","id":"ceae46","input":"# Lesson 08: Hyperparameter Optimization for Machine Learning (Project)\n\n**Some code in this notebook runs pretty slowly in CoCalc.  If you have the ability, you might wish to run this notebook locally or on a more powerful remote machine.**","pos":1,"type":"cell"}
{"cell_type":"markdown","id":"cf2ae9","input":"This is the same multiple linear regression model you've learned about in statistics.  We saw a bit about fitting a simple linear regression model in Python as part of Lesson 4.  Fitting a model in `sklearn` is pretty straightforward.","pos":14,"type":"cell"}
{"cell_type":"markdown","id":"db39d8","input":"Depending on what random seeds and such you're using, you may find that xgboost does better or worse than some of the other models.\n\nOur best model thus far with the seed we set is the XGBoost model.  Perhaps using the default hyperparameter values for training the other models wasn't the best choice.  In what follows, we'll try to improve the xgboost model by tuning its parameters.","pos":34,"type":"cell"}
{"cell_type":"markdown","id":"dcbf2b","input":"Optimize a random forest regression model and a XGboost classification model by completing the work in the `Lesson_08_Homework.ipynb` notebook.","metadata":{"hidden":true},"pos":85,"type":"cell"}
{"cell_type":"markdown","id":"e0570d","input":"This value of r-squared is the same as the value you met in statistics.  We interpret this to mean that our linear regression model captures 87.3% of the variation in the training data.  However, in a machine learning context we want to know how this model works on new data.  \n\nIf we apply the score() method to the test data it's no longer the value of r-squared that we learned about in a statistics class.  *This is because we are evaluating r-squared score on data that was not used to build the model.* For example, we can get a negative r-squared number which indicates that the model is performing worse, on the test data, than simply predicting that all of the target values are the same as the average target value.  \n\nIn short, when we compute the r-squared score() for the test data, values near one are great.  Values near zero just mean the model isn't really helping and negative values mean that are model is worse than no model at all.  This is the metric we will use to select our regression models so **bigger is better**.  Here is how we compute it:","pos":18,"type":"cell"}
{"cell_type":"markdown","id":"eae19d","input":"On average Bayesian optimization does better than random search.  It especially excels when there are lots of hyperparameters, but it won't beat random search every time.  The power of Bayesian optimization is that it can often achieve good results with a relatively short number of training iterations.  \n\nThe setup is a quite similar to random search, with just slight differences in how we set up our parameters, called <a href=\"https://scikit-optimize.github.io/stable/modules/generated/skopt.Optimizer.html\">dimensions</a> in the documentation. \n* for lists of values, we need to wrap the list in the Categorical() function\n* when we want a range of integers optimized, we just pass the low and high values\n* when we want a range of real numbers, we pass the low and high, making sure to use floats or wrap it in REAL()\n* when we know the distribution, we can pass the name of the distribution, along with the low and high (the default is uniform)\n","pos":61,"type":"cell"}
{"cell_type":"markdown","id":"ec0c2e","input":"In a basic machine learning pipeline we could look at the model score.  This is basically the objective function value being optimized by sci-kit (or a quantity derived from that).  For regression models we usually look at either\n* mean squared error: the average squared difference between the true and predicted target values \nor\n* r-squared: the proportion of the total variation in the target values that is accounted for by the model\n\n### Mathematical representation of $R^2$ and MSE:\n\nDefinitions:\n* $y_{i}$ = actual i*th* observation\n* $f_{i}$ = predicted i*th* observation\n* $\\bar{y}$ = mean of actual observations\n* $n$ = degrees of freedom (or num observations)\n\n**Total sum of squares** (proportional to the variance of the data):\n\n$ SS_{total} = \\displaystyle \\sum_{i} (y_{i} - \\bar{y})^2$\n\n**Residuals sum of squares** , also called sum of squares residuals:\n\n$ SS_{res} = \\displaystyle \\sum_{i} (y_{i} - f_{i})^2$\n\n**Mean squared error**:\n\nMSE = $SS_{res}\\over n $\n\n#### The most general definition of the coefficient of determination is:\n\n$ R^2 = 1 - {SS_{res}\\over SS_{tot}}$\n\nFor regression, if we compute the score of the model applied to the training data:","pos":16,"slide":"slide","type":"cell"}
{"cell_type":"markdown","id":"ed7860","input":"One nice feature of TPOT is that it can write the optimized model to a file which you can use elsewhere.\n\nWe can display the results on the test data in the same way as with our other models.","metadata":{"hidden":true},"pos":70,"type":"cell"}
{"cell_type":"markdown","id":"f993f6","input":"## Establishing a baseline","pos":11,"type":"cell"}
{"cell_type":"markdown","id":"fb3922","input":"# Summary","metadata":{"heading_collapsed":true},"pos":82,"type":"cell"}
{"id":0,"time":1693264109608,"type":"user"}
{"last_load":1693264109584,"type":"file"}