{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from tpot import TPOTClassifier\n",
    "from tpot import TPOTRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Lesson 08 Homework - Hyperparameter Optimization (Project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Note: This introduction is not included in the Canvas quiz.**\n",
    "\n",
    "For this project you're going to apply hyperparameter optimization to both a regression and a classification problem. It looks like a lot to do below, but it's mostly a matter of modifying code from the presentation. \n",
    "\n",
    "## Objective\n",
    "\n",
    "For each of the models in parts 1 and 2 below, apply the following 4 tuning methods from the presentation: GridSearchCV, RandomSearchCV, BayesSearchCV, and TPOT.\n",
    "* **For TPOT**: In Part 1 you will only do hyperparameter optimization for ExtraTreesRegressor. In Part 2 you will do **both** hyperparameter optimization and also run TPOT and let it choose the model. See the presentation for examples of both.\n",
    "\n",
    "## Specific Quiz Questions\n",
    "\n",
    "Follow along and use the required parameters and random seeds so that you can correctly answer the quiz questions.\n",
    "\n",
    "### Regarding data\n",
    "\n",
    "* To answer the multiple choice quiz questions, you'll need to use the data we have chosen. \n",
    "* We encourage you to try these out on your own data, too, to deepen your learning.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# **Part 1** - Optimize Extra Trees Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Find optimized hyperparameters for an extra trees regression model. \n",
    "\n",
    "In the lesson, our TPOT AutoML code suggested that a viable algorithm to explore would be the `ExtraTreesRegressor`. For part 1 of your homework, you'll use sklearn's ExtraTreesRegressor and attempt to optimize the hyperparameters.\n",
    "\n",
    "You must use the diamonds data used in the presentation.  **You do not need to include the TPOT general search for this problem** (use TPOT to optimize ExtraTreesRegressor, but don't run TPOT to choose a model). Here are ranges for a subset of the hyperparameters:\n",
    "\n",
    "Hyperparameter |Type | Default Value | Typical Range\n",
    "---- | ---- | ---- | ----\n",
    "n_estimators | discrete / integer | 100 | 10 to 150\n",
    "min_samples_split | discrete / integer | 2 | 2 to 20\n",
    "min_samples_leaf | discrete / integer | 1 | 1 to 10\n",
    "max_features | discrete/integer | auto | 1 to 30\n",
    "bootstrap | discrete / boolean | False | True, False\n",
    "\n",
    "\n",
    "Note: there other hyperparameters that could be added, but we will focus on these for the project. Consult the \n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html\">documentation for sklearn ExtraTreesRegressor</a> to see all of the available hyperparameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## **Question 1: Setup** <font color=\"magenta\">(1 points)</font>\n",
    "\n",
    "* Load the diamonds dataset (diamonds_transformed.csv in the data directory). \n",
    "* Set up your X and y variables. \n",
    "* Split into 80% training data and 20% testing data. \n",
    "* **Use random_state = 123**\n",
    "\n",
    "How many rows are in your training data?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## **Question 2** <font color=\"magenta\">(1 points)</font>\n",
    "In the following cell, we provide you with the same `my_regression_results` function we used in the lesson. Create an `ExtraTreesRegressor` model using **random_state=123**. Fit your model. Use the `my_regression_results` function to get the Root Mean Squared Error on the test data.\n",
    "\n",
    "What is the RMSE (Root Mean Squared Error) using the default hyperparameters?\n",
    "\n",
    "* 1875.57\n",
    "* 2056.87\n",
    "* 9688.00\n",
    "* 1833.88\n",
    "* 2053.20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#function to easily assess different models (not included in Canvas Quiz)\n",
    "def my_regression_results(model):\n",
    "    score_test = model.score(X_test,y_test)\n",
    "    print('Model r-squared score from test data: {:0.4f}'.format(score_test))\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    %matplotlib inline\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(y_test,y_pred,'k.')\n",
    "    plt.xlabel('Test Values')\n",
    "    plt.ylabel('Predicted Values');\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    mse = mean_squared_error(y_test,y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print('Mean squared error on test data: {:0.2f}'.format(mse))\n",
    "    print('Root mean squared error on test data: {:0.2f}'.format(rmse))\n",
    "    return (round(rmse, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## **Question 3** (Manually Graded) <font color=\"magenta\">(2 points)</font>\n",
    "\n",
    "Modify the `track_results` function to work with the Extra Trees Regressor hyperparameters. Enter your results based on the default hyperparameters and display the dataframe of results.\n",
    "\n",
    "In the Canvas quiz, copy your code and provide a screenshot of the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## ExtraTreesRegressor Grid Search \n",
    "\n",
    "Perform a cross-validated grid search using the following values for your hyperparameter search space.\n",
    "\n",
    "* n_estimators: [50, 100, 150]\n",
    "* max_features: [1, 15, 30]\n",
    "* min_samples_split: [2, 8]\n",
    "* min_samples_leaf: [1, 15]\n",
    "* bootstrap: [True, False]\n",
    "\n",
    "Use the following setting in your grid search:\n",
    "* cv=5\n",
    "\n",
    "*Note: this took about 16 minutes to run on CoCalc*\n",
    "\n",
    "Be sure to track your results using your `track_results` function.\n",
    "\n",
    "### **Question 4:** <font color=\"magenta\">(2 points)</font>\n",
    "**What is the RMSE of your optimized grid search, rounded to 2 digits?**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **Question 5:** <font color=\"magenta\">(2 points)</font>\n",
    "**What is the optimal value of max_features chosen by the grid search?**\n",
    "\n",
    "* 1\n",
    "* 15\n",
    "* 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#use this seed\n",
    "np.random.seed(123)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# track your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Randomized Search\n",
    "\n",
    "Use the following values to set up your randomized search space:\n",
    "* n_estimators: random integers between 10 and 150\n",
    "* min_samples_split: random integers between 2 and 20\n",
    "* min_samples_leaf: random integers between 1 and 20\n",
    "* max_features:  random integers between 1 and 30\n",
    "* bootstrap: True or False\n",
    "\n",
    "Use the following settings for your randomized search:\n",
    "\n",
    "* n_iter of 25\n",
    "* cv of 5\n",
    "* random_state of 123\n",
    "\n",
    "\n",
    "### **Question 6** <font color=\"magenta\">(2 points)</font>\n",
    "**What is the RMSE of your randomized search, rounded to 2 digits?**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **Question 7** <font color=\"magenta\">(2 points)</font>\n",
    "**What is the max_features chosen by your random search?**\n",
    "\n",
    "* 5\n",
    "* 7\n",
    "* 9\n",
    "* 11\n",
    "* 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#run your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#remember to track your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Bayesian Optimization\n",
    "\n",
    "For your Bayesian Optimization, we'll use the same ranges we used in random search. You won't need to wrap any of your integer ranges in Integer(), but you will need to use `Categorical([True, False])` for your bootstrap parameter.\n",
    "\n",
    "Use the following values to set up your search space:\n",
    "* n_estimators: integers between 10 and 150\n",
    "* min_samples_split: integers between 2 and 20\n",
    "* min_samples_leaf: integers between 1 and 10\n",
    "* max_features:  integers between 1 and 30\n",
    "* bootstrap: Categorical of True or False\n",
    "\n",
    "Use the following settings for your search:\n",
    "\n",
    "* n_iter of 25\n",
    "* cv of 5\n",
    "* random_state of 123\n",
    "\n",
    "### **Question 8** <font color=\"magenta\">(2 points)</font>\n",
    "**What is the RMSE of your Bayesian search, rounded to 2 digits?**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **Question 9** <font color=\"magenta\">(2 points)</font>\n",
    "**What is the value of min_samples_leaf chosen by your Bayesian search?**\n",
    "\n",
    "* 1\n",
    "* 3\n",
    "* 5\n",
    "* 7\n",
    "* 9\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#Run your Bayesian Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#remember to track your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### TPOT\n",
    "\n",
    "For TPOT, you'll use the following search config values:\n",
    "\n",
    "* n_estimators: each of the following integers - 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150\n",
    "* min_samples_split: all integers between 2 and 20 (inclusive of 20)\n",
    "* 'min_samples_leaf': all integers between 1 and 20 (inclusive of 20)\n",
    "* 'max_features': all integers between 1 and 30 (inclusive of 30)\n",
    "* 'bootstrap': either zero or 1\n",
    "\n",
    "Use 5 generations and a population size of 10 and a cv of 3. Use random state of 123. (Note, this is not nearly enough generations or big enough population to truly find the best hyperparameters. But, we also don't want you to have to sit and watch it chug through for an hour.)  We'll include stacked models here, so do not use `template='Regressor'` in your configuration dictionary.\n",
    "\n",
    "**Hint** the ExtraTreesRegressor is from the sklearn.ensemble package.\n",
    "\n",
    "\n",
    "### **Question 10** <font color=\"magenta\">(2 points)</font>\n",
    "**What is the RMSE of your TPOT search, rounded to 2 digits?**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **Question 11** <font color=\"magenta\">(2 points)</font>\n",
    "**What is the value of n_estimators chosen by your TPOT search?**\n",
    "\n",
    "* 60\n",
    "* 70\n",
    "* 80\n",
    "* 90\n",
    "* 110\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#run your TPOT code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Question 12 (Manually Graded) <font color=\"magenta\">(2 points)</font>\n",
    "\n",
    "Take a screenshot of your final results dataframe from the `track_results` function and upload it. Briefly comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#track results, output and screenshot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font color=\"green\">Don't forget to comment on your results. Comments should include some information about which method you'd choose and why.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# **Part Two** - Loan Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In part two, we'll explore optimizing hyperparameters for loan classification.\n",
    "\n",
    "### Notes:\n",
    "\n",
    "#### About the data\n",
    "The first cell below loads a subset of the loans default data from DS705 and your job is to predict whether a loan defaults or not.  The `status_bad` column is the target column and a 1 indicates a loan that defaulted.  We have selected a subset of the original data that includes 2000 each of good and bad loans.  The data has already been cleaned and encoded.\n",
    "\n",
    "#### This is classification, not regression\n",
    "The score for each model will be accuracy and not RMSE.  Your summary table should include accuracy, sensitivity, and precision for each optimized model applied to the test data.  (<a href=\"https://classeval.wordpress.com/introduction/basic-evaluation-measures/\">Here is a nice overview of metrics for binary classification data</a>) that includes definitions of accuracy and such.\n",
    "\n",
    "### Load the Data\n",
    "In the following cell, we load the data for you and split it into train and test dataframes. Do not change anything in this cell. (Cell not included in Canvas Quiz.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Do not change this cell for loading and preparing the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X = pd.read_csv('./data/loans_subset.csv')\n",
    "\n",
    "# split into predictors and target\n",
    "# convert to numpy arrays for xgboost, OK for other models too\n",
    "y = np.array(X['status_Bad']) # 1 for bad loan, 0 for good loan\n",
    "X = np.array(X.drop(columns = ['status_Bad']))\n",
    "\n",
    "# split into test and training data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## **Question 13** - Display Results Function (Manually graded)<font color=\"magenta\">(2 points)</font>\n",
    "\n",
    "In the next cell, we've demonstrated using the LogisticRegression model to perform classification and generate a confusion matrix.\n",
    "\n",
    "**Hint:** You can read more about the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\">confusion_matrix function</a> and a <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\">classification report function</a> in the Scikit-learn documentation. Both are also demonstrated in the extras folder of this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# we do need to go higher than the default iterations for the solver to get convergence\n",
    "# and the explicity declaration of the solver avoids a warning message, otherwise\n",
    "# the parameters are defaults.\n",
    "logreg_model = LogisticRegression(solver='lbfgs',max_iter=1000)\n",
    "\n",
    "#fit the model\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "# Use score method to get accuracy of model\n",
    "score = logreg_model.score(X_test, y_test) # this is accuracy\n",
    "print(f'The accuracy is {score}')\n",
    "\n",
    "# obtaining the confusion matrix and making it look nice\n",
    "y_pred = logreg_model.predict(X_test)\n",
    "\n",
    "# must put true before predictions in confusion matrix function\n",
    "cmtx = pd.DataFrame(\n",
    "    confusion_matrix(y_test, y_pred, labels=[1,0]), \n",
    "    index=['true:bad', 'true:good'], \n",
    "    columns=['pred:bad','pred:good']\n",
    ")\n",
    "display(cmtx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Based on the example above write a function called `my_classifier_results` modeled after `my_regression_results` that applies a model to the test data and prints out the accuracy, sensitivity, precision, and the confusion matrix and returns the accuracy, sensitivity and precision.  There is no need to make a plot.\n",
    "\n",
    "Call your function using the logistic regression model we just demonstrated. (Note that your confusion matrix and accuracy should match what is shown above.) Upload the code and a screenshot of the output.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Solution for 13\n",
    "def my_classifier_results(model):\n",
    "    #add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### XGBoost Classifier\n",
    "The algorithm that we will use to tune hyperparameters is the XGBClassifier algorithm from XGBoost. We've included the hyperparameters we'll tune and their defaults below:\n",
    "\n",
    "Hyperparameter | Type | Default Value | Typical Range\n",
    "---- | ---- | ---- | ----\n",
    "n_estimators | discrete / integer | 100 | 50 to 150\n",
    "max_depth | discrete / integer | 3| 1 to 10\n",
    "min_child_weight | discrete / integer | 1 | 1 to 20\n",
    "learning_rate | continuous / float | 0.1 | 0.001 to 1\n",
    "subsample | continuous / float | 1 | 0.05 to 1\n",
    "reg_lambda | continuous / float | 1 | 0 to 5\n",
    "reg_alpha  | continuous / float | 0 | 0 to 5\n",
    "\n",
    "### **Question 14** - <font color=\"magenta\">(2 points)</font>\n",
    "Generate the Default XBGClassifer Model. Note, you'll need to pass in objective = 'binary:logistic' when you instantiate the XGBClassifier.\n",
    "\n",
    "What is the accuracy of the default model, rounded to 3 digits?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Tracking Results Function\n",
    "\n",
    "Create a `track_results_classifier` function based on the `track_results` function. You'll be tracking each of the XGBClassifier hyperparameters as well as the name of the optimization approach, accuracy, precision, recall, and number of fits.\n",
    "\n",
    "Add the results from your default XGBClassifier model to the tracker.\n",
    "\n",
    "(Note: this is not graded here, but the output will be graded as part of the summary.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Grid Search for XGBClassifier\n",
    "Perform a grid search using the following parameters:\n",
    "\n",
    "* learning_rate: [0.01, 0.1],\n",
    "* max_depth: [3, 6],\n",
    "* n_estimators: [10, 100],\n",
    "* subsample: [0.5, 1],\n",
    "* min_child_weight: [1, 20],\n",
    "* reg_lambda: [1, 3],\n",
    "* reg_alpha: [0, 1]\n",
    "\n",
    "Use the following setting in your GridSearch:\n",
    "\n",
    "* cv = 3\n",
    "\n",
    "Set the np.random.seed to 123 (done for you in the cell below).\n",
    "\n",
    "*Note: This is a smaller than optimal grid, but we don't want you to have to wait forever to process. On CoCalc, this took about 10 minutes to run.\n",
    "\n",
    "## **Question 15** <font color=\"magenta\">(2 points)</font>\n",
    "\n",
    "What is the **accuracy** for your Grid Search, rounded to 3 digits?\n",
    "\n",
    "\n",
    "\n",
    "## **Question 16** <font color=\"magenta\">(2 points)</font>\n",
    "\n",
    "How many **fits** did your Grid Search do?\n",
    "\n",
    "* 384\n",
    "* 398\n",
    "* 279\n",
    "* 400\n",
    "* 375\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#Keep this random seed\n",
    "np.random.seed(123)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#don't forget to track your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Random Search\n",
    "Use the following parameters to generate a random search:\n",
    "\n",
    "* learning_rate: any value from the list [0.001, 0.01, 0.1, 0.5, 1.]\n",
    "* max_depth: any random integer between 1 and 10\n",
    "* n_estimators: any random integer between 50 and 150\n",
    "* subsample: uniform(0.05, 0.95)\n",
    "* min_child_weight: any random integer between 1 and 20\n",
    "* reg_alpha: uniform(0, 5)\n",
    "* reg_lambda: uniform(0, 5)\n",
    "\n",
    "Use the following settings in your random search:\n",
    "\n",
    "* random_state = 123\n",
    "* n_iter = 25\n",
    "* cv = 3\n",
    "\n",
    "## **Question 17** <font color=\"magenta\">(2 points)</font>\n",
    "\n",
    "What is the **accuracy** for your Random Search, rounded to 3 digits?\n",
    "\n",
    "\n",
    "\n",
    "## **Question 18** <font color=\"magenta\">(2 points)</font>\n",
    "\n",
    "What is the **learning_rate** chosen by your Random Search?\n",
    "\n",
    "* .001\n",
    "* .01\n",
    "* .1\n",
    "* .5\n",
    "* .1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#run your random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#don't forget to track your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Bayesian Optimization\n",
    "\n",
    "For your Bayesian Optimization, use the following parameters:\n",
    "* learning_rate: Any of the following values - [0.001, 0.01, 0.1, 0.5, 1.]) (Hint: you'll need to use categorical for this one)\n",
    "* max_depth: Any integer between 1 and 10\n",
    "* n_estimators: Any integer between 10 and 150\n",
    "* subsample: Any float between 0.05 and .95\n",
    "* min_child_weight: Any integer between 1 and 20\n",
    "* reg_alpha: Any integer between 0 and 5\n",
    "* reg_lambda: Any integer between 0 and 5\n",
    "\n",
    "For your call to BayesSearchCV use the following:\n",
    "* random_state = 123\n",
    "* n_inter = 15\n",
    "* cv = 3\n",
    "\n",
    "## **Question 19** <font color=\"magenta\">(2 points)</font>\n",
    "\n",
    "What is the **accuracy** for your Bayes Search, rounded to 3 decimal places?\n",
    "\n",
    "\n",
    "\n",
    "## **Question 20** <font color=\"magenta\">(2 points)</font>\n",
    "\n",
    "What is the **precision** for your Bayes Search, rounded to 3 decimal places?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#run your Bayesian Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#don't forget to track your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Genetic Algorithm from TPOT\n",
    "First, you'll tune the parameters specifically for the XGBClassifier using TPOTClassifier. This will be very similar to what we did in the lesson, except there we used TPOTRegressor. Use the following parameters in your configuration:\n",
    "\n",
    "* n_estimators: allow the values in the following list [50, 75, 100]\n",
    "* max_depth: allow all values between 1 and 10, inclusive (remember, range does not include the highest number)\n",
    "* learning_rate: use the values in the following list - [1e-3, 1e-2, 1e-1, 0.5, 1.],\n",
    "* subsample: <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.arange.html\">evenly spaced values</a> between .05 and 1, using a step of .05 (remember that you'll need to account for the stop number not being included)\n",
    "* min_child_weight: allow all values between 1 and 20, inclusive\n",
    "* reg_alpha: allow all values between 1 and 5, inclusive\n",
    "* reg_lambda: allow all values between 1 and 5, inclusive\n",
    "* objective: set it to ['binary:logistic']\n",
    "\n",
    "For the TPOTClassifier function use the following settings:\n",
    "\n",
    "* generations=5\n",
    "* population_size=20\n",
    "* cv=3\n",
    "* random_state=123\n",
    "\n",
    "## **Question 21** <font color=\"magenta\">(2 points)</font>\n",
    "\n",
    "What is the **accuracy** for your TPOT Search, rounded to 3 digits?\n",
    "\n",
    "\n",
    "\n",
    "## **Question 22** <font color=\"magenta\">(2 points)</font>\n",
    "\n",
    "What is the **n_estimators** chosen by your TPOT search?\n",
    "\n",
    "* 50\n",
    "* 75\n",
    "* 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#run TPOT for XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#don't forget to track your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### AutoML with TPOT\n",
    "\n",
    "Now that you've used TPOT to tune hyperparameters just for a single defined model (XGBoost), we're going to have you use TPOT to search for any algorithm. We refer to this as AutoML, for automated machine learning. We'll allow TPOT to find stacked models, so the hyperparameters being tuned won't be the same as the ones we've been tuning. When you store your results in your track results, you can just add 'n/a' for each of the hyperparameters.\n",
    "\n",
    "For AutoML with TPOT, we'd like you to use the following configuration:\n",
    "\n",
    "* generations=5\n",
    "* population_size=30\n",
    "* cv=3\n",
    "* scoring='accuracy'\n",
    "* random_state=123\n",
    "* config_dict='TPOT light'\n",
    "\n",
    "Note, we are using the 'TPOT light' configuration for speed here.  Only models that are quick to run are included.  If time was't an issue, then you would want to use the regular configuration, but we're trying to keep things simple for the homework, so stick with TPOT light.\n",
    "\n",
    "Remember, you're using the TPOTClassifier, not the TPOTRegressor as you used in the lesson.\n",
    "\n",
    "## **Question 23** <font color=\"magenta\">(2 points)</font>\n",
    "\n",
    "What is the **accuracy** for your TPOT AutoML Search, rounded to 3 digits?\n",
    "\n",
    "\n",
    "## **Question 24** <font color=\"magenta\">(2 points)</font>\n",
    "\n",
    "Which of these pipelines did TPOT choose?\n",
    "\n",
    "* BernoulliNB, MultinomialNB, DecisionTreeClassifier and MaxABsScaler\n",
    "* BernouliNB, CombineDFs, and VarianceThreshold\n",
    "* BernoulliNB and SelectPercentile\n",
    "* BernouliNB and CombineDFs\n",
    "* LogisticRegression, SelectFwe, and MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#Run TPOT automl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#don't forget to track your results. You may use 'n/a' for all the hyperparameters, since we're not going to end up with the same model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## **Question 25** - Summary (Manually Graded) <font color=\"magenta\">(4 points)</font>\n",
    "\n",
    "Take a screen shot of your results tracking table and upload it. It should have the columns for the approach used, the hyperparameters chosen, the fits, the accuracy, the sensitivity, and the precision. Answer the following questions:\n",
    "\n",
    "If the bank just wants to have the most accurate predictions, which hyperparameter optimization approach would they choose?\n",
    "\n",
    "If the bank isn't as concerned about misclassifying some truly good loans as they are interested in correctly predicting truly bad loans.  Which model should they use?  Why?\n",
    "\n",
    "Why did TPOT fail to find the best hyperparameters? Why did grid search fail to find parameters at least as good as the default?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font color=\"green\">Be sure to answer each question and don't forget your results dataframe screenshot.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}