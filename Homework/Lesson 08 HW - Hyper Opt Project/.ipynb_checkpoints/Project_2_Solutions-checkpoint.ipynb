{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project you're going to apply hyperparameter optimization to both a regression and a classification problem. It looks like a lot to do below, but it's mostly a matter of modifying code from the presentation. \n",
    "\n",
    "## Objective\n",
    "\n",
    "For each of the models in problems 1 and 2 below, apply the following 4 tuning methods from the presentation: GridSearchCV, RandomSearchCV, BayesianOptimization, and TPOT.\n",
    "* **For TPOT**: In Problem 1 do only hyperparameter optimization. In Problem 2 do **both** hyperparameter optimization and also run TPOT and let it choose the model. See the presentation for examples of both.\n",
    "\n",
    "### What to submit\n",
    "\n",
    "For each problem you need to include the following:\n",
    "\n",
    "1. A pandas table that reports:\n",
    "    * The best parameters for each tuning method\n",
    "    * The optimized score from the test data\n",
    "    * The number of model fits used in the optimization\n",
    "2. A brief discussion about which hyperparameter optimization approach worked best\n",
    "\n",
    "### Notes:\n",
    "* **For problem 1**: your pandas table should include the best parameters for each of the 4 tuning methods above.\n",
    "* **For problem 2**: your pandas table should include the best parameters for each of the 5 tuning methods (the 4 methods above and the TPOT model search).\n",
    "* **For GridSearchCV**: you should include at least 2 or 3 values for each hyperparameter and one of those values should be the default.\n",
    "* **For BayesianOptimization**: you'll have to use `int()` or `bool()` to cast the float values of the hyperparameters inside your `cv_score()` function.\n",
    "* **For TPOT**: you should use a finer grid than for GridSearchCV, but not more than 10 to 20 possible values for each hyperparameter.  You could lower the number of possible values to keep the search space smaller.\n",
    "    * If your code is too slow you can reduce the number of cross-validation folds to 3 and if your dataset is really large you can randomly choose a smaller subset of the rows.\n",
    "* Use section headers to label your work.  Your summary / discussion should be more than simply \"XYZ is the best model\", but it also shouldn't be more than a few paragraphs and a table.\n",
    "\n",
    "\n",
    "### Regarding data\n",
    "\n",
    "* You can use either the specified dataset or you can choose your own.  \n",
    "    * If you use your own data it should have at least 500 rows and 10 features.  \n",
    "    * If your data has categorical features you'll need \"one hot\" encode it (convert categorical features into multiple binary features).  <a href=\"https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/\">Here is a nice tutorial</a>.  For categories with only two values you can remove one of the two hot encoded columns.\n",
    "* If you do want to use your own data, we suggest first getting things working with the suggested datasets.  Finding, cleaning, and preparing data can take a lot of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - Optimize Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find optimized hyperparameters for a random forest regression model. \n",
    "\n",
    "You may use either the diabetes data used in the presentation or a dataset that you choose.  **You do not need to include the TPOT general search for this problem** (use TPOT to optimize RandomForestRegressor, but don't run TOPT to choose a model). Here are ranges for a subset of the hyperparameters:\n",
    "\n",
    "Hyperparameter |Type | Default Value | Typical Range\n",
    "---- | ---- | ---- | ----\n",
    "n_estimators | discrete / integer | 100 | 10 to 150\n",
    "max_features | continuous / float | 1.0 | 0.05 to 1.0\n",
    "min_samples_split | discrete / integer | 2 | 2 to 20\n",
    "min_samples_leaf | discrete / integer | 1 | 1 to 20\n",
    "bootstrap | discrete / boolean | True | True, False\n",
    "\n",
    "\n",
    "You can add other hyperparameters to the optimization if you wish.\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\">Documentation for sklearn RandomForestRegressor</a>\n",
    "\n",
    "<font color = \"blue\"> *** 15 points: </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "#split into x and y\n",
    "import numpy as np\n",
    "X = np.array(diabetes.data)\n",
    "y = np.array(diabetes.target)\n",
    "\n",
    "#split into train (80%) and test (20%) sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to easily assess different models\n",
    "def my_regression_results(model):\n",
    "    score_test = model.score(X_test,y_test)\n",
    "    print('Model r-squared score from test data: {:0.4f}'.format(score_test))\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    %matplotlib inline\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(y_test,y_pred,'k.')\n",
    "    plt.xlabel('Test Values')\n",
    "    plt.ylabel('Predicted Values');\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    mse = mean_squared_error(y_test,y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print('Mean squared error on test data: {:0.2f}'.format(mse))\n",
    "    print('Root mean squared error on test data: {:0.2f}'.format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model r-squared score from test data: 0.5039\n",
      "Mean squared error on test data: 3125.67\n",
      "Root mean squared error on test data: 55.91\n"
     ]
    },
    {
     "data": {
      "image/png": "b87a265f6ef345265ac68e64333ab1e4787c0318",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 261,
       "width": 397
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Random Forest Regression Model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_model = RandomForestRegressor(random_state=0)\n",
    "rf_model.fit(X_train,y_train)\n",
    "\n",
    "#baseline\n",
    "my_regression_results(rf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 810 out of 810 | elapsed:  2.2min finished\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=RandomForestRegressor(bootstrap=True, criterion='mse',\n",
       "                                             max_depth=None,\n",
       "                                             max_features='auto',\n",
       "                                             max_leaf_nodes=None,\n",
       "                                             min_impurity_decrease=0.0,\n",
       "                                             min_impurity_split=None,\n",
       "                                             min_samples_leaf=1,\n",
       "                                             min_samples_split=2,\n",
       "                                             min_weight_fraction_leaf=0.0,\n",
       "                                             n_estimators=10, n_jobs=None,\n",
       "                                             oob_score=False, random_state=0,\n",
       "                                             verbose=0, warm_start=False),\n",
       "             iid='warn', n_jobs=1,\n",
       "             param_grid={'bootstrap': [True, False],\n",
       "                         'max_features': [0.33, 0.66, 1.0],\n",
       "                         'min_samples_leaf': [1, 5, 10],\n",
       "                         'min_samples_split': [2, 8, 14],\n",
       "                         'n_estimators': [50, 100, 150]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define the grid\n",
    "params = {\n",
    "    \"n_estimators\": [50, 100, 150],\n",
    "    \"max_features\": [0.33, 0.66, 1.0],\n",
    "    \"min_samples_split\": [2, 8, 14],\n",
    "    \"min_samples_leaf\": [1, 5, 10],\n",
    "    \"bootstrap\": [True, False]\n",
    "}\n",
    "\n",
    "# setup the grid search\n",
    "grid_search = GridSearchCV(rf_model,\n",
    "                           param_grid=params,\n",
    "                           cv=5,\n",
    "                           verbose=1,\n",
    "                           n_jobs=1,\n",
    "                           return_train_score=True)\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'max_features': 0.66,\n",
       " 'min_samples_leaf': 10,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 150}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best hyperparameter values\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model r-squared score from test data: 0.5601\n",
      "Mean squared error on test data: 2771.56\n",
      "Root mean squared error on test data: 52.65\n"
     ]
    },
    {
     "data": {
      "image/png": "515d61fffc6f84d1e96243b1b1e8bf533d327c02",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 261,
       "width": 397
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#apply model to test data\n",
    "my_regression_results(grid_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 125 out of 125 | elapsed:   15.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "                   estimator=RandomForestRegressor(bootstrap=True,\n",
       "                                                   criterion='mse',\n",
       "                                                   max_depth=None,\n",
       "                                                   max_features='auto',\n",
       "                                                   max_leaf_nodes=None,\n",
       "                                                   min_impurity_decrease=0.0,\n",
       "                                                   min_impurity_split=None,\n",
       "                                                   min_samples_leaf=1,\n",
       "                                                   min_samples_split=2,\n",
       "                                                   min_weight_fraction_leaf=0.0,\n",
       "                                                   n_estimators=10, n_jobs=None,\n",
       "                                                   oob_score=False,\n",
       "                                                   random_state=0...\n",
       "                                        'min_samples_leaf': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7ffa41638710>,\n",
       "                                        'min_samples_split': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7ffa416384a8>,\n",
       "                                        'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7ffa41638240>},\n",
       "                   pre_dispatch='2*n_jobs', random_state=8675309, refit=True,\n",
       "                   return_train_score=True, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "params = {\n",
    "    \"n_estimators\": randint(10, 150),\n",
    "    \"max_features\": uniform(0.05, 0.95),\n",
    "    \"min_samples_split\": randint(2, 20),\n",
    "    \"min_samples_leaf\": randint(1, 20),\n",
    "    \"bootstrap\": [True, False]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf_model,\n",
    "    param_distributions=params,\n",
    "    random_state=8675309,\n",
    "    n_iter=25,\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=1,\n",
    "    return_train_score=True)\n",
    "\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'max_features': 0.840654300939659,\n",
       " 'min_samples_leaf': 8,\n",
       " 'min_samples_split': 7,\n",
       " 'n_estimators': 98}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best hyperparameter values\n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model r-squared score from test data: 0.5668\n",
      "Mean squared error on test data: 2729.53\n",
      "Root mean squared error on test data: 52.24\n"
     ]
    },
    {
     "data": {
      "image/png": "7f25fe9c4f552b02491a4d012f3975a16e21cff0",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 261,
       "width": 397
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#apply model to test data\n",
    "my_regression_results(random_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num acquisition: 1, time elapsed: 1.65s\n",
      "num acquisition: 2, time elapsed: 6.29s\n",
      "num acquisition: 3, time elapsed: 8.29s\n",
      "num acquisition: 4, time elapsed: 9.48s\n",
      "num acquisition: 5, time elapsed: 11.08s\n",
      "num acquisition: 6, time elapsed: 12.18s\n",
      "num acquisition: 7, time elapsed: 13.97s\n",
      "num acquisition: 8, time elapsed: 14.57s\n",
      "num acquisition: 9, time elapsed: 16.05s\n",
      "num acquisition: 10, time elapsed: 18.84s\n",
      "num acquisition: 11, time elapsed: 21.59s\n",
      "num acquisition: 12, time elapsed: 22.86s\n",
      "num acquisition: 13, time elapsed: 24.95s\n",
      "num acquisition: 14, time elapsed: 26.05s\n",
      "num acquisition: 15, time elapsed: 28.22s\n",
      "num acquisition: 16, time elapsed: 29.98s\n",
      "num acquisition: 17, time elapsed: 32.90s\n",
      "num acquisition: 18, time elapsed: 35.15s\n",
      "num acquisition: 19, time elapsed: 37.30s\n",
      "num acquisition: 20, time elapsed: 39.32s\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(8675309)\n",
    "from GPyOpt.methods import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "hp_bounds = [{\n",
    "    'name': 'n_estimators',\n",
    "    'type': 'discrete',\n",
    "    'domain': (10, 150)\n",
    "}, {\n",
    "    'name': 'max_features',\n",
    "    'type': 'continuous',\n",
    "    'domain': (0.05, 1.0)\n",
    "}, {\n",
    "    'name': 'min_samples_split',\n",
    "    'type': 'discrete',\n",
    "    'domain': (2, 20)\n",
    "}, {\n",
    "    'name': 'min_samples_leaf',\n",
    "    'type': 'discrete',\n",
    "    'domain': (1, 20)\n",
    "}, {\n",
    "    'name': 'bootstrap',\n",
    "    'type': 'discrete',\n",
    "    'domain': (0, 1)\n",
    "}]\n",
    "\n",
    "\n",
    "# Optimization objective\n",
    "def cv_score(hyp_parameters):\n",
    "    hyp_parameters = hyp_parameters[0]\n",
    "    rf_model = RandomForestRegressor(random_state=0,\n",
    "                                 n_estimators=int(hyp_parameters[0]),\n",
    "                                 max_features=hyp_parameters[1],\n",
    "                                 min_samples_split=int(hyp_parameters[2]),\n",
    "                                 min_samples_leaf=int(hyp_parameters[3]),\n",
    "                                 bootstrap=bool(hyp_parameters[4]))\n",
    "    scores = cross_val_score(rf_model,\n",
    "                             X=X_train,\n",
    "                             y=y_train,\n",
    "                             cv=KFold(n_splits=5))\n",
    "    return np.array(scores.mean())  # return average of 5-fold scores\n",
    "\n",
    "\n",
    "optimizer = BayesianOptimization(f=cv_score,\n",
    "                                 domain=hp_bounds,\n",
    "                                 model_type='GP',\n",
    "                                 acquisition_type='EI',\n",
    "                                 acquisition_jitter=0.05,\n",
    "                                 exact_feval=True,\n",
    "                                 maximize=True,\n",
    "                                 verbosity=True)\n",
    "\n",
    "optimizer.run_optimization(max_iter=20,verbosity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 150,\n",
       " 'max_features': 0.7034719511528766,\n",
       " 'min_samples_split': 20,\n",
       " 'min_samples_leaf': 1,\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best hyperparameter values\n",
    "best_hyp_set = {}\n",
    "for i in range(len(hp_bounds)):\n",
    "    if hp_bounds[i]['type'] == 'continuous':\n",
    "        best_hyp_set[hp_bounds[i]['name']] = optimizer.x_opt[i]\n",
    "    elif hp_bounds[i]['name'] == 'bootstrap':\n",
    "        best_hyp_set[hp_bounds[i]['name']] = bool(optimizer.x_opt[i])\n",
    "    else:\n",
    "        best_hyp_set[hp_bounds[i]['name']] = int(optimizer.x_opt[i])\n",
    "best_hyp_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "                      max_features=0.7034719511528766, max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=20,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=150,\n",
       "                      n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                      warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create instance of model and train it\n",
    "bayopt_search = RandomForestRegressor(random_state=0,**best_hyp_set)\n",
    "bayopt_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model r-squared score from test data: 0.5539\n",
      "Mean squared error on test data: 2810.42\n",
      "Root mean squared error on test data: 53.01\n"
     ]
    },
    {
     "data": {
      "image/png": "b716fed74010fac1fb3a81399efd43eaf2f9a1ee",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 263,
       "width": 397
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#apply model to test data\n",
    "my_regression_results(bayopt_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=120, style=ProgressStyle(descriptâ€¦"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.4331236662876766\n",
      "Generation 2 - Current best internal CV score: 0.435774258957007\n",
      "Generation 3 - Current best internal CV score: 0.4362182878178885\n",
      "Generation 4 - Current best internal CV score: 0.4362182878178885\n",
      "Generation 5 - Current best internal CV score: 0.4362182878178885\n",
      "\n",
      "Best pipeline: RandomForestRegressor(CombineDFs(input_matrix, input_matrix), bootstrap=1, max_features=0.75, min_samples_leaf=11, min_samples_split=18, n_estimators=30)\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTRegressor\n",
    "\n",
    "tpot_config = {\n",
    "    'sklearn.ensemble.RandomForestRegressor': {\n",
    "        'n_estimators': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150],\n",
    "        'max_features': [0.05, 0.25, 0.5, 0.75, 1.0],\n",
    "        'min_samples_split': range(2, 21),\n",
    "        'min_samples_leaf': range(1, 21),\n",
    "        'bootstrap': [0, 1]\n",
    "    }\n",
    "}\n",
    "\n",
    "tpot = TPOTRegressor(generations=5,\n",
    "                     population_size=20,\n",
    "                     verbosity=2,\n",
    "                     config_dict=tpot_config,\n",
    "                     cv=5,\n",
    "                     scoring='r2',\n",
    "                     random_state=8675309)\n",
    "tpot.fit(X_train, y_train)\n",
    "tpot.export('tpot_RandomForestRegressor.py') # export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model r-squared score from test data: 0.5633\n",
      "Mean squared error on test data: 2751.56\n",
      "Root mean squared error on test data: 52.46\n"
     ]
    },
    {
     "data": {
      "image/png": "27e2dd687539337c7c568e7144a650db5d2eaee4",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 261,
       "width": 397
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#apply model to test data\n",
    "my_regression_results(tpot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Summary Table of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#code for summary table\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#best hyperparameters for tpot\n",
    "best_tpot = {'n_estimators': 30, 'max_features': 0.75, 'min_samples_split': 18, 'min_samples_leaf': 11, 'bootstrap': True}\n",
    "\n",
    "#lists of hyperparamters, tuning methods\n",
    "param = ['n_estimators', 'max_features', 'min_samples_split', 'min_samples_leaf', 'bootstrap']\n",
    "tuning = ['GridSearch', 'RandomizedSearch', 'BayesianOptimization', 'TPOT']\n",
    "\n",
    "#best hyperparameters by tuning method\n",
    "best_param = [grid_search.best_params_, random_search.best_params_, best_hyp_set, best_tpot]\n",
    "best_param_dict = dict(zip(tuning, best_param))\n",
    "\n",
    "#create dataframe\n",
    "problem1 = pd.DataFrame([[best_param_dict[t][p] for p in param] for t in tuning], columns = param, index=tuning)\n",
    "\n",
    "#add in R^2, MSE, number of model fits\n",
    "problem1['R^2'] = [grid_search.score(X_test,y_test),random_search.score(X_test,y_test), bayopt_search.score(X_test,y_test),tpot.score(X_test,y_test)]\n",
    "problem1['MSE'] = [mean_squared_error(y_test,grid_search.predict(X_test)), mean_squared_error(y_test,random_search.predict(X_test)), mean_squared_error(y_test,bayopt_search.predict(X_test)), mean_squared_error(y_test,tpot.predict(X_test))]\n",
    "problem1['model_fits'] = [810, 125, 125, 600]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "<font color = \"blue\"> *** 5 points: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_features</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>bootstrap</th>\n",
       "      <th>R^2</th>\n",
       "      <th>MSE</th>\n",
       "      <th>model_fits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>GridSearch</td>\n",
       "      <td>150</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>0.560081</td>\n",
       "      <td>2771.562077</td>\n",
       "      <td>810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>RandomizedSearch</td>\n",
       "      <td>98</td>\n",
       "      <td>0.840654</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>0.566752</td>\n",
       "      <td>2729.532660</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>BayesianOptimization</td>\n",
       "      <td>150</td>\n",
       "      <td>0.703472</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.553913</td>\n",
       "      <td>2810.422879</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>TPOT</td>\n",
       "      <td>30</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "      <td>0.563257</td>\n",
       "      <td>2751.555380</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      n_estimators  max_features  min_samples_split  \\\n",
       "GridSearch                     150      0.660000                  2   \n",
       "RandomizedSearch                98      0.840654                  7   \n",
       "BayesianOptimization           150      0.703472                 20   \n",
       "TPOT                            30      0.750000                 18   \n",
       "\n",
       "                      min_samples_leaf  bootstrap       R^2          MSE  \\\n",
       "GridSearch                          10       True  0.560081  2771.562077   \n",
       "RandomizedSearch                     8       True  0.566752  2729.532660   \n",
       "BayesianOptimization                 1       True  0.553913  2810.422879   \n",
       "TPOT                                11       True  0.563257  2751.555380   \n",
       "\n",
       "                      model_fits  \n",
       "GridSearch                   810  \n",
       "RandomizedSearch             125  \n",
       "BayesianOptimization         125  \n",
       "TPOT                         600  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the four tuning methods are summarized in the table above: it lists the bests hyperparameter values, the $R^2$ and MSE scores, and the number of model fits. In this case, the best tuning method is the randomized search. It has the highest $R^2$ value (and lowest MSE), although there isn't a huge difference in the scores between the four methods. The randomized search was also more computationally efficient as it had far fewer model fits than the grid search or TPOT tuning methods. However, changing the random seed value, including different hyperparameter values to test, or increasing the number of model fits could potentially change the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - Optimize XGBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find optimized hyperparameters for an xgboost classifier model. \n",
    "\n",
    "This problem contains 5 parts.\n",
    "\n",
    "\n",
    "### Notes:\n",
    "\n",
    "#### About the data\n",
    "The first cell below loads a subset of the loans default data from DS705 and your job is to predict whether a loan defaults or not.  The `status_bad` column is the target column and a 1 indicates a loan that defaulted.  We have selected a subset of the original data that includes 2000 each of good and bad loans.  The data has already been cleaned and encoded.  You're welcome to look into a different dataset, but start by getting this working and then add your own data.\n",
    "\n",
    "#### This is classification, not regression\n",
    "The score for each model will be accuracy and not MSE.  Your summary table should include accuracy, sensitivity, and precision for each optimized model applied to the test data.  (<a href=\"https://classeval.wordpress.com/introduction/basic-evaluation-measures/\">Here is a nice overview of metrics for binary classification data</a>) that includes definitions of accuracy and such.\n",
    "\n",
    "For the models you'll mostly just need to change 'regressor' to 'classifier', e.g. `XGBClassifier` instead of `XGBRegressor`.\n",
    "\n",
    "\n",
    "Hyperparameter | Type | Default Value | Typical Range\n",
    "---- | ---- | ---- | ----\n",
    "n_estimators | discrete / integer | 100 | 50 to 150\n",
    "max_depth | discrete / integer | 3| 1 to 10\n",
    "min_child_weight | discrete / integer | 1 | 1 to 20\n",
    "learning_rate | continuous / float | 0.1 | 0.001 to 1\n",
    "sub_sample | continuous / float | 1 | 0.05 to 1\n",
    "reg_lambda | continuous / float | 1 | 0 to 5\n",
    "reg_alpha  | continuous / float | 0 | 0 to 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Part 1: Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Do not change this cell for loading and preparing the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X = pd.read_csv('./data/loans_subset.csv')\n",
    "\n",
    "# split into predictors and target\n",
    "# convert to numpy arrays for xgboost, OK for other models too\n",
    "y = np.array(X['status_Bad']) # 1 for bad loan, 0 for good loan\n",
    "X = np.array(X.drop(columns = ['status_Bad']))\n",
    "\n",
    "# split into test and training data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Part 2\n",
    "\n",
    "Write a function called `my_classifier_results` modeled after `my_regression_results` that applies a model to the test data and prints out the accuracy, sensitivity, precision, and the confusion matrix.  There is no need to make a plot.\n",
    "\n",
    "<font color = \"blue\"> *** 5 points - (don't delete this cell) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# check the code in Part 3 to see how to get the confusion matrix to help you write your function\n",
    "def my_classifier_results(model):\n",
    "    score_test = model.score(X_test,y_test)\n",
    "    print('Model accuracy score from test data: {:0.4f}'.format(score_test))\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import pandas as pd\n",
    "    cmtx = pd.DataFrame(confusion_matrix(y_test, y_pred, labels=[1,0]), index=['true:bad', 'true:good'], columns=['pred:bad','pred:good'])\n",
    "    display(cmtx)\n",
    "    \n",
    "    #correctly predicted bad loans/all actually bad loans\n",
    "    sensitivity = cmtx.iloc[0,0]/(cmtx.iloc[0,0]+cmtx.iloc[0,1])\n",
    "    #correctly predicted bad loans/all predicted bad\n",
    "    precision = cmtx.iloc[0,0]/(cmtx.iloc[0,0]+cmtx.iloc[1,0])\n",
    "    print('Model sensitivity score from test data: {:0.4f}'.format(sensitivity))\n",
    "    print('Model precision score from test data: {:0.4f}'.format(precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Part 3\n",
    "\n",
    "Start by training some baseline models using default values of the hyperparameters.  We've included logistic regression in a cell below to get you started.  Use `LogisticRegression`, `RandomForestClassifier`, and `GaussianNB` (Gaussian Naive Bayes) from `sklearn`.  Also use `XGBClassifier` from `xgboost` where you may need to include `objective=\"binary:logistic\"` as an option. The default scoring method for all of the `sklearn` classifiers is accuracy. Apply `my_classifier_results` to the test data for each model.\n",
    "\n",
    "<font color = \"blue\"> *** 10 points - (don't delete this cell) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5475\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred:bad</th>\n",
       "      <th>pred:good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>true:bad</td>\n",
       "      <td>126</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>true:good</td>\n",
       "      <td>110</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pred:bad  pred:good\n",
       "true:bad        126         71\n",
       "true:good       110         93"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using my function:\n",
      "\n",
      "Model accuracy score from test data: 0.5475\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred:bad</th>\n",
       "      <th>pred:good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>true:bad</td>\n",
       "      <td>126</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>true:good</td>\n",
       "      <td>110</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pred:bad  pred:good\n",
       "true:bad        126         71\n",
       "true:good       110         93"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sensitivity score from test data: 0.6396\n",
      "Model precision score from test data: 0.5339\n"
     ]
    }
   ],
   "source": [
    "# We've included this code to get you started\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# we do need to go higher than the default iterations for the solver to get convergence\n",
    "# and the explicity declaration of the solver avoids a warning message, otherwise\n",
    "# the parameters are defaults.\n",
    "logreg_model = LogisticRegression(solver='lbfgs',max_iter=1000)\n",
    "\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "# Use score method to get accuracy of model\n",
    "score = logreg_model.score(X_test, y_test) # this is accuracy\n",
    "print(score)\n",
    "\n",
    "# obtaining the confusion matrix and making it look nice\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "y_pred = logreg_model.predict(X_test)\n",
    "\n",
    "# must put true before predictions in confusion matrix function\n",
    "cmtx = pd.DataFrame(\n",
    "    confusion_matrix(y_test, y_pred, labels=[1,0]), \n",
    "    index=['true:bad', 'true:good'], \n",
    "    columns=['pred:bad','pred:good']\n",
    ")\n",
    "display(cmtx)\n",
    "\n",
    "# using my function\n",
    "print(\"\\nUsing my function:\\n\")\n",
    "my_classifier_results(logreg_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy score from test data: 0.6000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred:bad</th>\n",
       "      <th>pred:good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>true:bad</td>\n",
       "      <td>100</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>true:good</td>\n",
       "      <td>63</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pred:bad  pred:good\n",
       "true:bad        100         97\n",
       "true:good        63        140"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sensitivity score from test data: 0.5076\n",
      "Model precision score from test data: 0.6135\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=0)\n",
    "rf_model.fit(X_train,y_train)\n",
    "\n",
    "my_classifier_results(rf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy score from test data: 0.5600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred:bad</th>\n",
       "      <th>pred:good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>true:bad</td>\n",
       "      <td>160</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>true:good</td>\n",
       "      <td>139</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pred:bad  pred:good\n",
       "true:bad        160         37\n",
       "true:good       139         64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sensitivity score from test data: 0.8122\n",
      "Model precision score from test data: 0.5351\n"
     ]
    }
   ],
   "source": [
    "#Gaussian Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X_train,y_train)\n",
    "\n",
    "my_classifier_results(gnb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy score from test data: 0.6625\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred:bad</th>\n",
       "      <th>pred:good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>true:bad</td>\n",
       "      <td>132</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>true:good</td>\n",
       "      <td>70</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pred:bad  pred:good\n",
       "true:bad        132         65\n",
       "true:good        70        133"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sensitivity score from test data: 0.6701\n",
      "Model precision score from test data: 0.6535\n"
     ]
    }
   ],
   "source": [
    "#XGBoost Classifier\n",
    "import xgboost as xgb\n",
    "\n",
    "xgbr_model = xgb.XGBClassifier(objective ='binary:logistic')\n",
    "xgbr_model.fit(X_train,y_train)\n",
    "\n",
    "my_classifier_results(xgbr_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4\n",
    "\n",
    "Now use the four hyperparameter optimization techniques on `XGBClassifier` and TPOT general model optimization.  Apply `my_classifer_results` to the test data in each case.\n",
    "* Feel free to use 3 folds instead of 5 for cross validation to speed things up. \n",
    "* Choose a very small number of iterations, population size, etc. until you're sure things are working correctly, then turn up the numbers.  General TPOT optimization will take a while (fair warning: it took about 30 minutes on my Macbook Pro with generations = 10, population_size=40, and cv=5)  \n",
    "* The hyperparameters to consider for are the same as they were in the presentation , but here they are again for convenience:\n",
    "\n",
    "<font color = \"blue\"> *** 10 points - (don't delete this cell) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1440 out of 1440 | elapsed: 10.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                     colsample_bylevel=1, colsample_bynode=1,\n",
       "                                     colsample_bytree=1, gamma=0,\n",
       "                                     learning_rate=0.1, max_delta_step=0,\n",
       "                                     max_depth=3, min_child_weight=1,\n",
       "                                     missing=None, n_estimators=100, n_jobs=1,\n",
       "                                     nthread=None, objective='binary:logistic',\n",
       "                                     random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                                     scale_pos_weight=1, seed=None, silent=None,\n",
       "                                     subsample=1, verbosity=1),\n",
       "             iid='warn', n_jobs=1,\n",
       "             param_grid={'learning_rate': [0.01, 0.1], 'max_depth': [2, 3, 6],\n",
       "                         'min_child_weight': [1, 3], 'n_estimators': [10, 100],\n",
       "                         'reg_alpha': [0, 1, 3], 'reg_lambda': [1, 3],\n",
       "                         'subsample': [0.8, 1]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the grid\n",
    "params = {\n",
    "    \"learning_rate\": [0.01, 0.1],\n",
    "    \"max_depth\": [2, 3, 6],\n",
    "    \"n_estimators\": [10, 100],\n",
    "    \"subsample\": [0.8, 1],\n",
    "    \"min_child_weight\": [1, 3],\n",
    "    \"reg_lambda\": [1, 3],\n",
    "    \"reg_alpha\": [0, 1, 3]\n",
    "}\n",
    "\n",
    "# setup the grid search\n",
    "grid_search = GridSearchCV(xgbr_model,\n",
    "                           param_grid=params,\n",
    "                           cv=5,\n",
    "                           verbose=1,\n",
    "                           n_jobs=1,\n",
    "                           return_train_score=True)\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1,\n",
       " 'max_depth': 3,\n",
       " 'min_child_weight': 1,\n",
       " 'n_estimators': 100,\n",
       " 'reg_alpha': 1,\n",
       " 'reg_lambda': 3,\n",
       " 'subsample': 0.8}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best hyperparameter values\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy score from test data: 0.6350\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred:bad</th>\n",
       "      <th>pred:good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>true:bad</td>\n",
       "      <td>126</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>true:good</td>\n",
       "      <td>75</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pred:bad  pred:good\n",
       "true:bad        126         71\n",
       "true:good        75        128"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sensitivity score from test data: 0.6396\n",
      "Model precision score from test data: 0.6269\n"
     ]
    }
   ],
   "source": [
    "#apply model to test data\n",
    "my_classifier_results(grid_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 125 out of 125 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "                   estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                           colsample_bylevel=1,\n",
       "                                           colsample_bynode=1,\n",
       "                                           colsample_bytree=1, gamma=0,\n",
       "                                           learning_rate=0.1, max_delta_step=0,\n",
       "                                           max_depth=3, min_child_weight=1,\n",
       "                                           missing=None, n_estimators=100,\n",
       "                                           n_jobs=1, nthread=None,\n",
       "                                           objective='binary:logistic',\n",
       "                                           random_state=0, reg_alpha=0...\n",
       "                                        'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7ffa33c3f978>,\n",
       "                                        'reg_alpha': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7ffa33c3fe48>,\n",
       "                                        'reg_lambda': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7ffa33b2f6a0>,\n",
       "                                        'subsample': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7ffa33c3fac8>},\n",
       "                   pre_dispatch='2*n_jobs', random_state=8675309, refit=True,\n",
       "                   return_train_score=True, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    \"learning_rate\": [0.001, 0.01, 0.1, 0.5, 1.],\n",
    "    \"max_depth\": randint(1, 10),\n",
    "    \"n_estimators\": randint(50, 150),\n",
    "    \"subsample\": uniform(0.05, 0.95),  # so uniform on [.05,.05+.95] = [.05,1.]\n",
    "    \"min_child_weight\": randint(1, 20),\n",
    "    \"reg_alpha\": uniform(0, 5),\n",
    "    \"reg_lambda\": uniform(0, 5)\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgbr_model,\n",
    "    param_distributions=params,\n",
    "    random_state=8675309,\n",
    "    n_iter=25,\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=1,\n",
    "    return_train_score=True)\n",
    "\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.01,\n",
       " 'max_depth': 4,\n",
       " 'min_child_weight': 8,\n",
       " 'n_estimators': 133,\n",
       " 'reg_alpha': 3.6150505500896517,\n",
       " 'reg_lambda': 0.7229439539293253,\n",
       " 'subsample': 0.11363586446628897}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best hyperparameter values\n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy score from test data: 0.6600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred:bad</th>\n",
       "      <th>pred:good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>true:bad</td>\n",
       "      <td>134</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>true:good</td>\n",
       "      <td>73</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pred:bad  pred:good\n",
       "true:bad        134         63\n",
       "true:good        73        130"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sensitivity score from test data: 0.6802\n",
      "Model precision score from test data: 0.6473\n"
     ]
    }
   ],
   "source": [
    "#apply model to test data\n",
    "my_classifier_results(random_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num acquisition: 1, time elapsed: 1.11s\n",
      "num acquisition: 2, time elapsed: 2.06s\n",
      "num acquisition: 3, time elapsed: 4.31s\n",
      "num acquisition: 4, time elapsed: 11.88s\n",
      "num acquisition: 5, time elapsed: 13.61s\n",
      "num acquisition: 6, time elapsed: 16.24s\n",
      "num acquisition: 7, time elapsed: 18.83s\n",
      "num acquisition: 8, time elapsed: 19.72s\n",
      "num acquisition: 9, time elapsed: 21.51s\n",
      "num acquisition: 10, time elapsed: 22.74s\n",
      "num acquisition: 11, time elapsed: 23.71s\n",
      "num acquisition: 12, time elapsed: 24.62s\n",
      "num acquisition: 13, time elapsed: 26.51s\n",
      "num acquisition: 14, time elapsed: 28.23s\n",
      "num acquisition: 15, time elapsed: 30.52s\n",
      "num acquisition: 16, time elapsed: 33.00s\n",
      "num acquisition: 17, time elapsed: 34.66s\n",
      "num acquisition: 18, time elapsed: 36.63s\n",
      "num acquisition: 19, time elapsed: 38.29s\n",
      "num acquisition: 20, time elapsed: 52.25s\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(8675309)\n",
    "from GPyOpt.methods import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "hp_bounds = [{\n",
    "    'name': 'learning_rate',\n",
    "    'type': 'continuous',\n",
    "    'domain': (0.001, 1.0)\n",
    "}, {\n",
    "    'name': 'max_depth',\n",
    "    'type': 'discrete',\n",
    "    'domain': (1, 10)\n",
    "}, {\n",
    "    'name': 'n_estimators',\n",
    "    'type': 'discrete',\n",
    "    'domain': (50, 150)\n",
    "}, {\n",
    "    'name': 'subsample',\n",
    "    'type': 'continuous',\n",
    "    'domain': (0.05, 1.0)\n",
    "}, {\n",
    "    'name': 'min_child_weight',\n",
    "    'type': 'discrete',\n",
    "    'domain': (1, 20)\n",
    "}, {\n",
    "    'name': 'reg_alpha',\n",
    "    'type': 'continuous',\n",
    "    'domain': (0, 5)\n",
    "}, {\n",
    "    'name': 'reg_lambda',\n",
    "    'type': 'continuous',\n",
    "    'domain': (0, 5)\n",
    "}]\n",
    "\n",
    "\n",
    "# Optimization objective\n",
    "def cv_score(hyp_parameters):\n",
    "    hyp_parameters = hyp_parameters[0]\n",
    "    xgb_model = xgb.XGBClassifier(objective='binary:logistic',\n",
    "                                 learning_rate=hyp_parameters[0],\n",
    "                                 max_depth=int(hyp_parameters[1]),\n",
    "                                 n_estimators=int(hyp_parameters[2]),\n",
    "                                 subsample=hyp_parameters[3],\n",
    "                                 min_child_weight=int(hyp_parameters[4]),\n",
    "                                 reg_alpha=hyp_parameters[5],\n",
    "                                 reg_lambda=hyp_parameters[6])\n",
    "    scores = cross_val_score(xgb_model,\n",
    "                             X=X_train,\n",
    "                             y=y_train,\n",
    "                             cv=KFold(n_splits=5))\n",
    "    return np.array(scores.mean())  # return average of 5-fold scores\n",
    "\n",
    "\n",
    "optimizer = BayesianOptimization(f=cv_score,\n",
    "                                 domain=hp_bounds,\n",
    "                                 model_type='GP',\n",
    "                                 acquisition_type='EI',\n",
    "                                 acquisition_jitter=0.05,\n",
    "                                 exact_feval=True,\n",
    "                                 maximize=True,\n",
    "                                 verbosity=True)\n",
    "\n",
    "optimizer.run_optimization(max_iter=20,verbosity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.35515841209430643,\n",
       " 'max_depth': 1,\n",
       " 'n_estimators': 50,\n",
       " 'subsample': 0.35874249133049757,\n",
       " 'min_child_weight': 1,\n",
       " 'reg_alpha': 4.911647022562429,\n",
       " 'reg_lambda': 0.41835225577303237}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best hyperparamter values\n",
    "best_hyp_set = {}\n",
    "for i in range(len(hp_bounds)):\n",
    "    if hp_bounds[i]['type'] == 'continuous':\n",
    "        best_hyp_set[hp_bounds[i]['name']] = optimizer.x_opt[i]\n",
    "    else:\n",
    "        best_hyp_set[hp_bounds[i]['name']] = int(optimizer.x_opt[i])\n",
    "best_hyp_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.35515841209430643, max_delta_step=0, max_depth=1,\n",
       "              min_child_weight=1, missing=None, n_estimators=50, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=4.911647022562429, reg_lambda=0.41835225577303237,\n",
       "              scale_pos_weight=1, seed=None, silent=None,\n",
       "              subsample=0.35874249133049757, verbosity=1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create instance of model and train it\n",
    "bayopt_search = xgb.XGBClassifier(objective='binary:logistic',**best_hyp_set)\n",
    "bayopt_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy score from test data: 0.6300\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred:bad</th>\n",
       "      <th>pred:good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>true:bad</td>\n",
       "      <td>120</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>true:good</td>\n",
       "      <td>71</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pred:bad  pred:good\n",
       "true:bad        120         77\n",
       "true:good        71        132"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sensitivity score from test data: 0.6091\n",
      "Model precision score from test data: 0.6283\n"
     ]
    }
   ],
   "source": [
    "#apply model to test data\n",
    "my_classifier_results(bayopt_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genetic Algorithm from TPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=120, style=ProgressStyle(descriptâ€¦"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.6605449931209144\n",
      "Generation 2 - Current best internal CV score: 0.6605449931209144\n",
      "Generation 3 - Current best internal CV score: 0.6605449931209144\n",
      "Generation 4 - Current best internal CV score: 0.6605449931209144\n",
      "Generation 5 - Current best internal CV score: 0.6605527145435595\n",
      "\n",
      "Best pipeline: XGBClassifier(input_matrix, learning_rate=0.1, max_depth=1, min_child_weight=11, n_estimators=100, nthread=1, objective=binary:logistic, reg_alpha=5, reg_lambda=2, subsample=0.15000000000000002)\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTClassifier\n",
    "\n",
    "tpot_config = {\n",
    "    'xgboost.XGBClassifier': {\n",
    "        'n_estimators': [100],\n",
    "        'max_depth': range(1, 11),\n",
    "        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],\n",
    "        'subsample': np.arange(0.05, 1.01, 0.05),\n",
    "        'min_child_weight': range(1, 21),\n",
    "        'reg_alpha': range(1, 6),\n",
    "        'reg_lambda': range(1, 6),\n",
    "        'nthread': [1],\n",
    "        'objective': ['binary:logistic']\n",
    "    }\n",
    "}\n",
    "\n",
    "tpot = TPOTClassifier(generations=5,\n",
    "                      population_size=20,\n",
    "                      verbosity=2,\n",
    "                      config_dict=tpot_config,\n",
    "                      cv=5,\n",
    "                      scoring='accuracy',\n",
    "                      random_state=8675309)\n",
    "tpot.fit(X_train, y_train)\n",
    "tpot.export('tpot_XGBClassifier.py')  # export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy score from test data: 0.6475\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred:bad</th>\n",
       "      <th>pred:good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>true:bad</td>\n",
       "      <td>128</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>true:good</td>\n",
       "      <td>72</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pred:bad  pred:good\n",
       "true:bad        128         69\n",
       "true:good        72        131"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sensitivity score from test data: 0.6497\n",
      "Model precision score from test data: 0.6400\n"
     ]
    }
   ],
   "source": [
    "#apply model to test data\n",
    "my_classifier_results(tpot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### AutoML with TPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=440, style=ProgressStyle(descriptâ€¦"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.6652672314182705\n",
      "Generation 2 - Current best internal CV score: 0.6672163087163019\n",
      "Generation 3 - Current best internal CV score: 0.668881435385142\n",
      "Generation 4 - Current best internal CV score: 0.668881435385142\n",
      "Generation 5 - Current best internal CV score: 0.668881435385142\n",
      "Generation 6 - Current best internal CV score: 0.6691576667350396\n",
      "Generation 7 - Current best internal CV score: 0.6691576667350396\n",
      "Generation 8 - Current best internal CV score: 0.6702660799028461\n",
      "Generation 9 - Current best internal CV score: 0.6702660799028461\n",
      "Generation 10 - Current best internal CV score: 0.6708251071514841\n",
      "\n",
      "Best pipeline: LogisticRegression(GaussianNB(MaxAbsScaler(MinMaxScaler(CombineDFs(input_matrix, input_matrix)))), C=1.0, dual=False, penalty=l1)\n",
      "0.6475\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTClassifier\n",
    "\n",
    "tpot_a = TPOTClassifier(generations=10,\n",
    "                        population_size=40,\n",
    "                        verbosity=2,\n",
    "                        cv=5,\n",
    "                        scoring='accuracy',\n",
    "                        random_state=8675309,\n",
    "                        n_jobs=-1,\n",
    "                        config_dict='TPOT light')\n",
    "tpot_a.fit(X_train, y_train)\n",
    "print(tpot_a.score(X_test, y_test))\n",
    "tpot_a.export('tpot_optimal_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#best model found from export 'tpot_optimal_pipeline.py'\n",
    "\n",
    "# Average CV score on the training set was:0.6708251071514841\n",
    "exported_pipeline = make_pipeline(\n",
    "    make_union(\n",
    "        FunctionTransformer(copy),\n",
    "        FunctionTransformer(copy)\n",
    "    ),\n",
    "    MinMaxScaler(),\n",
    "    MaxAbsScaler(),\n",
    "    StackingEstimator(estimator=GaussianNB()), #Gaussian Naive Bayes\n",
    "    LogisticRegression(C=1.0, dual=False, penalty=\"l1\") #Logistic Regression\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy score from test data: 0.6475\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred:bad</th>\n",
       "      <th>pred:good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>true:bad</td>\n",
       "      <td>129</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>true:good</td>\n",
       "      <td>73</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pred:bad  pred:good\n",
       "true:bad        129         68\n",
       "true:good        73        130"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sensitivity score from test data: 0.6548\n",
      "Model precision score from test data: 0.6386\n"
     ]
    }
   ],
   "source": [
    "#apply model to test data\n",
    "my_classifier_results(tpot_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Summary Table of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#code for summary table\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#best hyperparameters for tpot and tpot_auto\n",
    "best_tpot = {'n_estimators': 100, 'max_depth': 1, 'min_child_weight': 11, 'learning_rate': 0.1, 'subsample': 0.15, 'reg_lambda': 2, 'reg_alpha': 5}\n",
    "best_tpot_auto = {'n_estimators': \" \", 'max_depth': \" \", 'min_child_weight': \" \", 'learning_rate': \" \", 'subsample': \" \", 'reg_lambda': \" \", 'reg_alpha': \" \"}\n",
    "\n",
    "#lists of hyperparamters, tuning methods\n",
    "param = ['n_estimators', 'max_depth', 'min_child_weight', 'learning_rate', 'subsample', 'reg_lambda', 'reg_alpha']\n",
    "tuning = ['GridSearch', 'RandomizedSearch', 'BayesianOptimization', 'TPOT', 'Auto TPOT']\n",
    "\n",
    "#best hyperparameters by tuning method\n",
    "best_param = [grid_search.best_params_, random_search.best_params_, best_hyp_set, best_tpot, best_tpot_auto]\n",
    "best_param_dict = dict(zip(tuning, best_param))\n",
    "\n",
    "#create dataframe\n",
    "problem2 = pd.DataFrame([[best_param_dict[t][p] for p in param] for t in tuning], columns = param, index=tuning)\n",
    "\n",
    "#add in accuracy, sensitivy, precision, model_fits\n",
    "problem2['accuracy'] = [grid_search.score(X_test,y_test),random_search.score(X_test,y_test), bayopt_search.score(X_test,y_test),tpot.score(X_test,y_test),tpot_a.score(X_test,y_test)]\n",
    "problem2['sensitivity'] = [0.6396, 0.6802, 0.6091, 0.6497, 0.6548]\n",
    "problem2['precision'] = [0.6269, 0.6473, 0.6283, 0.6400, 0.6386]\n",
    "problem2['model_fits'] = [1440, 125, 125, 600, \"\"] #model fits not needed for Auto TPOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Part 5 - Summary\n",
    "\n",
    "* In addition to your summary table, answer:\n",
    "    * The bank isn't as concerned about misclassifying some truly good loans as they are interested in correctly predicting truly bad loans.  Which model should they use?  Why?\n",
    "\n",
    "<font color = \"blue\"> *** 5 points - (don't delete this cell) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>subsample</th>\n",
       "      <th>reg_lambda</th>\n",
       "      <th>reg_alpha</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>precision</th>\n",
       "      <th>model_fits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>GridSearch</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6350</td>\n",
       "      <td>0.6396</td>\n",
       "      <td>0.6269</td>\n",
       "      <td>1440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>RandomizedSearch</td>\n",
       "      <td>133</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.113636</td>\n",
       "      <td>0.722944</td>\n",
       "      <td>3.61505</td>\n",
       "      <td>0.6600</td>\n",
       "      <td>0.6802</td>\n",
       "      <td>0.6473</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>BayesianOptimization</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.355158</td>\n",
       "      <td>0.358742</td>\n",
       "      <td>0.418352</td>\n",
       "      <td>4.91165</td>\n",
       "      <td>0.6300</td>\n",
       "      <td>0.6091</td>\n",
       "      <td>0.6283</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>TPOT</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6475</td>\n",
       "      <td>0.6497</td>\n",
       "      <td>0.6400</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Auto TPOT</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.6475</td>\n",
       "      <td>0.6548</td>\n",
       "      <td>0.6386</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     n_estimators max_depth min_child_weight learning_rate  \\\n",
       "GridSearch                    100         3                1           0.1   \n",
       "RandomizedSearch              133         4                8          0.01   \n",
       "BayesianOptimization           50         1                1      0.355158   \n",
       "TPOT                          100         1               11           0.1   \n",
       "Auto TPOT                                                                    \n",
       "\n",
       "                     subsample reg_lambda reg_alpha  accuracy  sensitivity  \\\n",
       "GridSearch                 0.8          3         1    0.6350       0.6396   \n",
       "RandomizedSearch      0.113636   0.722944   3.61505    0.6600       0.6802   \n",
       "BayesianOptimization  0.358742   0.418352   4.91165    0.6300       0.6091   \n",
       "TPOT                      0.15          2         5    0.6475       0.6497   \n",
       "Auto TPOT                                              0.6475       0.6548   \n",
       "\n",
       "                      precision model_fits  \n",
       "GridSearch               0.6269       1440  \n",
       "RandomizedSearch         0.6473        125  \n",
       "BayesianOptimization     0.6283        125  \n",
       "TPOT                     0.6400        600  \n",
       "Auto TPOT                0.6386             "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem2\n",
    "# Average CV score on the training set was:0.6708251071514841\n",
    "exported_pipeline = make_pipeline(\n",
    "    make_union(\n",
    "        FunctionTransformer(copy),\n",
    "        FunctionTransformer(copy)\n",
    "    ),\n",
    "    MinMaxScaler(),\n",
    "    MaxAbsScaler(),\n",
    "    StackingEstimator(estimator=GaussianNB()),\n",
    "    LogisticRegression(C=1.0, dual=False, penalty=\"l1\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The results of the five tuning methods are summarized in the table above: it lists the bests hyperparameter values, the accuracy, sensitivity, and precision scores, and the number of model fits. For the Auto TPOT tuning method, the code for the best model found is above as well: it involves a Gaussian Naive Bayes and Logistic Regression.\n",
    "\n",
    "The best tuning method in terms of accuracy is the random search method. It was also more computationally efficient with far fewer model fits than the grid search and TPOT tuning methods. However, all of the tuning methods actually performed worse than the baseline method, which had an accuracy of 0.6625. This could be due to the random seed values chosen or overfitting occurring with the training data, causing the test accuracy to be lower.\n",
    "\n",
    "If the bank is more interested in correctly predicting truly bad loans than concerned about misclassifying some truly good loans, they should also use the random search method. It had the most correctly predicted bad loans, 134, with a reasonable amount of misclassified good loans, 73, especially when compared to the other methods involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
