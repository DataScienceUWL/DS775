{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project you're going to apply hyperparameter optimization to both a regression and a classification problem. It looks like a lot to do below, but it's mostly a matter of modifying code from the presentation. \n",
    "\n",
    "## Objective\n",
    "\n",
    "For each of the models in problems 1 and 2 below, apply the following 4 tuning methods from the presentation: GridSearchCV, RandomSearchCV, BayesianOptimization, and TPOT.\n",
    "* **For TPOT**: In Problem 1 do only hyperparameter optimization. In Problem 2 do **both** hyperparameter optimization and also run TPOT and let it choose the model. See the presentation for examples of both.\n",
    "\n",
    "### What to submit\n",
    "\n",
    "For each problem you need to include the following:\n",
    "\n",
    "1. A pandas table that reports:\n",
    "    * The best parameters for each tuning method\n",
    "    * The optimized score from the test data\n",
    "    * The number of model fits used in the optimization\n",
    "2. A brief discussion about which hyperparameter optimization approach worked best\n",
    "\n",
    "### Notes:\n",
    "* **For problem 1**: your pandas table should include the best parameters for each of the 4 tuning methods above.\n",
    "* **For problem 2**: your pandas table should include the best parameters for each of the 5 tuning methods (the 4 methods above and the TPOT model search).\n",
    "* **For GridSearchCV**: you should include at least 2 or 3 values for each hyperparameter and one of those values should be the default.\n",
    "* **For BayesianOptimization**: you'll have to use `int()` or `bool()` to cast the float values of the hyperparameters inside your `cv_score()` function.\n",
    "* **For TPOT**: you should use a finer grid than for GridSearchCV, but not more than 10 to 20 possible values for each hyperparameter.  You could lower the number of possible values to keep the search space smaller.\n",
    "    * If your code is too slow you can reduce the number of cross-validation folds to 3 and if your dataset is really large you can randomly choose a smaller subset of the rows.\n",
    "* Use section headers to label your work.  Your summary / discussion should be more than simply \"XYZ is the best model\", but it also shouldn't be more than a few paragraphs and a table.\n",
    "\n",
    "\n",
    "### Regarding data\n",
    "\n",
    "* You can use either the specified dataset or you can choose your own.  \n",
    "    * If you use your own data it should have at least 500 rows and 10 features.  \n",
    "    * If your data has categorical features you'll need \"one hot\" encode it (convert categorical features into multiple binary features).  <a href=\"https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/\">Here is a nice tutorial</a>.  For categories with only two values you can remove one of the two hot encoded columns.\n",
    "* If you do want to use your own data, we suggest first getting things working with the suggested datasets.  Finding, cleaning, and preparing data can take a lot of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - Optimize Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find optimized hyperparameters for a random forest regression model. \n",
    "\n",
    "You may use either the diabetes data used in the presentation or a dataset that you choose.  **You do not need to include the TPOT general search for this problem** (use TPOT to optimize RandomForestRegressor, but don't run TOPT to choose a model). Here are ranges for a subset of the hyperparameters:\n",
    "\n",
    "Hyperparameter |Type | Default Value | Typical Range\n",
    "---- | ---- | ---- | ----\n",
    "n_estimators | discrete / integer | 100 | 10 to 150\n",
    "max_features | continuous / float | 1.0 | 0.05 to 1.0\n",
    "min_samples_split | discrete / integer | 2 | 2 to 20\n",
    "min_samples_leaf | discrete / integer | 1 | 1 to 20\n",
    "bootstrap | discrete / boolean | True | True, False\n",
    "\n",
    "\n",
    "You can add other hyperparameters to the optimization if you wish.\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\">Documentation for sklearn RandomForestRegressor</a>\n",
    "\n",
    "<font color = \"blue\"> *** 15 points: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "                      max_features='auto', max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                      n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                      warm_start=False)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#REGRESSION PROBLEM\n",
    "\n",
    "#Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Loading and formatting the data\n",
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "X = np.array(diabetes.data)\n",
    "y = np.array(diabetes.target)\n",
    "\n",
    "#Setting up results df\n",
    "results = pd.DataFrame(None, index=[\"GridSearch\", \"RandomSearch\", \"BayesianOptimization\", \"tpot\"], columns=[\"bootstrap\",\"max_features\",\"min_samples_leaf\",\"min_samples_split\",\"n_estimators\",\n",
    "                                      \"Best R-Squared\",\"Best MSE\",\"Best Root MSE\",\"Number of fits\"])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=394) #my results will differ from lecture\n",
    "\n",
    "#creating the Random Forest Regression model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf_model = RandomForestRegressor(random_state=0)\n",
    "rf_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 480 candidates, totalling 2400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 2400 out of 2400 | elapsed:  4.0min finished\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=RandomForestRegressor(bootstrap=True, criterion='mse',\n",
       "                                             max_depth=None,\n",
       "                                             max_features='auto',\n",
       "                                             max_leaf_nodes=None,\n",
       "                                             min_impurity_decrease=0.0,\n",
       "                                             min_impurity_split=None,\n",
       "                                             min_samples_leaf=1,\n",
       "                                             min_samples_split=2,\n",
       "                                             min_weight_fraction_leaf=0.0,\n",
       "                                             n_estimators=10, n_jobs=None,\n",
       "                                             oob_score=False, random_state=0,\n",
       "                                             verbose=0, warm_start=False),\n",
       "             iid='warn', n_jobs=1,\n",
       "             param_grid={'bootstrap': [True, False],\n",
       "                         'max_features': [0.1, 0.5, 1],\n",
       "                         'min_samples_leaf': [1, 5, 10, 15],\n",
       "                         'min_samples_split': [2, 5, 10, 15],\n",
       "                         'n_estimators': [25, 50, 75, 100, 125]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#*******************GRID SEARCH********************\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define the grid of hyperparams\n",
    "params = {\n",
    "    \"n_estimators\": [25, 50, 75, 100, 125],\n",
    "    \"max_features\": [0.1, 0.5, 1],\n",
    "    \"min_samples_split\": [2, 5, 10, 15],\n",
    "    \"min_samples_leaf\": [1, 5, 10, 15],\n",
    "    \"bootstrap\": [True, False]\n",
    "}\n",
    "\n",
    "# setup the grid search\n",
    "grid_search = GridSearchCV(rf_model,\n",
    "                           param_grid=params,\n",
    "                           cv=5,\n",
    "                           verbose=1,\n",
    "                           n_jobs=1,\n",
    "                           return_train_score=True)\n",
    "\n",
    "#we fit to training data first\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the best hyperparams\n",
    "results.at[\"GridSearch\",\"bootstrap\":\"n_estimators\"] = grid_search.best_params_\n",
    "#save the number of model fits into the dataframe manually\n",
    "results.at[\"GridSearch\",\"Number of fits\"] = 5*3*4*4*2*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function definition\n",
    "def my_regression_results(model):\n",
    "    score_test = model.score(X_test,y_test)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    mse = mean_squared_error(y_test,y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    return [round(score_test,4), round(mse,4), round(rmse,4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply my_regression_results to test data\n",
    "stats = my_regression_results(grid_search)\n",
    "results.at[\"GridSearch\", \"Best R-Squared\"] = stats[0]\n",
    "results.at[\"GridSearch\", \"Best MSE\"] = stats[1]\n",
    "results.at[\"GridSearch\", \"Best Root MSE\"] = stats[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 125 out of 125 | elapsed:   14.4s finished\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "                   estimator=RandomForestRegressor(bootstrap=True,\n",
       "                                                   criterion='mse',\n",
       "                                                   max_depth=None,\n",
       "                                                   max_features='auto',\n",
       "                                                   max_leaf_nodes=None,\n",
       "                                                   min_impurity_decrease=0.0,\n",
       "                                                   min_impurity_split=None,\n",
       "                                                   min_samples_leaf=1,\n",
       "                                                   min_samples_split=2,\n",
       "                                                   min_weight_fraction_leaf=0.0,\n",
       "                                                   n_estimators=10, n_jobs=None,\n",
       "                                                   oob_score=False,\n",
       "                                                   random_state=0...\n",
       "                                        'min_samples_leaf': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fdc870cdd68>,\n",
       "                                        'min_samples_split': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fdc870cdb00>,\n",
       "                                        'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fdc870cddd8>},\n",
       "                   pre_dispatch='2*n_jobs', random_state=394, refit=True,\n",
       "                   return_train_score=True, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#*************RANDOM SEARCH*****************\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "params = {\n",
    "    \"n_estimators\": randint(10, 150),\n",
    "    \"max_features\": uniform(0.05, 0.95),\n",
    "    \"min_samples_split\": randint(2, 20),\n",
    "    \"min_samples_leaf\": randint(1, 20),\n",
    "    \"bootstrap\": [True, False]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf_model,\n",
    "    param_distributions=params,\n",
    "    random_state=394, # like setting the seed\n",
    "    n_iter=25, #just checking 25 randomly selected sets of hyperparameters\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=1,\n",
    "    return_train_score=True)\n",
    "\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the best hyperparams\n",
    "results.at[\"RandomSearch\",\"bootstrap\":\"n_estimators\"] = random_search.best_params_\n",
    "#save the number of model fits into the dataframe manually\n",
    "results.at[\"RandomSearch\",\"Number of fits\"] = 25*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply my_regression_results to test data\n",
    "stats = my_regression_results(random_search)\n",
    "results.at[\"RandomSearch\", \"Best R-Squared\"] = stats[0]\n",
    "results.at[\"RandomSearch\", \"Best MSE\"] = stats[1]\n",
    "results.at[\"RandomSearch\", \"Best Root MSE\"] = stats[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num acquisition: 1, time elapsed: 2.16s\n",
      "num acquisition: 2, time elapsed: 3.79s\n",
      "num acquisition: 3, time elapsed: 5.45s\n",
      "num acquisition: 4, time elapsed: 7.70s\n",
      "num acquisition: 5, time elapsed: 9.22s\n",
      "num acquisition: 6, time elapsed: 11.49s\n",
      "num acquisition: 7, time elapsed: 14.04s\n",
      "num acquisition: 8, time elapsed: 17.52s\n",
      "num acquisition: 9, time elapsed: 19.63s\n",
      "num acquisition: 10, time elapsed: 21.49s\n",
      "num acquisition: 11, time elapsed: 23.56s\n",
      "num acquisition: 12, time elapsed: 25.72s\n",
      "num acquisition: 13, time elapsed: 28.07s\n",
      "num acquisition: 14, time elapsed: 31.39s\n",
      "num acquisition: 15, time elapsed: 34.45s\n",
      "num acquisition: 16, time elapsed: 37.43s\n",
      "num acquisition: 17, time elapsed: 39.97s\n",
      "num acquisition: 18, time elapsed: 43.63s\n",
      "num acquisition: 19, time elapsed: 46.27s\n",
      "num acquisition: 20, time elapsed: 48.72s\n"
     ]
    }
   ],
   "source": [
    "#*************BAYESIAN OPTIMIZATION*****************\n",
    "\n",
    "np.random.seed(394) \n",
    "from GPyOpt.methods import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "hp_bounds = [{\n",
    "    'name': 'n_estimators',\n",
    "    'type': 'discrete',\n",
    "    'domain': (10, 150)\n",
    "}, {\n",
    "    'name': 'min_samples_split',\n",
    "    'type': 'discrete',\n",
    "    'domain': (2, 20)\n",
    "}, {\n",
    "    'name': 'min_samples_leaf',\n",
    "    'type': 'discrete',\n",
    "    'domain': (1, 20)\n",
    "}, {\n",
    "    'name': 'max_features',\n",
    "    'type': 'continuous',\n",
    "    'domain': (0.05, 1)\n",
    "}, {\n",
    "    'name': 'bootstrap',\n",
    "    'type': 'discrete',\n",
    "    'domain': (True, False)\n",
    "}]\n",
    "\n",
    "def cv_score(hyp_parameters):\n",
    "    hyp_parameters = hyp_parameters[0] # This just gets us inside the hp_bounds list\n",
    "    rf_model = RandomForestRegressor(n_estimators=int(hyp_parameters[0]),\n",
    "                                 min_samples_split=int(hyp_parameters[1]),\n",
    "                                 min_samples_leaf=int(hyp_parameters[2]),\n",
    "                                 max_features=hyp_parameters[3],\n",
    "                                 bootstrap=bool(hyp_parameters[4]))\n",
    "    scores = cross_val_score(rf_model,\n",
    "                             X=X_train,\n",
    "                             y=y_train,\n",
    "                             cv=KFold(n_splits=5))\n",
    "    return np.array(scores.mean())  # return average of 5-fold scores\n",
    "\n",
    "\n",
    "optimizer = BayesianOptimization(f=cv_score, #Here is our cv_score function\n",
    "                                 domain=hp_bounds, #The argument to our cv_score function\n",
    "                                 model_type='GP',\n",
    "                                 acquisition_type='EI',\n",
    "                                 acquisition_jitter=0.05,\n",
    "                                 exact_feval=True,\n",
    "                                 maximize=True,\n",
    "                                 verbosity=True)\n",
    "\n",
    "optimizer.run_optimization(max_iter=20,verbosity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the optimized hyperparams\n",
    "best_hyp_set = {}\n",
    "for i in range(len(hp_bounds)):\n",
    "    if hp_bounds[i]['type'] == 'continuous':\n",
    "        best_hyp_set[hp_bounds[i]['name']] = optimizer.x_opt[i]\n",
    "    else:\n",
    "        best_hyp_set[hp_bounds[i]['name']] = int(optimizer.x_opt[i])\n",
    "\n",
    "#Save the best hyperparams\n",
    "results.at[\"BayesianOptimization\",\"bootstrap\":\"n_estimators\"] = best_hyp_set\n",
    "#save the number of model fits into the dataframe manually\n",
    "results.at[\"BayesianOptimization\",\"Number of fits\"] = 25*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=1, criterion='mse', max_depth=None,\n",
       "                      max_features=0.3403767613646175, max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=150,\n",
       "                      n_jobs=None, oob_score=False, random_state=None,\n",
       "                      verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayopt_search = RandomForestRegressor(**best_hyp_set)\n",
    "bayopt_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply my_regression_results to test data\n",
    "stats = my_regression_results(bayopt_search)\n",
    "results.at[\"BayesianOptimization\", \"Best R-Squared\"] = stats[0]\n",
    "results.at[\"BayesianOptimization\", \"Best MSE\"] = stats[1]\n",
    "results.at[\"BayesianOptimization\", \"Best Root MSE\"] = stats[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=120, style=ProgressStyle(descript…"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.4486717031231568\n",
      "Generation 2 - Current best internal CV score: 0.4520778668841704\n",
      "Generation 3 - Current best internal CV score: 0.45739544435638324\n",
      "Generation 4 - Current best internal CV score: 0.45739544435638324\n",
      "Generation 5 - Current best internal CV score: 0.45739544435638324\n",
      "\n",
      "Best pipeline: RandomForestRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, bootstrap=True, max_features=1.0, min_samples_leaf=12, min_samples_split=12, n_estimators=10), bootstrap=True, max_features=0.1, min_samples_leaf=1, min_samples_split=2, n_estimators=75), bootstrap=True, max_features=1.0, min_samples_leaf=7, min_samples_split=10, n_estimators=125)\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "#********************  TPOT  **********************\n",
    "####################################################\n",
    "\n",
    "from tpot import TPOTRegressor\n",
    "\n",
    "tpot_config = {\n",
    "    'sklearn.ensemble.RandomForestRegressor': {\n",
    "        \"n_estimators\": [10, 25, 50, 75, 100, 125, 150],\n",
    "        \"max_features\": [0.05, 0.1, 0.3, 0.5, 0.7, 1.0],\n",
    "        \"min_samples_split\": [2, 5, 7, 10, 12, 15, 17, 20],\n",
    "        \"min_samples_leaf\": [1, 3, 5, 7, 10, 12, 15, 17, 20],\n",
    "        \"bootstrap\": [True, False]\n",
    "    }\n",
    "}\n",
    "\n",
    "tpot = TPOTRegressor(generations=5,\n",
    "                     population_size=20,\n",
    "                     verbosity=2,\n",
    "                     config_dict=tpot_config,\n",
    "                     cv=5,\n",
    "                     scoring='r2',\n",
    "                     random_state=394)\n",
    "tpot.fit(X_train, y_train)\n",
    "tpot.export('tpot_RandomForestRegressor.py') # export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best model results from TPOT (per Piazza post @512, it is not practical to save to my 'results' dataframe)\n",
    "\n",
    "#from \"tpot_RandomForestRegressor.py\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from tpot.builtins import StackingEstimator\n",
    "\n",
    "\n",
    "tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
    "features = tpot_data.drop('target', axis=1).values\n",
    "training_features, testing_features, training_target, testing_target = \\\n",
    "            train_test_split(features, tpot_data['target'].values, random_state=394)\n",
    "\n",
    "\n",
    "exported_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=RandomForestRegressor(bootstrap=True, max_features=1.0, min_samples_leaf=12, min_samples_split=12, n_estimators=10)),\n",
    "    StackingEstimator(estimator=RandomForestRegressor(bootstrap=True, max_features=0.1, min_samples_leaf=1, min_samples_split=2, n_estimators=75)),\n",
    "    RandomForestRegressor(bootstrap=True, max_features=1.0, min_samples_leaf=7, min_samples_split=10, n_estimators=125)\n",
    ")\n",
    "\n",
    "exported_pipeline.fit(training_features, training_target)\n",
    "results = exported_pipeline.predict(testing_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = my_regression_results(tpot)\n",
    "\n",
    "results.at[\"tpot\", \"Best R-Squared\"] = stats[0]\n",
    "results.at[\"tpot\", \"Best MSE\"] = stats[1]\n",
    "results.at[\"tpot\", \"Best Root MSE\"] = stats[2]\n",
    "results.at[\"tpot\",\"Number of fits\"] = 6*20*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bootstrap</th>\n",
       "      <th>max_features</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>Best R-Squared</th>\n",
       "      <th>Best MSE</th>\n",
       "      <th>Best Root MSE</th>\n",
       "      <th>Number of fits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>GridSearch</td>\n",
       "      <td>True</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>0.398</td>\n",
       "      <td>3818.58</td>\n",
       "      <td>61.7947</td>\n",
       "      <td>2400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>RandomSearch</td>\n",
       "      <td>True</td>\n",
       "      <td>0.399503</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>133</td>\n",
       "      <td>0.3996</td>\n",
       "      <td>3808.72</td>\n",
       "      <td>61.7148</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>BayesianOptimization</td>\n",
       "      <td>1</td>\n",
       "      <td>0.340377</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.4148</td>\n",
       "      <td>3711.98</td>\n",
       "      <td>60.9261</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>tpot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3467</td>\n",
       "      <td>4144.2</td>\n",
       "      <td>64.3755</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     bootstrap max_features min_samples_leaf  \\\n",
       "GridSearch                True          0.5                1   \n",
       "RandomSearch              True     0.399503                1   \n",
       "BayesianOptimization         1     0.340377                1   \n",
       "tpot                       NaN          NaN              NaN   \n",
       "\n",
       "                     min_samples_split n_estimators Best R-Squared Best MSE  \\\n",
       "GridSearch                           5          100          0.398  3818.58   \n",
       "RandomSearch                         7          133         0.3996  3808.72   \n",
       "BayesianOptimization                 2          150         0.4148  3711.98   \n",
       "tpot                               NaN          NaN         0.3467   4144.2   \n",
       "\n",
       "                     Best Root MSE Number of fits  \n",
       "GridSearch                 61.7947           2400  \n",
       "RandomSearch               61.7148            125  \n",
       "BayesianOptimization       60.9261            125  \n",
       "tpot                       64.3755            600  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "<font color = \"blue\"> *** 5 points: </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the Random Forest model does not fit the diabetes dataset as well as the XGBoost model from the Project handout. Even after optimizing the hyperparameters for four different methods, the resulting statistics from each fit consistently showed that the Random Forest model does not fit the data as well as the XGBoost model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - Optimize XGBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find optimized hyperparameters for an xgboost classifier model. \n",
    "\n",
    "This problem contains 5 parts.\n",
    "\n",
    "\n",
    "### Notes:\n",
    "\n",
    "#### About the data\n",
    "The first cell below loads a subset of the loans default data from DS705 and your job is to predict whether a loan defaults or not.  The `status_bad` column is the target column and a 1 indicates a loan that defaulted.  We have selected a subset of the original data that includes 2000 each of good and bad loans.  The data has already been cleaned and encoded.  You're welcome to look into a different dataset, but start by getting this working and then add your own data.\n",
    "\n",
    "#### This is classification, not regression\n",
    "The score for each model will be accuracy and not MSE.  Your summary table should include accuracy, sensitivity, and precision for each optimized model applied to the test data.  (<a href=\"https://classeval.wordpress.com/introduction/basic-evaluation-measures/\">Here is a nice overview of metrics for binary classification data</a>) that includes definitions of accuracy and such.\n",
    "\n",
    "For the models you'll mostly just need to change 'regressor' to 'classifier', e.g. `XGBClassifier` instead of `XGBRegressor`.\n",
    "\n",
    "\n",
    "Hyperparameter | Type | Default Value | Typical Range\n",
    "---- | ---- | ---- | ----\n",
    "n_estimators | discrete / integer | 100 | 50 to 150\n",
    "max_depth | discrete / integer | 3| 1 to 10\n",
    "min_child_weight | discrete / integer | 1 | 1 to 20\n",
    "learning_rate | continuous / float | 0.1 | 0.001 to 1\n",
    "sub_sample | continuous / float | 1 | 0.05 to 1\n",
    "reg_lambda | continuous / float | 1 | 0 to 5\n",
    "reg_alpha  | continuous / float | 0 | 0 to 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLASSIFICATION PROBLEM\n",
    "\n",
    "# Do not change this cell for loading and preparing the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X = pd.read_csv('./data/loans_subset.csv')\n",
    "\n",
    "# split into predictors and target\n",
    "# convert to numpy arrays for xgboost, OK for other models too\n",
    "y = np.array(X['status_Bad']) # 1 for bad loan, 0 for good loan\n",
    "X = np.array(X.drop(columns = ['status_Bad']))\n",
    "\n",
    "# split into test and training data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0) #we select 10% for the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "Write a function called `my_classifier_results` modeled after `my_regression_results` that applies a model to the test data and prints out the accuracy, sensitivity, precision, and the confusion matrix.  There is no need to make a plot.\n",
    "\n",
    "<font color = \"blue\"> *** 5 points - (don't delete this cell) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_classifier_results(model):\n",
    "    accuracy = model.score(X_test,y_test)\n",
    "    \n",
    "    \n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import pandas as pd\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    #must put true before predictions in confusion matrix function\n",
    "    cmtx = pd.DataFrame(\n",
    "        confusion_matrix(y_test, y_pred, labels=[1,0]), \n",
    "        index=['actual:bad', 'actual:good'], \n",
    "        columns=['pred:bad','pred:good']\n",
    "    )\n",
    "    display(cmtx)\n",
    "    \n",
    "    sensitivity = cmtx['pred:bad']['actual:bad']/(cmtx['pred:bad']['actual:bad']+cmtx['pred:good']['actual:bad'])\n",
    "    \n",
    "    precision = cmtx['pred:bad']['actual:bad']/(cmtx['pred:bad']['actual:bad']+cmtx['pred:bad']['actual:good'])\n",
    "    \n",
    "    return {\"Accuracy\":accuracy, \"Sensitivity\":sensitivity, \"Precision\":precision} \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "Start by training some baseline models using default values of the hyperparameters.  We've included logistic regression in a cell below to get you started.  Use `LogisticRegression`, `RandomForestClassifier`, and `GaussianNB` (Gaussian Naive Bayes) from `sklearn`.  Also use `XGBClassifier` from `xgboost` where you may need to include `objective=\"binary:logistic\"` as an option. The default scoring method for all of the `sklearn` classifiers is accuracy. Apply `my_classifier_results` to the test data for each model.\n",
    "\n",
    "<font color = \"blue\"> *** 10 points - (don't delete this cell) </font>\n",
    "\n",
    "<font color=\"red\"> -2 points, should </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5475\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred:bad</th>\n",
       "      <th>pred:good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>actual:bad</td>\n",
       "      <td>126</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>actual:good</td>\n",
       "      <td>110</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             pred:bad  pred:good\n",
       "actual:bad        126         71\n",
       "actual:good       110         93"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We've included this code to get you started\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# we do need to go higher than the default iterations for the solver to get convergence\n",
    "# and the explicity declaration of the solver avoids a warning message, otherwise\n",
    "# the parameters are defaults.\n",
    "logreg_model = LogisticRegression(solver='lbfgs',max_iter=1000)\n",
    "\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "# Use score method to get accuracy of model\n",
    "score = logreg_model.score(X_test, y_test) # this is accuracy\n",
    "print(score)\n",
    "\n",
    "# obtaining the confusion matrix and making it look nice\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "y_pred = logreg_model.predict(X_test)\n",
    "\n",
    "# must put true before predictions in confusion matrix function\n",
    "cmtx = pd.DataFrame(\n",
    "    confusion_matrix(y_test, y_pred, labels=[1,0]), \n",
    "    index=['actual:bad', 'actual:good'], \n",
    "    columns=['pred:bad','pred:good']\n",
    ")\n",
    "display(cmtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up final_results df\n",
    "final_results = pd.DataFrame(None, index=[\"Baseline_LR\",\"Baseline_RF\", \"Baseline_GaussianNB\",\"Baseline_XGBClassifier\",\n",
    "                                          \"GridSearch\", \"RandomSearch\", \"BayesianOptimization\", \"TPOT_parameters\", \"TPOT_model_selection\"], \n",
    "                                 columns=[\"learning_rate\", \"max_depth\", \"n_estimators\", \"subsample\", \"min_child_weight\", \"reg_lambda\",\"reg_alpha\",\n",
    "                                         \"Accuracy\",\"Sensitivity\",\"Precision\",\"Number of fits\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred:bad</th>\n",
       "      <th>pred:good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>actual:bad</td>\n",
       "      <td>126</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>actual:good</td>\n",
       "      <td>110</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             pred:bad  pred:good\n",
       "actual:bad        126         71\n",
       "actual:good       110         93"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#*********LOGISTIC REGRESSION BASELINE (using my_classifier_results)*************\n",
    "\n",
    "stats_dict = my_classifier_results(logreg_model)\n",
    "\n",
    "#Save the best hyperparams\n",
    "final_results.at[\"Baseline_LR\",:] = stats_dict\n",
    "#save the number of model fits into the dataframe manually\n",
    "final_results.at[\"Baseline_LR\",\"Number of fits\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#*********RANDOM FOREST BASELINE*************\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(random_state=0)\n",
    "rf_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred:bad</th>\n",
       "      <th>pred:good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>actual:bad</td>\n",
       "      <td>100</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>actual:good</td>\n",
       "      <td>63</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             pred:bad  pred:good\n",
       "actual:bad        100         97\n",
       "actual:good        63        140"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_dict = my_classifier_results(rf_model)\n",
    "\n",
    "#Save the best hyperparams\n",
    "final_results.at[\"Baseline_RF\",:] = stats_dict\n",
    "#save the number of model fits into the dataframe manually\n",
    "final_results.at[\"Baseline_RF\",\"Number of fits\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred:bad</th>\n",
       "      <th>pred:good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>actual:bad</td>\n",
       "      <td>160</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>actual:good</td>\n",
       "      <td>139</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             pred:bad  pred:good\n",
       "actual:bad        160         37\n",
       "actual:good       139         64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#*********GAUSSIAN NAIVE BAYES BASELINE*************\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gauss_model = GaussianNB()\n",
    "gauss_model.fit(X_train,y_train)\n",
    "\n",
    "stats_dict = my_classifier_results(gauss_model)\n",
    "\n",
    "#Save the best hyperparams\n",
    "final_results.at[\"Baseline_GaussianNB\",:] = stats_dict\n",
    "#save the number of model fits into the dataframe manually\n",
    "final_results.at[\"Baseline_GaussianNB\",\"Number of fits\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred:bad</th>\n",
       "      <th>pred:good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>actual:bad</td>\n",
       "      <td>132</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>actual:good</td>\n",
       "      <td>70</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             pred:bad  pred:good\n",
       "actual:bad        132         65\n",
       "actual:good        70        133"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#*********XGBClassifier BASELINE*************\n",
    "import xgboost as xgb\n",
    "\n",
    "xgbr_model = xgb.XGBClassifier(objective =\"binary:logistic\")\n",
    "xgbr_model.fit(X_train,y_train)\n",
    "\n",
    "stats_dict = my_classifier_results(xgbr_model)\n",
    "\n",
    "#Save the best hyperparams\n",
    "final_results.at[\"Baseline_XGBClassifier\",:] = stats_dict\n",
    "#save the number of model fits into the dataframe manually\n",
    "final_results.at[\"Baseline_XGBClassifier\",\"Number of fits\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4\n",
    "\n",
    "Now use the four hyperparameter optimization techniques on `XGBClassifier` and TPOT general model optimization.  Apply `my_classifer_results` to the test data in each case.\n",
    "* Feel free to use 3 folds instead of 5 for cross validation to speed things up. \n",
    "* Choose a very small number of iterations, population size, etc. until you're sure things are working correctly, then turn up the numbers.  General TPOT optimization will take a while (fair warning: it took about 30 minutes on my Macbook Pro with generations = 10, population_size=40, and cv=5)  \n",
    "* The hyperparameters to consider for are the same as they were in the presentation , but here they are again for convenience:\n",
    "\n",
    "<font color = \"blue\"> *** 10 points - (don't delete this cell) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 192 candidates, totalling 576 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 576 out of 576 | elapsed:  6.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                     colsample_bylevel=1, colsample_bynode=1,\n",
       "                                     colsample_bytree=1, gamma=0,\n",
       "                                     learning_rate=0.1, max_delta_step=0,\n",
       "                                     max_depth=3, min_child_weight=1,\n",
       "                                     missing=None, n_estimators=100, n_jobs=1,\n",
       "                                     nthread=None, objective='binary:logistic',\n",
       "                                     random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                                     scale_pos_weight=1, seed=None, silent=None,\n",
       "                                     subsample=1, verbosity=1),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.01, 0.1], 'max_depth': [2, 4, 6],\n",
       "                         'min_child_weight': [7, 13], 'n_estimators': [75, 125],\n",
       "                         'reg_alpha': [2, 4], 'reg_lambda': [2, 4],\n",
       "                         'subsample': [0.25, 0.75]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#*******************GRID SEARCH********************\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define the grid\n",
    "params = {\n",
    "    \"learning_rate\": [0.01, 0.1],\n",
    "    \"max_depth\": [2, 4, 6],\n",
    "    \"n_estimators\": [75, 125],\n",
    "    \"subsample\": [0.25, 0.75],\n",
    "    \"min_child_weight\": [7, 13],\n",
    "    \"reg_lambda\": [2, 4],\n",
    "    \"reg_alpha\": [2, 4]\n",
    "}\n",
    "\n",
    "# setup the grid search\n",
    "grid_search = GridSearchCV(xgbr_model,\n",
    "                           param_grid=params,\n",
    "                           cv=3,\n",
    "                           verbose=1,\n",
    "                           n_jobs=-1,\n",
    "                           return_train_score=True)\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred:bad</th>\n",
       "      <th>pred:good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>actual:bad</td>\n",
       "      <td>134</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>actual:good</td>\n",
       "      <td>71</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             pred:bad  pred:good\n",
       "actual:bad        134         63\n",
       "actual:good        71        132"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save the best hyperparams\n",
    "final_results.at[\"GridSearch\",\"learning_rate\":\"reg_alpha\"] = grid_search.best_params_\n",
    "\n",
    "stats_dict = my_classifier_results(grid_search)\n",
    "\n",
    "#Save the stats from the test data\n",
    "final_results.at[\"GridSearch\",\"Accuracy\":\"Precision\"] = stats_dict\n",
    "#save the number of model fits into the dataframe manually\n",
    "final_results.at[\"GridSearch\",\"Number of fits\"] = 2*3*2*2*2*2*2*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 125 out of 125 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "                   estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                           colsample_bylevel=1,\n",
       "                                           colsample_bynode=1,\n",
       "                                           colsample_bytree=1, gamma=0,\n",
       "                                           learning_rate=0.1, max_delta_step=0,\n",
       "                                           max_depth=3, min_child_weight=1,\n",
       "                                           missing=None, n_estimators=100,\n",
       "                                           n_jobs=1, nthread=None,\n",
       "                                           objective='binary:logistic',\n",
       "                                           random_state=0, reg_alpha=0...\n",
       "                                        'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f3f8b09bac8>,\n",
       "                                        'reg_alpha': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f3f93ef1198>,\n",
       "                                        'reg_lambda': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f3f93ef17f0>,\n",
       "                                        'subsample': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f3f8b09bc50>},\n",
       "                   pre_dispatch='2*n_jobs', random_state=394, refit=True,\n",
       "                   return_train_score=True, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#*************RANDOM SEARCH*****************\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "params = {\n",
    "    \"learning_rate\": [0.01, 0.1, 0.25, 0.75, 1.0],\n",
    "    \"max_depth\": randint(1, 10),\n",
    "    \"n_estimators\": randint(50, 100),\n",
    "    \"subsample\": uniform(0.05, 0.95),  # so uniform on [.05,.05+.95] = [.05,1.]\n",
    "    \"min_child_weight\": randint(1, 20),\n",
    "    \"reg_alpha\": uniform(0, 5),\n",
    "    \"reg_lambda\": uniform(0, 5)\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgbr_model,\n",
    "    param_distributions=params,\n",
    "    random_state=394, # like setting the seed\n",
    "    n_iter=25, #just checking 25 randomly selected sets of hyperparameters\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=1,\n",
    "    return_train_score=True)\n",
    "\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred:bad</th>\n",
       "      <th>pred:good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>actual:bad</td>\n",
       "      <td>131</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>actual:good</td>\n",
       "      <td>77</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             pred:bad  pred:good\n",
       "actual:bad        131         66\n",
       "actual:good        77        126"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save the best hyperparams\n",
    "final_results.at[\"RandomSearch\",\"learning_rate\":\"reg_alpha\"] = random_search.best_params_\n",
    "\n",
    "stats_dict = my_classifier_results(random_search)\n",
    "\n",
    "#Save the stats from the test data\n",
    "final_results.at[\"RandomSearch\",\"Accuracy\":\"Precision\"] = stats_dict\n",
    "#save the number of model fits into the dataframe manually\n",
    "final_results.at[\"RandomSearch\",\"Number of fits\"] = 5*25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num acquisition: 1, time elapsed: 2.27s\n",
      "num acquisition: 2, time elapsed: 4.00s\n",
      "num acquisition: 3, time elapsed: 6.04s\n",
      "num acquisition: 4, time elapsed: 7.74s\n",
      "num acquisition: 5, time elapsed: 9.06s\n",
      "num acquisition: 6, time elapsed: 10.80s\n",
      "num acquisition: 7, time elapsed: 12.63s\n",
      "num acquisition: 8, time elapsed: 14.71s\n",
      "num acquisition: 9, time elapsed: 16.60s\n",
      "num acquisition: 10, time elapsed: 18.42s\n",
      "num acquisition: 11, time elapsed: 19.95s\n",
      "num acquisition: 12, time elapsed: 22.17s\n",
      "num acquisition: 13, time elapsed: 25.05s\n",
      "num acquisition: 14, time elapsed: 27.08s\n",
      "num acquisition: 15, time elapsed: 29.32s\n",
      "num acquisition: 16, time elapsed: 31.54s\n",
      "num acquisition: 17, time elapsed: 33.76s\n",
      "num acquisition: 18, time elapsed: 36.26s\n",
      "num acquisition: 19, time elapsed: 38.85s\n",
      "num acquisition: 20, time elapsed: 42.12s\n"
     ]
    }
   ],
   "source": [
    "#*************BAYESIAN OPTIMIZATION*****************\n",
    "\n",
    "np.random.seed(394)  # seed courtesy of Tommy Tutone\n",
    "from GPyOpt.methods import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "hp_bounds = [{\n",
    "    'name': 'learning_rate',\n",
    "    'type': 'continuous',\n",
    "    'domain': (0.001, 1.0)\n",
    "}, {\n",
    "    'name': 'max_depth',\n",
    "    'type': 'discrete',\n",
    "    'domain': (1, 10)\n",
    "}, {\n",
    "    'name': 'n_estimators',\n",
    "    'type': 'discrete',\n",
    "    'domain': (50, 100)\n",
    "}, {\n",
    "    'name': 'subsample',\n",
    "    'type': 'continuous',\n",
    "    'domain': (0.05, 1.0)\n",
    "}, {\n",
    "    'name': 'min_child_weight',\n",
    "    'type': 'discrete',\n",
    "    'domain': (1, 20)\n",
    "}, {\n",
    "    'name': 'reg_alpha',\n",
    "    'type': 'continuous',\n",
    "    'domain': (0, 5)\n",
    "}, {\n",
    "    'name': 'reg_lambda',\n",
    "    'type': 'continuous',\n",
    "    'domain': (0, 5)\n",
    "}]\n",
    "\n",
    "\n",
    "# Optimization objective\n",
    "def cv_score(hyp_parameters):\n",
    "    hyp_parameters = hyp_parameters[0] # I THINK this just gets us inside the hp_bounds list\n",
    "    xgb_model = xgb.XGBClassifier(objective='binary:logistic',\n",
    "                                 learning_rate=hyp_parameters[0],\n",
    "                                 max_depth=int(hyp_parameters[1]),\n",
    "                                 n_estimators=int(hyp_parameters[2]),\n",
    "                                 subsample=hyp_parameters[3],\n",
    "                                 min_child_weight=int(hyp_parameters[4]),\n",
    "                                 reg_alpha=hyp_parameters[5],\n",
    "                                 reg_lambda=hyp_parameters[6])\n",
    "    scores = cross_val_score(xgb_model,\n",
    "                             X=X_train,\n",
    "                             y=y_train,\n",
    "                             cv=KFold(n_splits=5))\n",
    "    return np.array(scores.mean())  # return average of 5-fold scores\n",
    "\n",
    "\n",
    "optimizer = BayesianOptimization(f=cv_score, #Here is our cv_score function\n",
    "                                 domain=hp_bounds, #The argument to our cv_score function\n",
    "                                 model_type='GP',\n",
    "                                 acquisition_type='EI',\n",
    "                                 acquisition_jitter=0.05,\n",
    "                                 exact_feval=True,\n",
    "                                 maximize=True,\n",
    "                                 verbosity=True)\n",
    "\n",
    "optimizer.run_optimization(max_iter=20,verbosity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=1.0, max_delta_step=0, max_depth=1,\n",
       "              min_child_weight=20, missing=None, n_estimators=100, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=3.733804277871873, reg_lambda=5.0, scale_pos_weight=1,\n",
       "              seed=None, silent=None, subsample=1.0, verbosity=1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hyp_set = {}\n",
    "for i in range(len(hp_bounds)):\n",
    "    if hp_bounds[i]['type'] == 'continuous':\n",
    "        best_hyp_set[hp_bounds[i]['name']] = optimizer.x_opt[i]\n",
    "    else:\n",
    "        best_hyp_set[hp_bounds[i]['name']] = int(optimizer.x_opt[i])\n",
    "\n",
    "bayopt_search = xgb.XGBClassifier(objective='binary:logistic',**best_hyp_set)\n",
    "bayopt_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred:bad</th>\n",
       "      <th>pred:good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>actual:bad</td>\n",
       "      <td>130</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>actual:good</td>\n",
       "      <td>65</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             pred:bad  pred:good\n",
       "actual:bad        130         67\n",
       "actual:good        65        138"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save the best hyperparams\n",
    "final_results.at[\"BayesianOptimization\",\"learning_rate\":\"reg_alpha\"] = best_hyp_set\n",
    "\n",
    "stats_dict = my_classifier_results(bayopt_search)\n",
    "\n",
    "#Save the stats from the test data\n",
    "final_results.at[\"BayesianOptimization\",\"Accuracy\":\"Precision\"] = stats_dict\n",
    "#save the number of model fits into the dataframe manually\n",
    "final_results.at[\"BayesianOptimization\",\"Number of fits\"] = 5*25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=120, style=ProgressStyle(descript…"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.6588888888888889\n",
      "Generation 2 - Current best internal CV score: 0.6588888888888889\n",
      "Generation 3 - Current best internal CV score: 0.6588888888888889\n",
      "Generation 4 - Current best internal CV score: 0.6588888888888889\n",
      "Generation 5 - Current best internal CV score: 0.6588888888888889\n",
      "\n",
      "Best pipeline: XGBClassifier(input_matrix, learning_rate=0.001, max_depth=3, min_child_weight=11, n_estimators=125, nthread=1, objective=binary:logistic, reg_alpha=3, reg_lambda=1, subsample=0.3)\n"
     ]
    }
   ],
   "source": [
    "#************* TPOT - hyperparameter optimization only *****************\n",
    "    \n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "tpot_config = {\n",
    "    'xgboost.XGBClassifier': {\n",
    "        'n_estimators': [50, 100, 125],\n",
    "        'max_depth': range(1, 11),\n",
    "        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],\n",
    "        'subsample': np.arange(0.05, 1.01, 0.05),\n",
    "        'min_child_weight': range(1, 21),\n",
    "        'reg_alpha': range(1, 6),\n",
    "        'reg_lambda': range(1, 6),\n",
    "        'nthread': [1],\n",
    "        'objective': ['binary:logistic']\n",
    "    }\n",
    "}\n",
    "\n",
    "tpot = TPOTClassifier(generations=5,\n",
    "                     population_size=20,\n",
    "                     verbosity=2,\n",
    "                     config_dict=tpot_config,\n",
    "                     cv=3,\n",
    "                     random_state=394)\n",
    "tpot.fit(X_train, y_train)\n",
    "tpot.export('tpot_XGBClassifier.py') # export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'tpot_XGBClassifier.py' file contents\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# NOTE: Make sure that the class is labeled 'target' in the data file\n",
    "tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
    "features = tpot_data.drop('target', axis=1).values\n",
    "training_features, testing_features, training_target, testing_target = \\\n",
    "            train_test_split(features, tpot_data['target'].values, random_state=394)\n",
    "\n",
    "# Average CV score on the training set was:0.6588888888888889\n",
    "exported_pipeline = XGBClassifier(learning_rate=0.001, max_depth=3, min_child_weight=11, n_estimators=125, nthread=1, objective=\"binary:logistic\", reg_alpha=3, reg_lambda=1, subsample=0.3)\n",
    "\n",
    "exported_pipeline.fit(training_features, training_target)\n",
    "results = exported_pipeline.predict(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred:bad</th>\n",
       "      <th>pred:good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>actual:bad</td>\n",
       "      <td>137</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>actual:good</td>\n",
       "      <td>85</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             pred:bad  pred:good\n",
       "actual:bad        137         60\n",
       "actual:good        85        118"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_dict = my_classifier_results(tpot)\n",
    "\n",
    "#Save the stats from the test data\n",
    "final_results.at[\"TPOT_parameters\",\"Accuracy\":\"Precision\"] = stats_dict\n",
    "#save the number of model fits into the dataframe manually\n",
    "final_results.at[\"TPOT_parameters\",\"Number of fits\"] = 4*20*5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=120, style=ProgressStyle(descript…"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.6602777777777779\n",
      "Generation 2 - Current best internal CV score: 0.6644444444444445\n",
      "Generation 3 - Current best internal CV score: 0.6644444444444445\n",
      "Generation 4 - Current best internal CV score: 0.6644444444444445\n",
      "Generation 5 - Current best internal CV score: 0.6644444444444445\n",
      "\n",
      "Best pipeline: BernoulliNB(GradientBoostingClassifier(VarianceThreshold(input_matrix, threshold=0.01), learning_rate=0.1, max_depth=2, max_features=0.35000000000000003, min_samples_leaf=5, min_samples_split=14, n_estimators=100, subsample=0.8500000000000001), alpha=10.0, fit_prior=False)\n"
     ]
    }
   ],
   "source": [
    "#************* TPOT - including model selection *****************\n",
    "\n",
    "tpot = TPOTClassifier(generations=5,\n",
    "                     population_size=20,\n",
    "                     verbosity=2,\n",
    "                     cv=3,\n",
    "                     random_state=394)\n",
    "\n",
    "tpot.fit(X_train, y_train)\n",
    "tpot.export('tpot_model_selection.py') # export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'tpot_model_selection.py' file contents\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from tpot.builtins import StackingEstimator\n",
    "\n",
    "# NOTE: Make sure that the class is labeled 'target' in the data file\n",
    "tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
    "features = tpot_data.drop('target', axis=1).values\n",
    "training_features, testing_features, training_target, testing_target = \\\n",
    "            train_test_split(features, tpot_data['target'].values, random_state=394)\n",
    "\n",
    "# Average CV score on the training set was:0.6644444444444445\n",
    "exported_pipeline = make_pipeline(\n",
    "    VarianceThreshold(threshold=0.01),\n",
    "    StackingEstimator(estimator=GradientBoostingClassifier(learning_rate=0.1, max_depth=2, max_features=0.35000000000000003, min_samples_leaf=5, min_samples_split=14, n_estimators=100, subsample=0.8500000000000001)),\n",
    "    BernoulliNB(alpha=10.0, fit_prior=False)\n",
    ")\n",
    "\n",
    "exported_pipeline.fit(training_features, training_target)\n",
    "results = exported_pipeline.predict(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred:bad</th>\n",
       "      <th>pred:good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>actual:bad</td>\n",
       "      <td>132</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>actual:good</td>\n",
       "      <td>73</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             pred:bad  pred:good\n",
       "actual:bad        132         65\n",
       "actual:good        73        130"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>subsample</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>reg_lambda</th>\n",
       "      <th>reg_alpha</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Number of fits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Baseline_LR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5475</td>\n",
       "      <td>0.639594</td>\n",
       "      <td>0.533898</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Baseline_RF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.507614</td>\n",
       "      <td>0.613497</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Baseline_GaussianNB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.812183</td>\n",
       "      <td>0.535117</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Baseline_XGBClassifier</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.670051</td>\n",
       "      <td>0.653465</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>GridSearch</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.680203</td>\n",
       "      <td>0.653659</td>\n",
       "      <td>576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>RandomSearch</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>0.779526</td>\n",
       "      <td>13</td>\n",
       "      <td>2.57853</td>\n",
       "      <td>0.844422</td>\n",
       "      <td>0.6425</td>\n",
       "      <td>0.664975</td>\n",
       "      <td>0.629808</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>BayesianOptimization</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>3.7338</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.659898</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>TPOT_parameters</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6375</td>\n",
       "      <td>0.695431</td>\n",
       "      <td>0.617117</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>TPOT_model_selection</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.670051</td>\n",
       "      <td>0.643902</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       learning_rate max_depth n_estimators subsample  \\\n",
       "Baseline_LR                      NaN       NaN          NaN       NaN   \n",
       "Baseline_RF                      NaN       NaN          NaN       NaN   \n",
       "Baseline_GaussianNB              NaN       NaN          NaN       NaN   \n",
       "Baseline_XGBClassifier           NaN       NaN          NaN       NaN   \n",
       "GridSearch                       0.1         2           75      0.75   \n",
       "RandomSearch                     0.1         1           86  0.779526   \n",
       "BayesianOptimization               1         1          100         1   \n",
       "TPOT_parameters                  NaN       NaN          NaN       NaN   \n",
       "TPOT_model_selection             NaN       NaN          NaN       NaN   \n",
       "\n",
       "                       min_child_weight reg_lambda reg_alpha Accuracy  \\\n",
       "Baseline_LR                         NaN        NaN       NaN   0.5475   \n",
       "Baseline_RF                         NaN        NaN       NaN      0.6   \n",
       "Baseline_GaussianNB                 NaN        NaN       NaN     0.56   \n",
       "Baseline_XGBClassifier              NaN        NaN       NaN   0.6625   \n",
       "GridSearch                            7          4         2    0.665   \n",
       "RandomSearch                         13    2.57853  0.844422   0.6425   \n",
       "BayesianOptimization                 20          5    3.7338     0.67   \n",
       "TPOT_parameters                     NaN        NaN       NaN   0.6375   \n",
       "TPOT_model_selection                NaN        NaN       NaN    0.655   \n",
       "\n",
       "                       Sensitivity Precision Number of fits  \n",
       "Baseline_LR               0.639594  0.533898              1  \n",
       "Baseline_RF               0.507614  0.613497              1  \n",
       "Baseline_GaussianNB       0.812183  0.535117              1  \n",
       "Baseline_XGBClassifier    0.670051  0.653465              1  \n",
       "GridSearch                0.680203  0.653659            576  \n",
       "RandomSearch              0.664975  0.629808            125  \n",
       "BayesianOptimization      0.659898  0.666667            125  \n",
       "TPOT_parameters           0.695431  0.617117            400  \n",
       "TPOT_model_selection      0.670051  0.643902            400  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_dict = my_classifier_results(tpot)\n",
    "\n",
    "#Save the stats from the test data\n",
    "final_results.at[\"TPOT_model_selection\",\"Accuracy\":\"Precision\"] = stats_dict\n",
    "#save the number of model fits into the dataframe manually\n",
    "final_results.at[\"TPOT_model_selection\",\"Number of fits\"] = 4*20*5\n",
    "final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 - Summary\n",
    "\n",
    "* In addition to your summary table, answer:\n",
    "    * The bank isn't as concerned about misclassifying some truly good loans as they are interested in correctly predicting truly bad loans.  Which model should they use?  Why?\n",
    "\n",
    "<font color = \"blue\"> *** 5 points - (don't delete this cell) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the bank cares more about correctly labeling the true bad loads, they will want to use the model with the highest sensitivity. \n",
    "\n",
    "Out of all the models tested here, the baseline Gaussian Naive Bayes model with the default hyperparameters did the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
